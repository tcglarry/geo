{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stage_2_reproduce_Unet_colab_correct_ch32_NRMSE_loss.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcglarry/geo/blob/master/stage_2_reproduce_Unet_colab_correct_ch32_NRMSE_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ULOV3iUHb3AY",
        "colab_type": "code",
        "outputId": "13d02c6f-e08d-4795-810f-c9d0f138ca18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"ali_build_model_stage_2_MobileNetV2_aug_1000_dense.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1wSVEmcoWzRm-7B6UGkHB8ysfMDRT_xK5\n",
        "\"\"\"\n",
        "\n",
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 110842 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7nVDv8jnhjxa",
        "colab_type": "code",
        "outputId": "d36b59ee-fabf-467f-95c1-450ac70a7bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Romttyl6hnXo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "src=  'drive/My Drive/geo/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o1NXeS48b3Aa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import subprocess\n",
        "import os\n",
        "import pickle\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, merge, Lambda,UpSampling2D, concatenate, Conv2DTranspose\n",
        "from keras.models import Model, load_model\n",
        "import pandas as pd\n",
        "import sklearn \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ln7UmUkob3Af",
        "colab_type": "code",
        "outputId": "425707a8-aaea-4adc-9622-ff713e3f0799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "data= np.load(src+'train_data.npy')\n",
        "label = np.load(src+'train_labels.npy')\n",
        "print (np.mean(label))\n",
        "label = label[:,:,:,np.newaxis]\n",
        "print (data.shape)\n",
        "print (label.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.66184075511905\n",
            "(4200, 21, 21, 20)\n",
            "(4200, 20, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pt53pvkOb3Am",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "data_mean = np.mean(data)\n",
        "data_std = np.std(data)\n",
        "label_max = np.max(label)\n",
        "label_min = np.min(label)\n",
        "\n",
        "train_data = (data - data_mean)/data_std\n",
        "#label_data = (label-label_min)/(label_max - label_min)\n",
        "label_data = np.log(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vUAqqRRtb3Ap",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_x,val_x,train_y,val_y = train_test_split(train_data,label_data,test_size=0.2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SjsPK1q5b3As",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_block(ch, activation= 'relu', padding='same',kernel_regularizer=regularizers.l2(0.01)):       \n",
        "    return Conv2D(ch,(3,3),activation= activation, padding =padding )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uWUY57U4b3Av",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#def build_model_1(IMAGE_HEIGHT=IMAGE_HEIGHT,IMAGE_WIDTH=IMAGE_WIDTH,ch=ch):\n",
        "def build_model_1(ch=32):\n",
        "    #inputs = Input((IMAGE_HEIGHT,IMAGE_WIDTH,ch))\n",
        "    inputs = Input((21,21,20))\n",
        "\n",
        "    conv0 = Conv2D(32,(2,2),padding='valid')(inputs)\n",
        "    print ('conv0',conv0.get_shape())\n",
        "    conv1 =  conv_block(ch)(conv0)\n",
        "    conv1 = conv_block(ch)(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
        "    print ('pool1',pool1.get_shape())\n",
        "    \n",
        "    conv2 =  conv_block(ch*2)(pool1)\n",
        "    conv2 = conv_block(ch*2)(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
        "    print ('poo12',pool2.get_shape())\n",
        "    \n",
        "    conv_test = Conv2D(64,(2,2),padding='valid')(pool2)\n",
        "    print ('conv_test',conv_test.get_shape())\n",
        "    \n",
        "    \n",
        "    \n",
        "    conv3 =  conv_block(ch*4)(conv_test)\n",
        "    conv3 = conv_block(ch*4)(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
        "    print ('poo13',pool3.get_shape())\n",
        "    \n",
        "    conv4 =  conv_block(ch*8)(pool3)\n",
        "    conv4 = conv_block(ch*8)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2,2))(conv4)\n",
        "    print ('poo14',pool4.get_shape())\n",
        "    \n",
        "    conv5 =  conv_block(ch*16)(pool4)\n",
        "    conv5 = conv_block(ch*16)(conv5)\n",
        "\n",
        "    \n",
        "    up6 = concatenate([UpSampling2D(size=(2,2))(conv5), conv4], axis=3)\n",
        "    conv6 = conv_block(ch*8)(up6)\n",
        "    conv6 = conv_block(ch*8)(conv6)\n",
        "    \n",
        "    up7 = concatenate ([UpSampling2D(size=(2,2))(conv6), conv3],  axis=3)\n",
        "    conv7 = conv_block(ch*4)(up7)\n",
        "    conv7 = conv_block(ch*4)(conv7)\n",
        "    print ('conv7',conv7.get_shape())\n",
        "    conv7 = Conv2DTranspose(ch*4,(2,2),padding='valid')(conv7)\n",
        "    print ('conv7',conv7.get_shape())\n",
        "    \n",
        "    up8 = concatenate([UpSampling2D(size=(2,2))(conv7), conv2], axis=3)\n",
        "    conv8 = conv_block(ch*2)(up8)\n",
        "    conv8 = conv_block(ch*2)(conv8)\n",
        "    \n",
        "    up9 = concatenate([UpSampling2D(size=(2,2))(conv8), conv1],  axis=3)\n",
        "    conv9 = conv_block(ch)(up9)\n",
        "    conv9 = conv_block(ch)(conv9)\n",
        "    \n",
        "    \n",
        "    \n",
        "    conv10 = Conv2D(1,(1,1))(conv9)\n",
        "    \n",
        "  \n",
        "    model = Model(inputs= inputs, outputs=conv10)\n",
        "    \n",
        "    return model\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LdlE4ee0dNdN",
        "colab_type": "code",
        "outputId": "691cb54f-508c-4092-ea44-e9570cc28aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1598
        }
      },
      "cell_type": "code",
      "source": [
        "model = build_model_1()\n",
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv0 (?, 20, 20, 32)\n",
            "pool1 (?, 10, 10, 32)\n",
            "poo12 (?, 5, 5, 64)\n",
            "conv_test (?, 4, 4, 64)\n",
            "poo13 (?, 2, 2, 128)\n",
            "poo14 (?, 1, 1, 256)\n",
            "conv7 (?, 4, 4, 128)\n",
            "conv7 (?, ?, ?, 128)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 21, 21, 20)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 20, 20, 32)   2592        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 20, 20, 32)   9248        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 20, 20, 32)   9248        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 10, 10, 32)   0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 10, 10, 64)   18496       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 10, 10, 64)   36928       conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 5, 5, 64)     0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 4, 4, 64)     16448       max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 4, 4, 128)    73856       conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 4, 4, 128)    147584      conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 2, 2, 128)    0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 2, 2, 256)    295168      max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 2, 2, 256)    590080      conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 1, 1, 256)    0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 1, 1, 512)    1180160     max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 1, 1, 512)    2359808     conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2D)  (None, 2, 2, 512)    0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 2, 2, 768)    0           up_sampling2d_5[0][0]            \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 2, 2, 256)    1769728     concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 2, 2, 256)    590080      conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2D)  (None, 4, 4, 256)    0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 4, 4, 384)    0           up_sampling2d_6[0][0]            \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 4, 4, 128)    442496      concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 4, 4, 128)    147584      conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 5, 5, 128)    65664       conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2D)  (None, 10, 10, 128)  0           conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 10, 10, 192)  0           up_sampling2d_7[0][0]            \n",
            "                                                                 conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 10, 10, 64)   110656      concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 10, 10, 64)   36928       conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_8 (UpSampling2D)  (None, 20, 20, 64)   0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 20, 20, 96)   0           up_sampling2d_8[0][0]            \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 20, 20, 32)   27680       concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 20, 20, 32)   9248        conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 20, 20, 1)    33          conv2d_41[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 7,939,713\n",
            "Trainable params: 7,939,713\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1SJ49HxDb3Ay",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def gram_matrix(x):\n",
        "  #features = K.batch_flatten(K.permute_dimensions(x[0], (2, 0, 1)))\n",
        "  x = K.batch_flatten(x)\n",
        "  gram = K.dot(x, K.transpose(x))\n",
        "  print(gram)\n",
        "  #return K.expand_dims(gram, axis=0)\n",
        "  return gram\n",
        "  \n",
        "\n",
        "\n",
        "def my_loss(y_true,y_pred):\n",
        "  loss_1 = K.mean(K.square(y_pred - y_true), axis=-1)\n",
        "  loss_2= K.categorical_crossentropy(y_true, y_pred)\n",
        "  #diff_2 = K.mean(K.square(gram_matrix(y_pred) - gram_matrix(y_true)), axis=-1)\n",
        "  \n",
        "  total_loss = loss_1 * 1. + 5. * loss_2\n",
        "  return total_loss\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RC4WaYhmi5KH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile (optimizer='Adam', loss = 'mse', metrics =['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCSzKqgub3A1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3831b8d8-5283-48b3-e9f0-0221cccf5789"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# train the model on the new data for a few epochs\n",
        "\n",
        "use_model= False\n",
        "if use_model:\n",
        "  \n",
        "    \n",
        "\n",
        "  if os.path.isfile(src+'water_first_try_2.h5'):\n",
        "\n",
        "    model = load_model(src+'water_first_try_2.h5')\n",
        "    print ('model laoded')\n",
        "  else:\n",
        "    print ('model not exist')\n",
        "      \n",
        "else:\n",
        "  print ('no use model')\n",
        "  \n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no use model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DM8u4t-gciIa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QlEexRjDb3A9",
        "colab_type": "code",
        "outputId": "50df8527-30b2-42d6-86f2-fc2136f11e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10718
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "checkpoint = ModelCheckpoint(filepath=src+'stage2_reproduce_water_first_try_2_prepro_ch32_check_nrmse.h5', monitor = 'val_loss', save_best_only=True, mode= 'auto')\n",
        "earlystop = EarlyStopping(patience=20)\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, verbose=1, mode='auto', min_lr=0.00001)\n",
        "callback_list = [checkpoint, earlystop]\n",
        "\n",
        "model.fit(x=train_x, y=train_y, batch_size=64, epochs=300, callbacks = callback_list, verbose=1, validation_split=0.2,  shuffle=True)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2688 samples, validate on 672 samples\n",
            "Epoch 1/300\n",
            "2688/2688 [==============================] - 6s 2ms/step - loss: 1.3310 - mean_absolute_error: 0.8913 - val_loss: 0.7983 - val_mean_absolute_error: 0.7127\n",
            "Epoch 2/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.6973 - mean_absolute_error: 0.6632 - val_loss: 0.5174 - val_mean_absolute_error: 0.5660\n",
            "Epoch 3/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.4788 - mean_absolute_error: 0.5431 - val_loss: 0.4058 - val_mean_absolute_error: 0.5008\n",
            "Epoch 4/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.3418 - mean_absolute_error: 0.4554 - val_loss: 0.2964 - val_mean_absolute_error: 0.4235\n",
            "Epoch 5/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.2832 - mean_absolute_error: 0.4143 - val_loss: 0.2521 - val_mean_absolute_error: 0.3926\n",
            "Epoch 6/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.2358 - mean_absolute_error: 0.3768 - val_loss: 0.2162 - val_mean_absolute_error: 0.3606\n",
            "Epoch 7/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.2119 - mean_absolute_error: 0.3574 - val_loss: 0.2067 - val_mean_absolute_error: 0.3507\n",
            "Epoch 8/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.1923 - mean_absolute_error: 0.3401 - val_loss: 0.1767 - val_mean_absolute_error: 0.3259\n",
            "Epoch 9/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.1736 - mean_absolute_error: 0.3229 - val_loss: 0.1642 - val_mean_absolute_error: 0.3127\n",
            "Epoch 10/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.1633 - mean_absolute_error: 0.3133 - val_loss: 0.1537 - val_mean_absolute_error: 0.3035\n",
            "Epoch 11/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.1492 - mean_absolute_error: 0.2988 - val_loss: 0.1472 - val_mean_absolute_error: 0.2950\n",
            "Epoch 12/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.1388 - mean_absolute_error: 0.2882 - val_loss: 0.1317 - val_mean_absolute_error: 0.2809\n",
            "Epoch 13/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.1343 - mean_absolute_error: 0.2835 - val_loss: 0.1328 - val_mean_absolute_error: 0.2833\n",
            "Epoch 14/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.1217 - mean_absolute_error: 0.2698 - val_loss: 0.1157 - val_mean_absolute_error: 0.2619\n",
            "Epoch 15/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.1152 - mean_absolute_error: 0.2624 - val_loss: 0.1143 - val_mean_absolute_error: 0.2622\n",
            "Epoch 16/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.1115 - mean_absolute_error: 0.2579 - val_loss: 0.1117 - val_mean_absolute_error: 0.2577\n",
            "Epoch 17/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.1041 - mean_absolute_error: 0.2492 - val_loss: 0.1027 - val_mean_absolute_error: 0.2456\n",
            "Epoch 18/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0995 - mean_absolute_error: 0.2434 - val_loss: 0.1012 - val_mean_absolute_error: 0.2452\n",
            "Epoch 19/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0974 - mean_absolute_error: 0.2410 - val_loss: 0.0992 - val_mean_absolute_error: 0.2426\n",
            "Epoch 20/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0909 - mean_absolute_error: 0.2326 - val_loss: 0.0959 - val_mean_absolute_error: 0.2375\n",
            "Epoch 21/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0882 - mean_absolute_error: 0.2287 - val_loss: 0.0927 - val_mean_absolute_error: 0.2330\n",
            "Epoch 22/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0873 - mean_absolute_error: 0.2280 - val_loss: 0.0862 - val_mean_absolute_error: 0.2252\n",
            "Epoch 23/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0808 - mean_absolute_error: 0.2189 - val_loss: 0.0852 - val_mean_absolute_error: 0.2249\n",
            "Epoch 24/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0790 - mean_absolute_error: 0.2165 - val_loss: 0.0844 - val_mean_absolute_error: 0.2226\n",
            "Epoch 25/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0777 - mean_absolute_error: 0.2148 - val_loss: 0.0808 - val_mean_absolute_error: 0.2162\n",
            "Epoch 26/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0763 - mean_absolute_error: 0.2127 - val_loss: 0.0828 - val_mean_absolute_error: 0.2182\n",
            "Epoch 27/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0732 - mean_absolute_error: 0.2082 - val_loss: 0.0797 - val_mean_absolute_error: 0.2172\n",
            "Epoch 28/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0732 - mean_absolute_error: 0.2087 - val_loss: 0.0774 - val_mean_absolute_error: 0.2120\n",
            "Epoch 29/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0686 - mean_absolute_error: 0.2014 - val_loss: 0.0749 - val_mean_absolute_error: 0.2086\n",
            "Epoch 30/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0685 - mean_absolute_error: 0.2017 - val_loss: 0.0781 - val_mean_absolute_error: 0.2109\n",
            "Epoch 31/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0683 - mean_absolute_error: 0.2014 - val_loss: 0.0728 - val_mean_absolute_error: 0.2061\n",
            "Epoch 32/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0630 - mean_absolute_error: 0.1931 - val_loss: 0.0723 - val_mean_absolute_error: 0.2049\n",
            "Epoch 33/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0621 - mean_absolute_error: 0.1917 - val_loss: 0.0714 - val_mean_absolute_error: 0.2022\n",
            "Epoch 34/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0605 - mean_absolute_error: 0.1894 - val_loss: 0.0690 - val_mean_absolute_error: 0.1987\n",
            "Epoch 35/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0591 - mean_absolute_error: 0.1872 - val_loss: 0.0705 - val_mean_absolute_error: 0.2010\n",
            "Epoch 36/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0569 - mean_absolute_error: 0.1836 - val_loss: 0.0688 - val_mean_absolute_error: 0.1978\n",
            "Epoch 37/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0576 - mean_absolute_error: 0.1848 - val_loss: 0.0686 - val_mean_absolute_error: 0.1981\n",
            "Epoch 38/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0555 - mean_absolute_error: 0.1813 - val_loss: 0.0694 - val_mean_absolute_error: 0.1981\n",
            "Epoch 39/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0549 - mean_absolute_error: 0.1803 - val_loss: 0.0656 - val_mean_absolute_error: 0.1928\n",
            "Epoch 40/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0563 - mean_absolute_error: 0.1834 - val_loss: 0.0675 - val_mean_absolute_error: 0.1967\n",
            "Epoch 41/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0517 - mean_absolute_error: 0.1748 - val_loss: 0.0646 - val_mean_absolute_error: 0.1917\n",
            "Epoch 42/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0527 - mean_absolute_error: 0.1769 - val_loss: 0.0675 - val_mean_absolute_error: 0.1984\n",
            "Epoch 43/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0501 - mean_absolute_error: 0.1725 - val_loss: 0.0618 - val_mean_absolute_error: 0.1873\n",
            "Epoch 44/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0481 - mean_absolute_error: 0.1689 - val_loss: 0.0621 - val_mean_absolute_error: 0.1868\n",
            "Epoch 45/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0479 - mean_absolute_error: 0.1683 - val_loss: 0.0598 - val_mean_absolute_error: 0.1850\n",
            "Epoch 46/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0475 - mean_absolute_error: 0.1676 - val_loss: 0.0602 - val_mean_absolute_error: 0.1847\n",
            "Epoch 47/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0465 - mean_absolute_error: 0.1662 - val_loss: 0.0638 - val_mean_absolute_error: 0.1909\n",
            "Epoch 48/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0465 - mean_absolute_error: 0.1664 - val_loss: 0.0589 - val_mean_absolute_error: 0.1829\n",
            "Epoch 49/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0442 - mean_absolute_error: 0.1617 - val_loss: 0.0587 - val_mean_absolute_error: 0.1820\n",
            "Epoch 50/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0465 - mean_absolute_error: 0.1659 - val_loss: 0.0608 - val_mean_absolute_error: 0.1853\n",
            "Epoch 51/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0427 - mean_absolute_error: 0.1589 - val_loss: 0.0569 - val_mean_absolute_error: 0.1794\n",
            "Epoch 52/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0412 - mean_absolute_error: 0.1561 - val_loss: 0.0557 - val_mean_absolute_error: 0.1768\n",
            "Epoch 53/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0434 - mean_absolute_error: 0.1605 - val_loss: 0.0567 - val_mean_absolute_error: 0.1783\n",
            "Epoch 54/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0404 - mean_absolute_error: 0.1545 - val_loss: 0.0552 - val_mean_absolute_error: 0.1756\n",
            "Epoch 55/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0398 - mean_absolute_error: 0.1536 - val_loss: 0.0559 - val_mean_absolute_error: 0.1760\n",
            "Epoch 56/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0421 - mean_absolute_error: 0.1576 - val_loss: 0.0672 - val_mean_absolute_error: 0.1990\n",
            "Epoch 57/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0411 - mean_absolute_error: 0.1558 - val_loss: 0.0540 - val_mean_absolute_error: 0.1732\n",
            "Epoch 58/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0370 - mean_absolute_error: 0.1478 - val_loss: 0.0552 - val_mean_absolute_error: 0.1750\n",
            "Epoch 59/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0373 - mean_absolute_error: 0.1483 - val_loss: 0.0589 - val_mean_absolute_error: 0.1825\n",
            "Epoch 60/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0378 - mean_absolute_error: 0.1493 - val_loss: 0.0564 - val_mean_absolute_error: 0.1780\n",
            "Epoch 61/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0386 - mean_absolute_error: 0.1512 - val_loss: 0.0539 - val_mean_absolute_error: 0.1728\n",
            "Epoch 62/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0377 - mean_absolute_error: 0.1493 - val_loss: 0.0524 - val_mean_absolute_error: 0.1703\n",
            "Epoch 63/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0358 - mean_absolute_error: 0.1453 - val_loss: 0.0549 - val_mean_absolute_error: 0.1753\n",
            "Epoch 64/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0341 - mean_absolute_error: 0.1419 - val_loss: 0.0522 - val_mean_absolute_error: 0.1706\n",
            "Epoch 65/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0362 - mean_absolute_error: 0.1465 - val_loss: 0.0652 - val_mean_absolute_error: 0.1939\n",
            "Epoch 66/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0372 - mean_absolute_error: 0.1484 - val_loss: 0.0516 - val_mean_absolute_error: 0.1688\n",
            "Epoch 67/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0320 - mean_absolute_error: 0.1373 - val_loss: 0.0529 - val_mean_absolute_error: 0.1708\n",
            "Epoch 68/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0340 - mean_absolute_error: 0.1418 - val_loss: 0.0521 - val_mean_absolute_error: 0.1696\n",
            "Epoch 69/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0321 - mean_absolute_error: 0.1374 - val_loss: 0.0510 - val_mean_absolute_error: 0.1663\n",
            "Epoch 70/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0310 - mean_absolute_error: 0.1355 - val_loss: 0.0506 - val_mean_absolute_error: 0.1665\n",
            "Epoch 71/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0302 - mean_absolute_error: 0.1334 - val_loss: 0.0504 - val_mean_absolute_error: 0.1656\n",
            "Epoch 72/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0309 - mean_absolute_error: 0.1352 - val_loss: 0.0527 - val_mean_absolute_error: 0.1710\n",
            "Epoch 73/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0313 - mean_absolute_error: 0.1358 - val_loss: 0.0535 - val_mean_absolute_error: 0.1746\n",
            "Epoch 74/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0306 - mean_absolute_error: 0.1345 - val_loss: 0.0500 - val_mean_absolute_error: 0.1653\n",
            "Epoch 75/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0291 - mean_absolute_error: 0.1310 - val_loss: 0.0491 - val_mean_absolute_error: 0.1641\n",
            "Epoch 76/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0297 - mean_absolute_error: 0.1323 - val_loss: 0.0505 - val_mean_absolute_error: 0.1646\n",
            "Epoch 77/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0299 - mean_absolute_error: 0.1330 - val_loss: 0.0512 - val_mean_absolute_error: 0.1690\n",
            "Epoch 78/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0288 - mean_absolute_error: 0.1305 - val_loss: 0.0501 - val_mean_absolute_error: 0.1653\n",
            "Epoch 79/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0285 - mean_absolute_error: 0.1294 - val_loss: 0.0538 - val_mean_absolute_error: 0.1728\n",
            "Epoch 80/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0292 - mean_absolute_error: 0.1315 - val_loss: 0.0477 - val_mean_absolute_error: 0.1612\n",
            "Epoch 81/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0270 - mean_absolute_error: 0.1261 - val_loss: 0.0487 - val_mean_absolute_error: 0.1638\n",
            "Epoch 82/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0273 - mean_absolute_error: 0.1268 - val_loss: 0.0506 - val_mean_absolute_error: 0.1684\n",
            "Epoch 83/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0280 - mean_absolute_error: 0.1286 - val_loss: 0.0474 - val_mean_absolute_error: 0.1600\n",
            "Epoch 84/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0256 - mean_absolute_error: 0.1228 - val_loss: 0.0476 - val_mean_absolute_error: 0.1613\n",
            "Epoch 85/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0273 - mean_absolute_error: 0.1277 - val_loss: 0.0499 - val_mean_absolute_error: 0.1653\n",
            "Epoch 86/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0266 - mean_absolute_error: 0.1257 - val_loss: 0.0476 - val_mean_absolute_error: 0.1599\n",
            "Epoch 87/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0248 - mean_absolute_error: 0.1208 - val_loss: 0.0463 - val_mean_absolute_error: 0.1575\n",
            "Epoch 88/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0245 - mean_absolute_error: 0.1203 - val_loss: 0.0485 - val_mean_absolute_error: 0.1607\n",
            "Epoch 89/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0278 - mean_absolute_error: 0.1280 - val_loss: 0.0477 - val_mean_absolute_error: 0.1596\n",
            "Epoch 90/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0254 - mean_absolute_error: 0.1223 - val_loss: 0.0467 - val_mean_absolute_error: 0.1587\n",
            "Epoch 91/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0236 - mean_absolute_error: 0.1177 - val_loss: 0.0459 - val_mean_absolute_error: 0.1568\n",
            "Epoch 92/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0245 - mean_absolute_error: 0.1206 - val_loss: 0.0464 - val_mean_absolute_error: 0.1581\n",
            "Epoch 93/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0242 - mean_absolute_error: 0.1197 - val_loss: 0.0464 - val_mean_absolute_error: 0.1570\n",
            "Epoch 94/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0239 - mean_absolute_error: 0.1189 - val_loss: 0.0467 - val_mean_absolute_error: 0.1599\n",
            "Epoch 95/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0231 - mean_absolute_error: 0.1166 - val_loss: 0.0458 - val_mean_absolute_error: 0.1570\n",
            "Epoch 96/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0225 - mean_absolute_error: 0.1153 - val_loss: 0.0443 - val_mean_absolute_error: 0.1541\n",
            "Epoch 97/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0237 - mean_absolute_error: 0.1185 - val_loss: 0.0459 - val_mean_absolute_error: 0.1575\n",
            "Epoch 98/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0233 - mean_absolute_error: 0.1169 - val_loss: 0.0449 - val_mean_absolute_error: 0.1548\n",
            "Epoch 99/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0244 - mean_absolute_error: 0.1205 - val_loss: 0.0467 - val_mean_absolute_error: 0.1591\n",
            "Epoch 100/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0224 - mean_absolute_error: 0.1147 - val_loss: 0.0451 - val_mean_absolute_error: 0.1542\n",
            "Epoch 101/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0214 - mean_absolute_error: 0.1122 - val_loss: 0.0439 - val_mean_absolute_error: 0.1525\n",
            "Epoch 102/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0208 - mean_absolute_error: 0.1109 - val_loss: 0.0458 - val_mean_absolute_error: 0.1581\n",
            "Epoch 103/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0214 - mean_absolute_error: 0.1128 - val_loss: 0.0447 - val_mean_absolute_error: 0.1538\n",
            "Epoch 104/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0204 - mean_absolute_error: 0.1097 - val_loss: 0.0435 - val_mean_absolute_error: 0.1523\n",
            "Epoch 105/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0199 - mean_absolute_error: 0.1082 - val_loss: 0.0438 - val_mean_absolute_error: 0.1537\n",
            "Epoch 106/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0211 - mean_absolute_error: 0.1118 - val_loss: 0.0433 - val_mean_absolute_error: 0.1512\n",
            "Epoch 107/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0210 - mean_absolute_error: 0.1113 - val_loss: 0.0442 - val_mean_absolute_error: 0.1540\n",
            "Epoch 108/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0201 - mean_absolute_error: 0.1088 - val_loss: 0.0462 - val_mean_absolute_error: 0.1573\n",
            "Epoch 109/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0206 - mean_absolute_error: 0.1101 - val_loss: 0.0445 - val_mean_absolute_error: 0.1527\n",
            "Epoch 110/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0204 - mean_absolute_error: 0.1101 - val_loss: 0.0471 - val_mean_absolute_error: 0.1575\n",
            "Epoch 111/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0197 - mean_absolute_error: 0.1079 - val_loss: 0.0451 - val_mean_absolute_error: 0.1547\n",
            "Epoch 112/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0213 - mean_absolute_error: 0.1123 - val_loss: 0.0464 - val_mean_absolute_error: 0.1561\n",
            "Epoch 113/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0208 - mean_absolute_error: 0.1108 - val_loss: 0.0426 - val_mean_absolute_error: 0.1498\n",
            "Epoch 114/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0189 - mean_absolute_error: 0.1055 - val_loss: 0.0465 - val_mean_absolute_error: 0.1584\n",
            "Epoch 115/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0180 - mean_absolute_error: 0.1029 - val_loss: 0.0426 - val_mean_absolute_error: 0.1495\n",
            "Epoch 116/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0198 - mean_absolute_error: 0.1084 - val_loss: 0.0423 - val_mean_absolute_error: 0.1489\n",
            "Epoch 117/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0178 - mean_absolute_error: 0.1023 - val_loss: 0.0431 - val_mean_absolute_error: 0.1515\n",
            "Epoch 118/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0180 - mean_absolute_error: 0.1034 - val_loss: 0.0424 - val_mean_absolute_error: 0.1494\n",
            "Epoch 119/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0181 - mean_absolute_error: 0.1035 - val_loss: 0.0415 - val_mean_absolute_error: 0.1480\n",
            "Epoch 120/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0186 - mean_absolute_error: 0.1048 - val_loss: 0.0459 - val_mean_absolute_error: 0.1563\n",
            "Epoch 121/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0189 - mean_absolute_error: 0.1059 - val_loss: 0.0440 - val_mean_absolute_error: 0.1515\n",
            "Epoch 122/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0194 - mean_absolute_error: 0.1069 - val_loss: 0.0428 - val_mean_absolute_error: 0.1501\n",
            "Epoch 123/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0183 - mean_absolute_error: 0.1036 - val_loss: 0.0416 - val_mean_absolute_error: 0.1477\n",
            "Epoch 124/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0180 - mean_absolute_error: 0.1028 - val_loss: 0.0420 - val_mean_absolute_error: 0.1481\n",
            "Epoch 125/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0201 - mean_absolute_error: 0.1098 - val_loss: 0.0452 - val_mean_absolute_error: 0.1543\n",
            "Epoch 126/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0180 - mean_absolute_error: 0.1030 - val_loss: 0.0427 - val_mean_absolute_error: 0.1494\n",
            "Epoch 127/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0169 - mean_absolute_error: 0.0997 - val_loss: 0.0408 - val_mean_absolute_error: 0.1455\n",
            "Epoch 128/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0166 - mean_absolute_error: 0.0991 - val_loss: 0.0428 - val_mean_absolute_error: 0.1509\n",
            "Epoch 129/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0167 - mean_absolute_error: 0.0998 - val_loss: 0.0411 - val_mean_absolute_error: 0.1464\n",
            "Epoch 130/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0162 - mean_absolute_error: 0.0978 - val_loss: 0.0416 - val_mean_absolute_error: 0.1472\n",
            "Epoch 131/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0178 - mean_absolute_error: 0.1029 - val_loss: 0.0429 - val_mean_absolute_error: 0.1526\n",
            "Epoch 132/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0165 - mean_absolute_error: 0.0985 - val_loss: 0.0409 - val_mean_absolute_error: 0.1457\n",
            "Epoch 133/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0153 - mean_absolute_error: 0.0948 - val_loss: 0.0410 - val_mean_absolute_error: 0.1468\n",
            "Epoch 134/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0164 - mean_absolute_error: 0.0986 - val_loss: 0.0418 - val_mean_absolute_error: 0.1470\n",
            "Epoch 135/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0164 - mean_absolute_error: 0.0982 - val_loss: 0.0439 - val_mean_absolute_error: 0.1516\n",
            "Epoch 136/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0186 - mean_absolute_error: 0.1052 - val_loss: 0.0425 - val_mean_absolute_error: 0.1497\n",
            "Epoch 137/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0162 - mean_absolute_error: 0.0980 - val_loss: 0.0405 - val_mean_absolute_error: 0.1456\n",
            "Epoch 138/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0144 - mean_absolute_error: 0.0920 - val_loss: 0.0406 - val_mean_absolute_error: 0.1452\n",
            "Epoch 139/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0169 - mean_absolute_error: 0.1008 - val_loss: 0.0411 - val_mean_absolute_error: 0.1474\n",
            "Epoch 140/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0144 - mean_absolute_error: 0.0920 - val_loss: 0.0398 - val_mean_absolute_error: 0.1433\n",
            "Epoch 141/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0145 - mean_absolute_error: 0.0926 - val_loss: 0.0417 - val_mean_absolute_error: 0.1490\n",
            "Epoch 142/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0153 - mean_absolute_error: 0.0949 - val_loss: 0.0400 - val_mean_absolute_error: 0.1447\n",
            "Epoch 143/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0149 - mean_absolute_error: 0.0942 - val_loss: 0.0417 - val_mean_absolute_error: 0.1498\n",
            "Epoch 144/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0148 - mean_absolute_error: 0.0936 - val_loss: 0.0402 - val_mean_absolute_error: 0.1442\n",
            "Epoch 145/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0143 - mean_absolute_error: 0.0918 - val_loss: 0.0396 - val_mean_absolute_error: 0.1433\n",
            "Epoch 146/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0155 - mean_absolute_error: 0.0959 - val_loss: 0.0407 - val_mean_absolute_error: 0.1454\n",
            "Epoch 147/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0144 - mean_absolute_error: 0.0919 - val_loss: 0.0397 - val_mean_absolute_error: 0.1428\n",
            "Epoch 148/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0142 - mean_absolute_error: 0.0917 - val_loss: 0.0404 - val_mean_absolute_error: 0.1441\n",
            "Epoch 149/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0155 - mean_absolute_error: 0.0961 - val_loss: 0.0397 - val_mean_absolute_error: 0.1435\n",
            "Epoch 150/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0158 - mean_absolute_error: 0.0965 - val_loss: 0.0395 - val_mean_absolute_error: 0.1422\n",
            "Epoch 151/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0142 - mean_absolute_error: 0.0915 - val_loss: 0.0399 - val_mean_absolute_error: 0.1452\n",
            "Epoch 152/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0140 - mean_absolute_error: 0.0912 - val_loss: 0.0421 - val_mean_absolute_error: 0.1491\n",
            "Epoch 153/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0140 - mean_absolute_error: 0.0910 - val_loss: 0.0397 - val_mean_absolute_error: 0.1419\n",
            "Epoch 154/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0135 - mean_absolute_error: 0.0890 - val_loss: 0.0415 - val_mean_absolute_error: 0.1491\n",
            "Epoch 155/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0146 - mean_absolute_error: 0.0930 - val_loss: 0.0426 - val_mean_absolute_error: 0.1510\n",
            "Epoch 156/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0151 - mean_absolute_error: 0.0947 - val_loss: 0.0416 - val_mean_absolute_error: 0.1484\n",
            "Epoch 157/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0134 - mean_absolute_error: 0.0890 - val_loss: 0.0384 - val_mean_absolute_error: 0.1406\n",
            "Epoch 158/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0126 - mean_absolute_error: 0.0867 - val_loss: 0.0390 - val_mean_absolute_error: 0.1414\n",
            "Epoch 159/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0126 - mean_absolute_error: 0.0862 - val_loss: 0.0482 - val_mean_absolute_error: 0.1638\n",
            "Epoch 160/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0177 - mean_absolute_error: 0.1024 - val_loss: 0.0394 - val_mean_absolute_error: 0.1419\n",
            "Epoch 161/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0138 - mean_absolute_error: 0.0901 - val_loss: 0.0383 - val_mean_absolute_error: 0.1404\n",
            "Epoch 162/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0129 - mean_absolute_error: 0.0874 - val_loss: 0.0380 - val_mean_absolute_error: 0.1400\n",
            "Epoch 163/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0138 - mean_absolute_error: 0.0904 - val_loss: 0.0386 - val_mean_absolute_error: 0.1403\n",
            "Epoch 164/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0129 - mean_absolute_error: 0.0874 - val_loss: 0.0381 - val_mean_absolute_error: 0.1394\n",
            "Epoch 165/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0122 - mean_absolute_error: 0.0849 - val_loss: 0.0458 - val_mean_absolute_error: 0.1600\n",
            "Epoch 166/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0138 - mean_absolute_error: 0.0908 - val_loss: 0.0402 - val_mean_absolute_error: 0.1449\n",
            "Epoch 167/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0130 - mean_absolute_error: 0.0881 - val_loss: 0.0387 - val_mean_absolute_error: 0.1406\n",
            "Epoch 168/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0128 - mean_absolute_error: 0.0872 - val_loss: 0.0385 - val_mean_absolute_error: 0.1402\n",
            "Epoch 169/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0121 - mean_absolute_error: 0.0846 - val_loss: 0.0390 - val_mean_absolute_error: 0.1431\n",
            "Epoch 170/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0135 - mean_absolute_error: 0.0892 - val_loss: 0.0419 - val_mean_absolute_error: 0.1442\n",
            "Epoch 171/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0137 - mean_absolute_error: 0.0900 - val_loss: 0.0392 - val_mean_absolute_error: 0.1410\n",
            "Epoch 172/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0132 - mean_absolute_error: 0.0884 - val_loss: 0.0385 - val_mean_absolute_error: 0.1405\n",
            "Epoch 173/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0134 - mean_absolute_error: 0.0888 - val_loss: 0.0381 - val_mean_absolute_error: 0.1389\n",
            "Epoch 174/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0121 - mean_absolute_error: 0.0848 - val_loss: 0.0383 - val_mean_absolute_error: 0.1398\n",
            "Epoch 175/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0120 - mean_absolute_error: 0.0846 - val_loss: 0.0381 - val_mean_absolute_error: 0.1400\n",
            "Epoch 176/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0112 - mean_absolute_error: 0.0815 - val_loss: 0.0372 - val_mean_absolute_error: 0.1372\n",
            "Epoch 177/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0113 - mean_absolute_error: 0.0822 - val_loss: 0.0372 - val_mean_absolute_error: 0.1373\n",
            "Epoch 178/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0116 - mean_absolute_error: 0.0830 - val_loss: 0.0376 - val_mean_absolute_error: 0.1387\n",
            "Epoch 179/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0117 - mean_absolute_error: 0.0836 - val_loss: 0.0375 - val_mean_absolute_error: 0.1380\n",
            "Epoch 180/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0115 - mean_absolute_error: 0.0826 - val_loss: 0.0390 - val_mean_absolute_error: 0.1415\n",
            "Epoch 181/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0111 - mean_absolute_error: 0.0812 - val_loss: 0.0379 - val_mean_absolute_error: 0.1403\n",
            "Epoch 182/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0120 - mean_absolute_error: 0.0845 - val_loss: 0.0400 - val_mean_absolute_error: 0.1452\n",
            "Epoch 183/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0120 - mean_absolute_error: 0.0844 - val_loss: 0.0384 - val_mean_absolute_error: 0.1390\n",
            "Epoch 184/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0118 - mean_absolute_error: 0.0839 - val_loss: 0.0377 - val_mean_absolute_error: 0.1384\n",
            "Epoch 185/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0118 - mean_absolute_error: 0.0836 - val_loss: 0.0379 - val_mean_absolute_error: 0.1377\n",
            "Epoch 186/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0115 - mean_absolute_error: 0.0829 - val_loss: 0.0374 - val_mean_absolute_error: 0.1379\n",
            "Epoch 187/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0130 - mean_absolute_error: 0.0885 - val_loss: 0.0390 - val_mean_absolute_error: 0.1405\n",
            "Epoch 188/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0115 - mean_absolute_error: 0.0824 - val_loss: 0.0387 - val_mean_absolute_error: 0.1400\n",
            "Epoch 189/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0124 - mean_absolute_error: 0.0862 - val_loss: 0.0381 - val_mean_absolute_error: 0.1387\n",
            "Epoch 190/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0105 - mean_absolute_error: 0.0789 - val_loss: 0.0363 - val_mean_absolute_error: 0.1357\n",
            "Epoch 191/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0099 - mean_absolute_error: 0.0764 - val_loss: 0.0362 - val_mean_absolute_error: 0.1351\n",
            "Epoch 192/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0097 - mean_absolute_error: 0.0757 - val_loss: 0.0361 - val_mean_absolute_error: 0.1351\n",
            "Epoch 193/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0115 - mean_absolute_error: 0.0831 - val_loss: 0.0388 - val_mean_absolute_error: 0.1403\n",
            "Epoch 194/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0114 - mean_absolute_error: 0.0819 - val_loss: 0.0379 - val_mean_absolute_error: 0.1384\n",
            "Epoch 195/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0107 - mean_absolute_error: 0.0799 - val_loss: 0.0361 - val_mean_absolute_error: 0.1350\n",
            "Epoch 196/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0107 - mean_absolute_error: 0.0797 - val_loss: 0.0361 - val_mean_absolute_error: 0.1349\n",
            "Epoch 197/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0106 - mean_absolute_error: 0.0796 - val_loss: 0.0365 - val_mean_absolute_error: 0.1355\n",
            "Epoch 198/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0099 - mean_absolute_error: 0.0767 - val_loss: 0.0363 - val_mean_absolute_error: 0.1351\n",
            "Epoch 199/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0106 - mean_absolute_error: 0.0798 - val_loss: 0.0374 - val_mean_absolute_error: 0.1376\n",
            "Epoch 200/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0105 - mean_absolute_error: 0.0792 - val_loss: 0.0372 - val_mean_absolute_error: 0.1378\n",
            "Epoch 201/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0115 - mean_absolute_error: 0.0829 - val_loss: 0.0370 - val_mean_absolute_error: 0.1368\n",
            "Epoch 202/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0102 - mean_absolute_error: 0.0775 - val_loss: 0.0362 - val_mean_absolute_error: 0.1353\n",
            "Epoch 203/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0106 - mean_absolute_error: 0.0794 - val_loss: 0.0402 - val_mean_absolute_error: 0.1431\n",
            "Epoch 204/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0113 - mean_absolute_error: 0.0821 - val_loss: 0.0369 - val_mean_absolute_error: 0.1357\n",
            "Epoch 205/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0109 - mean_absolute_error: 0.0810 - val_loss: 0.0370 - val_mean_absolute_error: 0.1379\n",
            "Epoch 206/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0098 - mean_absolute_error: 0.0761 - val_loss: 0.0390 - val_mean_absolute_error: 0.1392\n",
            "Epoch 207/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0092 - mean_absolute_error: 0.0738 - val_loss: 0.0359 - val_mean_absolute_error: 0.1340\n",
            "Epoch 208/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0096 - mean_absolute_error: 0.0757 - val_loss: 0.0367 - val_mean_absolute_error: 0.1364\n",
            "Epoch 209/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0115 - mean_absolute_error: 0.0830 - val_loss: 0.0398 - val_mean_absolute_error: 0.1438\n",
            "Epoch 210/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0107 - mean_absolute_error: 0.0797 - val_loss: 0.0357 - val_mean_absolute_error: 0.1337\n",
            "Epoch 211/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0101 - mean_absolute_error: 0.0774 - val_loss: 0.0361 - val_mean_absolute_error: 0.1348\n",
            "Epoch 212/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0103 - mean_absolute_error: 0.0784 - val_loss: 0.0398 - val_mean_absolute_error: 0.1453\n",
            "Epoch 213/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0109 - mean_absolute_error: 0.0808 - val_loss: 0.0364 - val_mean_absolute_error: 0.1353\n",
            "Epoch 214/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0105 - mean_absolute_error: 0.0791 - val_loss: 0.0371 - val_mean_absolute_error: 0.1368\n",
            "Epoch 215/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0103 - mean_absolute_error: 0.0781 - val_loss: 0.0386 - val_mean_absolute_error: 0.1424\n",
            "Epoch 216/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0110 - mean_absolute_error: 0.0807 - val_loss: 0.0371 - val_mean_absolute_error: 0.1363\n",
            "Epoch 217/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0099 - mean_absolute_error: 0.0767 - val_loss: 0.0362 - val_mean_absolute_error: 0.1351\n",
            "Epoch 218/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0097 - mean_absolute_error: 0.0763 - val_loss: 0.0368 - val_mean_absolute_error: 0.1356\n",
            "Epoch 219/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0125 - mean_absolute_error: 0.0867 - val_loss: 0.0386 - val_mean_absolute_error: 0.1417\n",
            "Epoch 220/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0106 - mean_absolute_error: 0.0790 - val_loss: 0.0355 - val_mean_absolute_error: 0.1330\n",
            "Epoch 221/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0092 - mean_absolute_error: 0.0737 - val_loss: 0.0353 - val_mean_absolute_error: 0.1332\n",
            "Epoch 222/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0085 - mean_absolute_error: 0.0709 - val_loss: 0.0350 - val_mean_absolute_error: 0.1321\n",
            "Epoch 223/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0082 - mean_absolute_error: 0.0699 - val_loss: 0.0361 - val_mean_absolute_error: 0.1351\n",
            "Epoch 224/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0083 - mean_absolute_error: 0.0704 - val_loss: 0.0352 - val_mean_absolute_error: 0.1324\n",
            "Epoch 225/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0082 - mean_absolute_error: 0.0700 - val_loss: 0.0346 - val_mean_absolute_error: 0.1312\n",
            "Epoch 226/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0079 - mean_absolute_error: 0.0688 - val_loss: 0.0347 - val_mean_absolute_error: 0.1317\n",
            "Epoch 227/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0090 - mean_absolute_error: 0.0735 - val_loss: 0.0391 - val_mean_absolute_error: 0.1466\n",
            "Epoch 228/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0095 - mean_absolute_error: 0.0755 - val_loss: 0.0353 - val_mean_absolute_error: 0.1331\n",
            "Epoch 229/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0090 - mean_absolute_error: 0.0732 - val_loss: 0.0392 - val_mean_absolute_error: 0.1411\n",
            "Epoch 230/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0098 - mean_absolute_error: 0.0762 - val_loss: 0.0351 - val_mean_absolute_error: 0.1328\n",
            "Epoch 231/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0086 - mean_absolute_error: 0.0715 - val_loss: 0.0346 - val_mean_absolute_error: 0.1315\n",
            "Epoch 232/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0096 - mean_absolute_error: 0.0759 - val_loss: 0.0354 - val_mean_absolute_error: 0.1335\n",
            "Epoch 233/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0091 - mean_absolute_error: 0.0737 - val_loss: 0.0356 - val_mean_absolute_error: 0.1327\n",
            "Epoch 234/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0086 - mean_absolute_error: 0.0714 - val_loss: 0.0365 - val_mean_absolute_error: 0.1359\n",
            "Epoch 235/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0089 - mean_absolute_error: 0.0731 - val_loss: 0.0346 - val_mean_absolute_error: 0.1318\n",
            "Epoch 236/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0082 - mean_absolute_error: 0.0701 - val_loss: 0.0357 - val_mean_absolute_error: 0.1360\n",
            "Epoch 237/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0092 - mean_absolute_error: 0.0741 - val_loss: 0.0352 - val_mean_absolute_error: 0.1321\n",
            "Epoch 238/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0097 - mean_absolute_error: 0.0760 - val_loss: 0.0368 - val_mean_absolute_error: 0.1366\n",
            "Epoch 239/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0089 - mean_absolute_error: 0.0731 - val_loss: 0.0363 - val_mean_absolute_error: 0.1367\n",
            "Epoch 240/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0090 - mean_absolute_error: 0.0736 - val_loss: 0.0345 - val_mean_absolute_error: 0.1314\n",
            "Epoch 241/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0078 - mean_absolute_error: 0.0682 - val_loss: 0.0346 - val_mean_absolute_error: 0.1315\n",
            "Epoch 242/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0088 - mean_absolute_error: 0.0730 - val_loss: 0.0354 - val_mean_absolute_error: 0.1325\n",
            "Epoch 243/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0083 - mean_absolute_error: 0.0704 - val_loss: 0.0352 - val_mean_absolute_error: 0.1345\n",
            "Epoch 244/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0085 - mean_absolute_error: 0.0713 - val_loss: 0.0365 - val_mean_absolute_error: 0.1403\n",
            "Epoch 245/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0088 - mean_absolute_error: 0.0724 - val_loss: 0.0353 - val_mean_absolute_error: 0.1333\n",
            "Epoch 246/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0090 - mean_absolute_error: 0.0735 - val_loss: 0.0355 - val_mean_absolute_error: 0.1333\n",
            "Epoch 247/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0095 - mean_absolute_error: 0.0756 - val_loss: 0.0345 - val_mean_absolute_error: 0.1310\n",
            "Epoch 248/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0094 - mean_absolute_error: 0.0746 - val_loss: 0.0360 - val_mean_absolute_error: 0.1333\n",
            "Epoch 249/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0085 - mean_absolute_error: 0.0709 - val_loss: 0.0360 - val_mean_absolute_error: 0.1345\n",
            "Epoch 250/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0089 - mean_absolute_error: 0.0732 - val_loss: 0.0352 - val_mean_absolute_error: 0.1333\n",
            "Epoch 251/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0081 - mean_absolute_error: 0.0693 - val_loss: 0.0357 - val_mean_absolute_error: 0.1339\n",
            "Epoch 252/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0091 - mean_absolute_error: 0.0741 - val_loss: 0.0354 - val_mean_absolute_error: 0.1336\n",
            "Epoch 253/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0075 - mean_absolute_error: 0.0667 - val_loss: 0.0337 - val_mean_absolute_error: 0.1300\n",
            "Epoch 254/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0077 - mean_absolute_error: 0.0679 - val_loss: 0.0345 - val_mean_absolute_error: 0.1309\n",
            "Epoch 255/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0071 - mean_absolute_error: 0.0653 - val_loss: 0.0341 - val_mean_absolute_error: 0.1314\n",
            "Epoch 256/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0082 - mean_absolute_error: 0.0708 - val_loss: 0.0355 - val_mean_absolute_error: 0.1339\n",
            "Epoch 257/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0073 - mean_absolute_error: 0.0660 - val_loss: 0.0337 - val_mean_absolute_error: 0.1297\n",
            "Epoch 258/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0078 - mean_absolute_error: 0.0684 - val_loss: 0.0359 - val_mean_absolute_error: 0.1345\n",
            "Epoch 259/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0079 - mean_absolute_error: 0.0689 - val_loss: 0.0345 - val_mean_absolute_error: 0.1308\n",
            "Epoch 260/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0078 - mean_absolute_error: 0.0682 - val_loss: 0.0352 - val_mean_absolute_error: 0.1330\n",
            "Epoch 261/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0077 - mean_absolute_error: 0.0676 - val_loss: 0.0341 - val_mean_absolute_error: 0.1303\n",
            "Epoch 262/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0086 - mean_absolute_error: 0.0717 - val_loss: 0.0364 - val_mean_absolute_error: 0.1355\n",
            "Epoch 263/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0082 - mean_absolute_error: 0.0703 - val_loss: 0.0348 - val_mean_absolute_error: 0.1314\n",
            "Epoch 264/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0079 - mean_absolute_error: 0.0686 - val_loss: 0.0356 - val_mean_absolute_error: 0.1337\n",
            "Epoch 265/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0079 - mean_absolute_error: 0.0688 - val_loss: 0.0345 - val_mean_absolute_error: 0.1307\n",
            "Epoch 266/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0085 - mean_absolute_error: 0.0713 - val_loss: 0.0351 - val_mean_absolute_error: 0.1321\n",
            "Epoch 267/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0083 - mean_absolute_error: 0.0703 - val_loss: 0.0346 - val_mean_absolute_error: 0.1321\n",
            "Epoch 268/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0079 - mean_absolute_error: 0.0690 - val_loss: 0.0340 - val_mean_absolute_error: 0.1300\n",
            "Epoch 269/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0086 - mean_absolute_error: 0.0720 - val_loss: 0.0343 - val_mean_absolute_error: 0.1307\n",
            "Epoch 270/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0081 - mean_absolute_error: 0.0697 - val_loss: 0.0356 - val_mean_absolute_error: 0.1347\n",
            "Epoch 271/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0080 - mean_absolute_error: 0.0692 - val_loss: 0.0344 - val_mean_absolute_error: 0.1323\n",
            "Epoch 272/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0076 - mean_absolute_error: 0.0677 - val_loss: 0.0376 - val_mean_absolute_error: 0.1374\n",
            "Epoch 273/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0088 - mean_absolute_error: 0.0728 - val_loss: 0.0347 - val_mean_absolute_error: 0.1333\n",
            "Epoch 274/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0076 - mean_absolute_error: 0.0673 - val_loss: 0.0333 - val_mean_absolute_error: 0.1286\n",
            "Epoch 275/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0068 - mean_absolute_error: 0.0636 - val_loss: 0.0335 - val_mean_absolute_error: 0.1286\n",
            "Epoch 276/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0068 - mean_absolute_error: 0.0637 - val_loss: 0.0339 - val_mean_absolute_error: 0.1299\n",
            "Epoch 277/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0067 - mean_absolute_error: 0.0631 - val_loss: 0.0340 - val_mean_absolute_error: 0.1299\n",
            "Epoch 278/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0064 - mean_absolute_error: 0.0618 - val_loss: 0.0345 - val_mean_absolute_error: 0.1343\n",
            "Epoch 279/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0089 - mean_absolute_error: 0.0736 - val_loss: 0.0350 - val_mean_absolute_error: 0.1324\n",
            "Epoch 280/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0080 - mean_absolute_error: 0.0687 - val_loss: 0.0343 - val_mean_absolute_error: 0.1301\n",
            "Epoch 281/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0080 - mean_absolute_error: 0.0693 - val_loss: 0.0341 - val_mean_absolute_error: 0.1295\n",
            "Epoch 282/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0068 - mean_absolute_error: 0.0638 - val_loss: 0.0329 - val_mean_absolute_error: 0.1275\n",
            "Epoch 283/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0076 - mean_absolute_error: 0.0676 - val_loss: 0.0337 - val_mean_absolute_error: 0.1289\n",
            "Epoch 284/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0068 - mean_absolute_error: 0.0639 - val_loss: 0.0338 - val_mean_absolute_error: 0.1302\n",
            "Epoch 285/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0071 - mean_absolute_error: 0.0653 - val_loss: 0.0338 - val_mean_absolute_error: 0.1302\n",
            "Epoch 286/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0069 - mean_absolute_error: 0.0643 - val_loss: 0.0335 - val_mean_absolute_error: 0.1288\n",
            "Epoch 287/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0071 - mean_absolute_error: 0.0653 - val_loss: 0.0392 - val_mean_absolute_error: 0.1432\n",
            "Epoch 288/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0085 - mean_absolute_error: 0.0717 - val_loss: 0.0347 - val_mean_absolute_error: 0.1304\n",
            "Epoch 289/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0078 - mean_absolute_error: 0.0684 - val_loss: 0.0347 - val_mean_absolute_error: 0.1330\n",
            "Epoch 290/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0077 - mean_absolute_error: 0.0683 - val_loss: 0.0337 - val_mean_absolute_error: 0.1291\n",
            "Epoch 291/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0072 - mean_absolute_error: 0.0659 - val_loss: 0.0344 - val_mean_absolute_error: 0.1305\n",
            "Epoch 292/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0073 - mean_absolute_error: 0.0659 - val_loss: 0.0348 - val_mean_absolute_error: 0.1341\n",
            "Epoch 293/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0080 - mean_absolute_error: 0.0693 - val_loss: 0.0334 - val_mean_absolute_error: 0.1283\n",
            "Epoch 294/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0068 - mean_absolute_error: 0.0637 - val_loss: 0.0328 - val_mean_absolute_error: 0.1271\n",
            "Epoch 295/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0066 - mean_absolute_error: 0.0633 - val_loss: 0.0342 - val_mean_absolute_error: 0.1312\n",
            "Epoch 296/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0061 - mean_absolute_error: 0.0606 - val_loss: 0.0330 - val_mean_absolute_error: 0.1273\n",
            "Epoch 297/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0062 - mean_absolute_error: 0.0615 - val_loss: 0.0327 - val_mean_absolute_error: 0.1266\n",
            "Epoch 298/300\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0064 - mean_absolute_error: 0.0622 - val_loss: 0.0327 - val_mean_absolute_error: 0.1270\n",
            "Epoch 299/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0068 - mean_absolute_error: 0.0641 - val_loss: 0.0335 - val_mean_absolute_error: 0.1294\n",
            "Epoch 300/300\n",
            "2688/2688 [==============================] - 4s 1ms/step - loss: 0.0068 - mean_absolute_error: 0.0639 - val_loss: 0.0347 - val_mean_absolute_error: 0.1320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f48f18eab38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "ZAzYp6hPb3BA",
        "colab_type": "code",
        "outputId": "da20e38b-410a-4754-9fc0-fc89f1774447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "error = model.evaluate(val_x,val_y)\n",
        "print (error)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "840/840 [==============================] - 0s 591us/step\n",
            "[0.03607211040244216, 0.13424101244835626]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1PCg2HKuq7aV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nrmse(y_true,y_pred):\n",
        "    \n",
        "    return (np.sqrt(np.mean(np.square(y_true - y_pred))))/np.mean(y_pred)\n",
        "def calculate_nrmse():\n",
        "  y_pred = np.exp(model.predict(val_x))\n",
        "  print (f'shape of y_pred {y_pred.shape}')\n",
        "  y_true = np.exp(val_y)\n",
        "  print (f'shape of y_true {y_true.shape}')\n",
        "  \n",
        "  loss =  nrmse(y_true,y_pred)\n",
        "  \n",
        "  print (loss)\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PRPhKNoTVNTP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "fc135348-09fe-45fe-b183-76531a038836"
      },
      "cell_type": "code",
      "source": [
        "nrmse_loss = calculate_nrmse()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of y_pred (840, 20, 20, 1)\n",
            "shape of y_true (840, 20, 20, 1)\n",
            "0.2719575359504181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sKYG6wpioBfm",
        "colab_type": "code",
        "outputId": "63b525bb-0b4b-4e1f-a60a-80ec5367a840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print (os.path.isfile(src+'test2.npy'))\n",
        "test = np.load(src+'test2.npy')\n",
        "test_data = (test - data_mean)/data_std\n",
        "test_data.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1800, 21, 21, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "uvsGxbBi0N_S",
        "colab_type": "code",
        "outputId": "499950c1-00dc-44c3-8f8a-fb6f3f58c9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "prediction = model.predict(test_data)\n",
        "prediction = np.exp(prediction)\n",
        "pred = prediction.reshape(1800,-1)\n",
        "print (pred.shape)\n",
        "\n",
        "\n",
        "pred_df = pd.DataFrame(pred)\n",
        "pred_df.keys()\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1800, 400)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RangeIndex(start=0, stop=400, step=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "USg5RG6xkElF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "src_2 =  'drive/My Drive/geo/test_data/test_dataset/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QLWPALGKkFEx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da7faae4-df63-4343-a78c-217e4f307155"
      },
      "cell_type": "code",
      "source": [
        "test_label_df = pd.read_csv(src_2+'solution.csv')\n",
        "test_df = pd.read_csv(src+'upload_sample.csv')\n",
        "\n",
        "x = test_label_df['ID'] != test_df['ID']\n",
        "sum(x)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "ozdmQ8Bel7J3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "bbd7b949-4237-4b9b-893b-eac2d94c274e"
      },
      "cell_type": "code",
      "source": [
        "test_label_df = test_label_df.drop('ID', axis=1)\n",
        "test_label_df.keys()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['loc1', 'loc2', 'loc3', 'loc4', 'loc5', 'loc6', 'loc7', 'loc8', 'loc9',\n",
              "       'loc10',\n",
              "       ...\n",
              "       'loc391', 'loc392', 'loc393', 'loc394', 'loc395', 'loc396', 'loc397',\n",
              "       'loc398', 'loc399', 'loc400'],\n",
              "      dtype='object', length=400)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "WnHKdy4Vl7R7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ec77346-57d5-4457-d246-4b64751e29da"
      },
      "cell_type": "code",
      "source": [
        "test_label = test_label_df.values\n",
        "test_label.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1800, 400)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "lkwIO1jUl7VL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92f09796-602c-453a-f07d-1a65d8cad1d6"
      },
      "cell_type": "code",
      "source": [
        "nrmse(test_label,pred)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2777077958843121"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "Fy-F-kvVl7aq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xgKJT11Fl7d-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-cc8J2arl7Ya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nW4-1-tPkFNp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xKLtD2_1wsF_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(src+'upload_sample.csv')\n",
        "col_names = test_df.keys().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4dagTVwK1dD-",
        "colab_type": "code",
        "outputId": "88214ce4-b93e-4f8b-f4f7-f69c5d72ee46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "final = pd.concat([test_df['ID'],pred_df],axis=1)\n",
        "final.columns = col_names\n",
        "final.to_csv(src+'stage_2_test_submit_0831_ch32.csv', index=False)\n",
        "print ('submit file saved')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "submit file saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y8jFoHVG2NZl",
        "colab_type": "code",
        "outputId": "9e1aab28-a3de-49c6-b72c-90b798e126c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1989
        }
      },
      "cell_type": "code",
      "source": [
        "test_see_df = pd.read_csv(src+'test_submit_0831_ch32.csv')\n",
        "test_see_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>loc1</th>\n",
              "      <th>loc2</th>\n",
              "      <th>loc3</th>\n",
              "      <th>loc4</th>\n",
              "      <th>loc5</th>\n",
              "      <th>loc6</th>\n",
              "      <th>loc7</th>\n",
              "      <th>loc8</th>\n",
              "      <th>loc9</th>\n",
              "      <th>...</th>\n",
              "      <th>loc391</th>\n",
              "      <th>loc392</th>\n",
              "      <th>loc393</th>\n",
              "      <th>loc394</th>\n",
              "      <th>loc395</th>\n",
              "      <th>loc396</th>\n",
              "      <th>loc397</th>\n",
              "      <th>loc398</th>\n",
              "      <th>loc399</th>\n",
              "      <th>loc400</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>F0000009</td>\n",
              "      <td>2.303276</td>\n",
              "      <td>2.633120</td>\n",
              "      <td>0.967853</td>\n",
              "      <td>0.711195</td>\n",
              "      <td>0.612320</td>\n",
              "      <td>0.606436</td>\n",
              "      <td>0.659745</td>\n",
              "      <td>0.903510</td>\n",
              "      <td>1.337734</td>\n",
              "      <td>...</td>\n",
              "      <td>14.198577</td>\n",
              "      <td>8.562691</td>\n",
              "      <td>2.988225</td>\n",
              "      <td>5.268833</td>\n",
              "      <td>4.511308</td>\n",
              "      <td>2.368932</td>\n",
              "      <td>3.126402</td>\n",
              "      <td>3.378817</td>\n",
              "      <td>4.378636</td>\n",
              "      <td>3.183275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>F0000011</td>\n",
              "      <td>6.329229</td>\n",
              "      <td>5.513133</td>\n",
              "      <td>3.744405</td>\n",
              "      <td>2.303595</td>\n",
              "      <td>1.706632</td>\n",
              "      <td>2.405296</td>\n",
              "      <td>1.752342</td>\n",
              "      <td>1.457627</td>\n",
              "      <td>0.967599</td>\n",
              "      <td>...</td>\n",
              "      <td>2.244529</td>\n",
              "      <td>2.195623</td>\n",
              "      <td>1.610037</td>\n",
              "      <td>1.782069</td>\n",
              "      <td>1.366208</td>\n",
              "      <td>0.615595</td>\n",
              "      <td>0.960376</td>\n",
              "      <td>0.557508</td>\n",
              "      <td>0.523693</td>\n",
              "      <td>0.649285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F0000012</td>\n",
              "      <td>3.611720</td>\n",
              "      <td>5.277518</td>\n",
              "      <td>3.246185</td>\n",
              "      <td>1.668042</td>\n",
              "      <td>0.475114</td>\n",
              "      <td>0.388565</td>\n",
              "      <td>0.637970</td>\n",
              "      <td>0.502681</td>\n",
              "      <td>0.655798</td>\n",
              "      <td>...</td>\n",
              "      <td>1.773896</td>\n",
              "      <td>2.336798</td>\n",
              "      <td>1.996541</td>\n",
              "      <td>1.867982</td>\n",
              "      <td>1.414425</td>\n",
              "      <td>0.676193</td>\n",
              "      <td>0.828465</td>\n",
              "      <td>1.352033</td>\n",
              "      <td>1.516963</td>\n",
              "      <td>0.926687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>F0000014</td>\n",
              "      <td>2.683136</td>\n",
              "      <td>2.873166</td>\n",
              "      <td>1.564418</td>\n",
              "      <td>0.715690</td>\n",
              "      <td>0.310166</td>\n",
              "      <td>0.841908</td>\n",
              "      <td>2.362045</td>\n",
              "      <td>2.305695</td>\n",
              "      <td>1.127642</td>\n",
              "      <td>...</td>\n",
              "      <td>2.207136</td>\n",
              "      <td>2.887102</td>\n",
              "      <td>8.945212</td>\n",
              "      <td>9.453363</td>\n",
              "      <td>10.373502</td>\n",
              "      <td>6.186830</td>\n",
              "      <td>5.693193</td>\n",
              "      <td>4.179819</td>\n",
              "      <td>2.414889</td>\n",
              "      <td>3.788618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>F0000017</td>\n",
              "      <td>0.234505</td>\n",
              "      <td>0.538168</td>\n",
              "      <td>0.824665</td>\n",
              "      <td>1.241247</td>\n",
              "      <td>0.794359</td>\n",
              "      <td>0.691293</td>\n",
              "      <td>2.202758</td>\n",
              "      <td>3.200006</td>\n",
              "      <td>2.721208</td>\n",
              "      <td>...</td>\n",
              "      <td>6.542584</td>\n",
              "      <td>22.577510</td>\n",
              "      <td>32.344440</td>\n",
              "      <td>18.713957</td>\n",
              "      <td>15.682641</td>\n",
              "      <td>5.173859</td>\n",
              "      <td>2.250664</td>\n",
              "      <td>1.221537</td>\n",
              "      <td>5.205012</td>\n",
              "      <td>2.196197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>F0000018</td>\n",
              "      <td>3.106924</td>\n",
              "      <td>4.724935</td>\n",
              "      <td>3.024786</td>\n",
              "      <td>2.114809</td>\n",
              "      <td>1.122492</td>\n",
              "      <td>0.463340</td>\n",
              "      <td>0.880873</td>\n",
              "      <td>1.015929</td>\n",
              "      <td>1.780746</td>\n",
              "      <td>...</td>\n",
              "      <td>2.085273</td>\n",
              "      <td>2.548799</td>\n",
              "      <td>0.884403</td>\n",
              "      <td>1.468000</td>\n",
              "      <td>3.873965</td>\n",
              "      <td>5.753904</td>\n",
              "      <td>2.968388</td>\n",
              "      <td>1.870800</td>\n",
              "      <td>4.490972</td>\n",
              "      <td>9.098993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>F0000024</td>\n",
              "      <td>1.509835</td>\n",
              "      <td>1.381073</td>\n",
              "      <td>2.387015</td>\n",
              "      <td>1.938448</td>\n",
              "      <td>1.519532</td>\n",
              "      <td>1.164685</td>\n",
              "      <td>1.023207</td>\n",
              "      <td>1.416044</td>\n",
              "      <td>1.884219</td>\n",
              "      <td>...</td>\n",
              "      <td>0.844937</td>\n",
              "      <td>1.718213</td>\n",
              "      <td>3.039999</td>\n",
              "      <td>2.531460</td>\n",
              "      <td>2.925235</td>\n",
              "      <td>4.045678</td>\n",
              "      <td>2.195813</td>\n",
              "      <td>2.420788</td>\n",
              "      <td>4.041779</td>\n",
              "      <td>3.283170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>F0000036</td>\n",
              "      <td>4.549304</td>\n",
              "      <td>4.280723</td>\n",
              "      <td>2.687711</td>\n",
              "      <td>7.107157</td>\n",
              "      <td>5.854353</td>\n",
              "      <td>6.881705</td>\n",
              "      <td>8.659466</td>\n",
              "      <td>5.750557</td>\n",
              "      <td>8.395898</td>\n",
              "      <td>...</td>\n",
              "      <td>8.959080</td>\n",
              "      <td>4.130376</td>\n",
              "      <td>4.660564</td>\n",
              "      <td>7.090211</td>\n",
              "      <td>10.861546</td>\n",
              "      <td>14.283480</td>\n",
              "      <td>18.413473</td>\n",
              "      <td>7.699151</td>\n",
              "      <td>7.086923</td>\n",
              "      <td>8.154278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>F0000046</td>\n",
              "      <td>5.534216</td>\n",
              "      <td>9.097797</td>\n",
              "      <td>25.130870</td>\n",
              "      <td>15.975117</td>\n",
              "      <td>8.560527</td>\n",
              "      <td>4.248939</td>\n",
              "      <td>2.432146</td>\n",
              "      <td>2.700563</td>\n",
              "      <td>4.332023</td>\n",
              "      <td>...</td>\n",
              "      <td>4.914675</td>\n",
              "      <td>6.751660</td>\n",
              "      <td>6.020052</td>\n",
              "      <td>3.641826</td>\n",
              "      <td>4.516870</td>\n",
              "      <td>10.352589</td>\n",
              "      <td>11.004499</td>\n",
              "      <td>9.246391</td>\n",
              "      <td>8.224123</td>\n",
              "      <td>10.247271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>F0000047</td>\n",
              "      <td>1.926024</td>\n",
              "      <td>2.275108</td>\n",
              "      <td>2.933197</td>\n",
              "      <td>3.494335</td>\n",
              "      <td>2.513233</td>\n",
              "      <td>1.539907</td>\n",
              "      <td>1.563043</td>\n",
              "      <td>1.575975</td>\n",
              "      <td>2.180220</td>\n",
              "      <td>...</td>\n",
              "      <td>5.074250</td>\n",
              "      <td>6.098646</td>\n",
              "      <td>4.938331</td>\n",
              "      <td>3.050952</td>\n",
              "      <td>4.389782</td>\n",
              "      <td>5.872658</td>\n",
              "      <td>5.134856</td>\n",
              "      <td>6.942337</td>\n",
              "      <td>10.453601</td>\n",
              "      <td>8.602502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>F0000048</td>\n",
              "      <td>20.652632</td>\n",
              "      <td>33.995995</td>\n",
              "      <td>52.926582</td>\n",
              "      <td>17.868387</td>\n",
              "      <td>7.314888</td>\n",
              "      <td>5.734422</td>\n",
              "      <td>4.157221</td>\n",
              "      <td>5.034347</td>\n",
              "      <td>15.480470</td>\n",
              "      <td>...</td>\n",
              "      <td>3.063334</td>\n",
              "      <td>1.055917</td>\n",
              "      <td>0.393314</td>\n",
              "      <td>0.385540</td>\n",
              "      <td>0.623621</td>\n",
              "      <td>1.025196</td>\n",
              "      <td>0.418536</td>\n",
              "      <td>0.796182</td>\n",
              "      <td>4.360134</td>\n",
              "      <td>9.518021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>F0000051</td>\n",
              "      <td>12.437338</td>\n",
              "      <td>6.264972</td>\n",
              "      <td>2.177781</td>\n",
              "      <td>1.558816</td>\n",
              "      <td>1.509240</td>\n",
              "      <td>3.364384</td>\n",
              "      <td>3.158484</td>\n",
              "      <td>1.261669</td>\n",
              "      <td>2.506135</td>\n",
              "      <td>...</td>\n",
              "      <td>7.489040</td>\n",
              "      <td>4.709154</td>\n",
              "      <td>2.873084</td>\n",
              "      <td>1.953563</td>\n",
              "      <td>2.137640</td>\n",
              "      <td>4.766042</td>\n",
              "      <td>14.213503</td>\n",
              "      <td>29.409958</td>\n",
              "      <td>26.610662</td>\n",
              "      <td>13.395562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>F0000053</td>\n",
              "      <td>0.910943</td>\n",
              "      <td>1.103605</td>\n",
              "      <td>0.767685</td>\n",
              "      <td>0.974606</td>\n",
              "      <td>1.286200</td>\n",
              "      <td>1.468730</td>\n",
              "      <td>1.025374</td>\n",
              "      <td>1.041733</td>\n",
              "      <td>1.424034</td>\n",
              "      <td>...</td>\n",
              "      <td>0.668139</td>\n",
              "      <td>0.236537</td>\n",
              "      <td>0.457683</td>\n",
              "      <td>1.008293</td>\n",
              "      <td>2.800187</td>\n",
              "      <td>3.738060</td>\n",
              "      <td>6.314335</td>\n",
              "      <td>8.115027</td>\n",
              "      <td>3.156437</td>\n",
              "      <td>1.343943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>F0000054</td>\n",
              "      <td>0.750083</td>\n",
              "      <td>1.394417</td>\n",
              "      <td>3.304441</td>\n",
              "      <td>1.831993</td>\n",
              "      <td>0.756562</td>\n",
              "      <td>0.628499</td>\n",
              "      <td>0.816878</td>\n",
              "      <td>3.019567</td>\n",
              "      <td>5.123874</td>\n",
              "      <td>...</td>\n",
              "      <td>1.721946</td>\n",
              "      <td>2.220998</td>\n",
              "      <td>0.742731</td>\n",
              "      <td>0.330098</td>\n",
              "      <td>0.196398</td>\n",
              "      <td>0.629299</td>\n",
              "      <td>2.477335</td>\n",
              "      <td>3.594346</td>\n",
              "      <td>5.750381</td>\n",
              "      <td>2.457064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>F0000057</td>\n",
              "      <td>0.741816</td>\n",
              "      <td>0.657264</td>\n",
              "      <td>0.695477</td>\n",
              "      <td>0.558093</td>\n",
              "      <td>0.701038</td>\n",
              "      <td>1.150451</td>\n",
              "      <td>1.679969</td>\n",
              "      <td>1.638696</td>\n",
              "      <td>2.425700</td>\n",
              "      <td>...</td>\n",
              "      <td>2.144041</td>\n",
              "      <td>3.300222</td>\n",
              "      <td>4.771925</td>\n",
              "      <td>6.796495</td>\n",
              "      <td>3.329586</td>\n",
              "      <td>2.940020</td>\n",
              "      <td>9.063610</td>\n",
              "      <td>14.344852</td>\n",
              "      <td>29.723429</td>\n",
              "      <td>37.220543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>F0000061</td>\n",
              "      <td>33.306690</td>\n",
              "      <td>19.954653</td>\n",
              "      <td>10.333847</td>\n",
              "      <td>8.740312</td>\n",
              "      <td>3.814435</td>\n",
              "      <td>2.572137</td>\n",
              "      <td>2.151315</td>\n",
              "      <td>2.702223</td>\n",
              "      <td>1.683614</td>\n",
              "      <td>...</td>\n",
              "      <td>0.471152</td>\n",
              "      <td>0.841153</td>\n",
              "      <td>2.122401</td>\n",
              "      <td>3.926712</td>\n",
              "      <td>4.045723</td>\n",
              "      <td>4.517091</td>\n",
              "      <td>3.362950</td>\n",
              "      <td>1.696191</td>\n",
              "      <td>2.818758</td>\n",
              "      <td>2.435935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>F0000064</td>\n",
              "      <td>2.144422</td>\n",
              "      <td>1.269146</td>\n",
              "      <td>0.591189</td>\n",
              "      <td>0.442271</td>\n",
              "      <td>1.035759</td>\n",
              "      <td>1.504987</td>\n",
              "      <td>1.264643</td>\n",
              "      <td>2.248283</td>\n",
              "      <td>2.694662</td>\n",
              "      <td>...</td>\n",
              "      <td>1.940963</td>\n",
              "      <td>1.301586</td>\n",
              "      <td>1.700055</td>\n",
              "      <td>1.908244</td>\n",
              "      <td>2.415176</td>\n",
              "      <td>4.074230</td>\n",
              "      <td>3.452168</td>\n",
              "      <td>4.494447</td>\n",
              "      <td>5.617652</td>\n",
              "      <td>9.539695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>F0000066</td>\n",
              "      <td>11.645531</td>\n",
              "      <td>7.111301</td>\n",
              "      <td>3.323163</td>\n",
              "      <td>2.183275</td>\n",
              "      <td>2.512917</td>\n",
              "      <td>2.598790</td>\n",
              "      <td>1.745974</td>\n",
              "      <td>1.139034</td>\n",
              "      <td>1.535812</td>\n",
              "      <td>...</td>\n",
              "      <td>1.111168</td>\n",
              "      <td>0.502294</td>\n",
              "      <td>0.430600</td>\n",
              "      <td>0.967143</td>\n",
              "      <td>0.930188</td>\n",
              "      <td>4.759078</td>\n",
              "      <td>5.213235</td>\n",
              "      <td>2.600242</td>\n",
              "      <td>2.966337</td>\n",
              "      <td>1.876263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>F0000067</td>\n",
              "      <td>3.080466</td>\n",
              "      <td>2.188396</td>\n",
              "      <td>1.882559</td>\n",
              "      <td>1.483453</td>\n",
              "      <td>1.545256</td>\n",
              "      <td>1.831146</td>\n",
              "      <td>2.026785</td>\n",
              "      <td>1.394369</td>\n",
              "      <td>1.169270</td>\n",
              "      <td>...</td>\n",
              "      <td>0.841970</td>\n",
              "      <td>0.883476</td>\n",
              "      <td>0.953351</td>\n",
              "      <td>1.408569</td>\n",
              "      <td>1.596759</td>\n",
              "      <td>1.974259</td>\n",
              "      <td>1.388235</td>\n",
              "      <td>0.532499</td>\n",
              "      <td>0.679541</td>\n",
              "      <td>0.625160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>F0000077</td>\n",
              "      <td>1.500818</td>\n",
              "      <td>1.489934</td>\n",
              "      <td>1.635062</td>\n",
              "      <td>1.498792</td>\n",
              "      <td>1.547774</td>\n",
              "      <td>0.910363</td>\n",
              "      <td>1.886299</td>\n",
              "      <td>2.878254</td>\n",
              "      <td>2.859822</td>\n",
              "      <td>...</td>\n",
              "      <td>18.453215</td>\n",
              "      <td>6.355784</td>\n",
              "      <td>4.577859</td>\n",
              "      <td>9.068478</td>\n",
              "      <td>5.294091</td>\n",
              "      <td>4.772662</td>\n",
              "      <td>5.571238</td>\n",
              "      <td>13.266127</td>\n",
              "      <td>7.437178</td>\n",
              "      <td>1.510383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>F0000078</td>\n",
              "      <td>2.713030</td>\n",
              "      <td>3.408247</td>\n",
              "      <td>4.660220</td>\n",
              "      <td>4.057150</td>\n",
              "      <td>3.293992</td>\n",
              "      <td>2.931879</td>\n",
              "      <td>1.991378</td>\n",
              "      <td>0.806612</td>\n",
              "      <td>0.948601</td>\n",
              "      <td>...</td>\n",
              "      <td>0.611694</td>\n",
              "      <td>1.523846</td>\n",
              "      <td>1.907247</td>\n",
              "      <td>3.170228</td>\n",
              "      <td>3.494537</td>\n",
              "      <td>2.894914</td>\n",
              "      <td>4.021257</td>\n",
              "      <td>4.599502</td>\n",
              "      <td>3.673347</td>\n",
              "      <td>3.549517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>F0000079</td>\n",
              "      <td>6.621870</td>\n",
              "      <td>5.868481</td>\n",
              "      <td>3.552604</td>\n",
              "      <td>3.260004</td>\n",
              "      <td>5.629075</td>\n",
              "      <td>5.656404</td>\n",
              "      <td>5.099396</td>\n",
              "      <td>5.432373</td>\n",
              "      <td>5.272851</td>\n",
              "      <td>...</td>\n",
              "      <td>0.906899</td>\n",
              "      <td>1.727650</td>\n",
              "      <td>3.224317</td>\n",
              "      <td>7.051887</td>\n",
              "      <td>11.790594</td>\n",
              "      <td>16.643347</td>\n",
              "      <td>7.049145</td>\n",
              "      <td>8.583964</td>\n",
              "      <td>17.633648</td>\n",
              "      <td>16.254145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>F0000088</td>\n",
              "      <td>0.518932</td>\n",
              "      <td>0.464432</td>\n",
              "      <td>0.612854</td>\n",
              "      <td>0.785798</td>\n",
              "      <td>0.438779</td>\n",
              "      <td>0.685559</td>\n",
              "      <td>1.624833</td>\n",
              "      <td>1.721921</td>\n",
              "      <td>1.228552</td>\n",
              "      <td>...</td>\n",
              "      <td>2.578208</td>\n",
              "      <td>1.937333</td>\n",
              "      <td>1.724551</td>\n",
              "      <td>3.365602</td>\n",
              "      <td>2.924775</td>\n",
              "      <td>3.078687</td>\n",
              "      <td>2.782127</td>\n",
              "      <td>1.333418</td>\n",
              "      <td>1.685459</td>\n",
              "      <td>1.524903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>F0000092</td>\n",
              "      <td>0.269790</td>\n",
              "      <td>0.404652</td>\n",
              "      <td>0.366466</td>\n",
              "      <td>0.336078</td>\n",
              "      <td>0.435278</td>\n",
              "      <td>0.310754</td>\n",
              "      <td>0.456227</td>\n",
              "      <td>0.988275</td>\n",
              "      <td>0.675815</td>\n",
              "      <td>...</td>\n",
              "      <td>5.928824</td>\n",
              "      <td>5.870936</td>\n",
              "      <td>3.280750</td>\n",
              "      <td>1.706294</td>\n",
              "      <td>1.197328</td>\n",
              "      <td>2.347076</td>\n",
              "      <td>3.853735</td>\n",
              "      <td>3.917545</td>\n",
              "      <td>5.539401</td>\n",
              "      <td>3.259628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>F0000100</td>\n",
              "      <td>1.038231</td>\n",
              "      <td>1.136319</td>\n",
              "      <td>0.885931</td>\n",
              "      <td>0.750078</td>\n",
              "      <td>1.665736</td>\n",
              "      <td>2.860888</td>\n",
              "      <td>1.548715</td>\n",
              "      <td>1.701221</td>\n",
              "      <td>2.488202</td>\n",
              "      <td>...</td>\n",
              "      <td>7.957812</td>\n",
              "      <td>11.172690</td>\n",
              "      <td>6.819529</td>\n",
              "      <td>3.797462</td>\n",
              "      <td>2.592548</td>\n",
              "      <td>1.742366</td>\n",
              "      <td>2.864707</td>\n",
              "      <td>2.425534</td>\n",
              "      <td>2.244323</td>\n",
              "      <td>2.238122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>F0000103</td>\n",
              "      <td>0.477743</td>\n",
              "      <td>0.786121</td>\n",
              "      <td>1.645252</td>\n",
              "      <td>1.224810</td>\n",
              "      <td>1.019933</td>\n",
              "      <td>1.382263</td>\n",
              "      <td>0.995078</td>\n",
              "      <td>1.125195</td>\n",
              "      <td>1.225753</td>\n",
              "      <td>...</td>\n",
              "      <td>10.511009</td>\n",
              "      <td>6.419022</td>\n",
              "      <td>5.407313</td>\n",
              "      <td>2.035063</td>\n",
              "      <td>2.469227</td>\n",
              "      <td>2.116930</td>\n",
              "      <td>1.815346</td>\n",
              "      <td>1.955187</td>\n",
              "      <td>1.567535</td>\n",
              "      <td>1.478904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>F0000104</td>\n",
              "      <td>1.388092</td>\n",
              "      <td>1.576286</td>\n",
              "      <td>0.858428</td>\n",
              "      <td>0.551944</td>\n",
              "      <td>1.830480</td>\n",
              "      <td>1.562317</td>\n",
              "      <td>1.816000</td>\n",
              "      <td>3.216226</td>\n",
              "      <td>2.353058</td>\n",
              "      <td>...</td>\n",
              "      <td>1.428833</td>\n",
              "      <td>1.245269</td>\n",
              "      <td>0.472492</td>\n",
              "      <td>1.225905</td>\n",
              "      <td>1.285642</td>\n",
              "      <td>1.005735</td>\n",
              "      <td>0.724017</td>\n",
              "      <td>0.503107</td>\n",
              "      <td>0.512637</td>\n",
              "      <td>0.416839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>F0000105</td>\n",
              "      <td>0.714076</td>\n",
              "      <td>0.891166</td>\n",
              "      <td>1.478718</td>\n",
              "      <td>0.519320</td>\n",
              "      <td>0.711848</td>\n",
              "      <td>1.246055</td>\n",
              "      <td>0.783323</td>\n",
              "      <td>0.585090</td>\n",
              "      <td>0.453591</td>\n",
              "      <td>...</td>\n",
              "      <td>2.320153</td>\n",
              "      <td>2.394096</td>\n",
              "      <td>0.476611</td>\n",
              "      <td>1.266176</td>\n",
              "      <td>0.865764</td>\n",
              "      <td>0.722571</td>\n",
              "      <td>1.322277</td>\n",
              "      <td>2.253647</td>\n",
              "      <td>5.780131</td>\n",
              "      <td>4.890282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>F0000107</td>\n",
              "      <td>1.905543</td>\n",
              "      <td>1.602184</td>\n",
              "      <td>1.044503</td>\n",
              "      <td>0.886893</td>\n",
              "      <td>1.467033</td>\n",
              "      <td>1.596956</td>\n",
              "      <td>1.561978</td>\n",
              "      <td>1.867255</td>\n",
              "      <td>2.737040</td>\n",
              "      <td>...</td>\n",
              "      <td>4.258166</td>\n",
              "      <td>6.741268</td>\n",
              "      <td>10.497833</td>\n",
              "      <td>10.431764</td>\n",
              "      <td>7.872065</td>\n",
              "      <td>11.145880</td>\n",
              "      <td>6.622912</td>\n",
              "      <td>3.463826</td>\n",
              "      <td>1.891015</td>\n",
              "      <td>1.140295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>F0000109</td>\n",
              "      <td>0.149255</td>\n",
              "      <td>0.071287</td>\n",
              "      <td>0.178389</td>\n",
              "      <td>0.316939</td>\n",
              "      <td>0.597959</td>\n",
              "      <td>1.011843</td>\n",
              "      <td>0.883955</td>\n",
              "      <td>4.134008</td>\n",
              "      <td>3.133165</td>\n",
              "      <td>...</td>\n",
              "      <td>0.654023</td>\n",
              "      <td>1.257039</td>\n",
              "      <td>3.813466</td>\n",
              "      <td>7.821394</td>\n",
              "      <td>4.479205</td>\n",
              "      <td>5.301716</td>\n",
              "      <td>7.689736</td>\n",
              "      <td>2.943549</td>\n",
              "      <td>1.791588</td>\n",
              "      <td>2.820437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1770</th>\n",
              "      <td>F0005900</td>\n",
              "      <td>1.323523</td>\n",
              "      <td>1.295901</td>\n",
              "      <td>1.455428</td>\n",
              "      <td>0.753431</td>\n",
              "      <td>1.428795</td>\n",
              "      <td>2.009519</td>\n",
              "      <td>0.672810</td>\n",
              "      <td>0.888415</td>\n",
              "      <td>1.040625</td>\n",
              "      <td>...</td>\n",
              "      <td>3.000496</td>\n",
              "      <td>2.139266</td>\n",
              "      <td>1.611814</td>\n",
              "      <td>1.483957</td>\n",
              "      <td>1.450607</td>\n",
              "      <td>1.442716</td>\n",
              "      <td>1.521094</td>\n",
              "      <td>7.569211</td>\n",
              "      <td>4.721660</td>\n",
              "      <td>3.440695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1771</th>\n",
              "      <td>F0005905</td>\n",
              "      <td>3.209217</td>\n",
              "      <td>1.524227</td>\n",
              "      <td>2.423214</td>\n",
              "      <td>5.761286</td>\n",
              "      <td>6.936222</td>\n",
              "      <td>6.638852</td>\n",
              "      <td>8.835508</td>\n",
              "      <td>7.993683</td>\n",
              "      <td>7.503543</td>\n",
              "      <td>...</td>\n",
              "      <td>0.678277</td>\n",
              "      <td>0.815854</td>\n",
              "      <td>1.192992</td>\n",
              "      <td>0.446210</td>\n",
              "      <td>0.559226</td>\n",
              "      <td>1.476782</td>\n",
              "      <td>1.545443</td>\n",
              "      <td>1.361798</td>\n",
              "      <td>2.982006</td>\n",
              "      <td>5.245597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1772</th>\n",
              "      <td>F0005912</td>\n",
              "      <td>3.368048</td>\n",
              "      <td>2.576362</td>\n",
              "      <td>2.269005</td>\n",
              "      <td>2.212387</td>\n",
              "      <td>2.108429</td>\n",
              "      <td>1.106514</td>\n",
              "      <td>0.964778</td>\n",
              "      <td>0.696674</td>\n",
              "      <td>0.462625</td>\n",
              "      <td>...</td>\n",
              "      <td>4.031189</td>\n",
              "      <td>4.777591</td>\n",
              "      <td>4.667728</td>\n",
              "      <td>2.181439</td>\n",
              "      <td>0.982725</td>\n",
              "      <td>1.249798</td>\n",
              "      <td>1.109017</td>\n",
              "      <td>1.440018</td>\n",
              "      <td>2.662548</td>\n",
              "      <td>1.521867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1773</th>\n",
              "      <td>F0005915</td>\n",
              "      <td>0.961125</td>\n",
              "      <td>0.819193</td>\n",
              "      <td>0.647220</td>\n",
              "      <td>1.297425</td>\n",
              "      <td>0.925135</td>\n",
              "      <td>1.757982</td>\n",
              "      <td>1.187254</td>\n",
              "      <td>1.644746</td>\n",
              "      <td>3.944567</td>\n",
              "      <td>...</td>\n",
              "      <td>17.345568</td>\n",
              "      <td>8.764386</td>\n",
              "      <td>5.616642</td>\n",
              "      <td>2.226284</td>\n",
              "      <td>0.976604</td>\n",
              "      <td>1.439587</td>\n",
              "      <td>1.421871</td>\n",
              "      <td>1.214840</td>\n",
              "      <td>1.680860</td>\n",
              "      <td>1.278092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1774</th>\n",
              "      <td>F0005916</td>\n",
              "      <td>0.488487</td>\n",
              "      <td>0.539411</td>\n",
              "      <td>1.140720</td>\n",
              "      <td>1.281139</td>\n",
              "      <td>0.946188</td>\n",
              "      <td>0.796534</td>\n",
              "      <td>1.181490</td>\n",
              "      <td>3.118390</td>\n",
              "      <td>3.516017</td>\n",
              "      <td>...</td>\n",
              "      <td>2.327692</td>\n",
              "      <td>4.457804</td>\n",
              "      <td>2.626051</td>\n",
              "      <td>1.328008</td>\n",
              "      <td>2.163116</td>\n",
              "      <td>1.479488</td>\n",
              "      <td>2.761025</td>\n",
              "      <td>5.556460</td>\n",
              "      <td>6.655053</td>\n",
              "      <td>6.380516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1775</th>\n",
              "      <td>F0005917</td>\n",
              "      <td>2.601917</td>\n",
              "      <td>1.570904</td>\n",
              "      <td>0.722545</td>\n",
              "      <td>0.909375</td>\n",
              "      <td>0.741372</td>\n",
              "      <td>1.179749</td>\n",
              "      <td>2.833306</td>\n",
              "      <td>3.598268</td>\n",
              "      <td>4.436368</td>\n",
              "      <td>...</td>\n",
              "      <td>2.975135</td>\n",
              "      <td>4.605570</td>\n",
              "      <td>4.312715</td>\n",
              "      <td>4.348446</td>\n",
              "      <td>2.648516</td>\n",
              "      <td>2.593340</td>\n",
              "      <td>2.531201</td>\n",
              "      <td>1.229766</td>\n",
              "      <td>0.925853</td>\n",
              "      <td>0.317516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1776</th>\n",
              "      <td>F0005923</td>\n",
              "      <td>1.229617</td>\n",
              "      <td>1.345330</td>\n",
              "      <td>1.348163</td>\n",
              "      <td>2.388293</td>\n",
              "      <td>3.126362</td>\n",
              "      <td>3.293191</td>\n",
              "      <td>3.489855</td>\n",
              "      <td>2.391400</td>\n",
              "      <td>5.047786</td>\n",
              "      <td>...</td>\n",
              "      <td>6.122244</td>\n",
              "      <td>6.090610</td>\n",
              "      <td>3.748130</td>\n",
              "      <td>4.084436</td>\n",
              "      <td>5.214523</td>\n",
              "      <td>6.685337</td>\n",
              "      <td>7.968226</td>\n",
              "      <td>3.614980</td>\n",
              "      <td>4.437085</td>\n",
              "      <td>6.718983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1777</th>\n",
              "      <td>F0005926</td>\n",
              "      <td>1.132353</td>\n",
              "      <td>1.220101</td>\n",
              "      <td>0.850112</td>\n",
              "      <td>1.412708</td>\n",
              "      <td>1.596978</td>\n",
              "      <td>2.247459</td>\n",
              "      <td>6.328268</td>\n",
              "      <td>11.974008</td>\n",
              "      <td>4.930750</td>\n",
              "      <td>...</td>\n",
              "      <td>3.248424</td>\n",
              "      <td>3.912272</td>\n",
              "      <td>3.434962</td>\n",
              "      <td>7.491676</td>\n",
              "      <td>17.559752</td>\n",
              "      <td>28.621363</td>\n",
              "      <td>22.057838</td>\n",
              "      <td>14.134838</td>\n",
              "      <td>11.995984</td>\n",
              "      <td>8.610427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1778</th>\n",
              "      <td>F0005927</td>\n",
              "      <td>0.707323</td>\n",
              "      <td>0.664875</td>\n",
              "      <td>0.719043</td>\n",
              "      <td>1.259513</td>\n",
              "      <td>0.607309</td>\n",
              "      <td>0.380661</td>\n",
              "      <td>0.878974</td>\n",
              "      <td>0.869244</td>\n",
              "      <td>0.626905</td>\n",
              "      <td>...</td>\n",
              "      <td>4.469632</td>\n",
              "      <td>4.028175</td>\n",
              "      <td>3.718959</td>\n",
              "      <td>1.926531</td>\n",
              "      <td>1.365657</td>\n",
              "      <td>1.266352</td>\n",
              "      <td>1.624813</td>\n",
              "      <td>4.899743</td>\n",
              "      <td>3.889246</td>\n",
              "      <td>2.353610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1779</th>\n",
              "      <td>F0005928</td>\n",
              "      <td>6.045567</td>\n",
              "      <td>11.909250</td>\n",
              "      <td>7.740517</td>\n",
              "      <td>2.534117</td>\n",
              "      <td>2.461152</td>\n",
              "      <td>1.939848</td>\n",
              "      <td>3.434960</td>\n",
              "      <td>18.061798</td>\n",
              "      <td>36.109850</td>\n",
              "      <td>...</td>\n",
              "      <td>2.875150</td>\n",
              "      <td>3.679949</td>\n",
              "      <td>2.375665</td>\n",
              "      <td>2.557974</td>\n",
              "      <td>1.981741</td>\n",
              "      <td>1.048983</td>\n",
              "      <td>0.559934</td>\n",
              "      <td>0.868441</td>\n",
              "      <td>2.852430</td>\n",
              "      <td>8.421205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1780</th>\n",
              "      <td>F0005933</td>\n",
              "      <td>0.533600</td>\n",
              "      <td>0.392396</td>\n",
              "      <td>0.850073</td>\n",
              "      <td>1.531713</td>\n",
              "      <td>2.185963</td>\n",
              "      <td>0.868813</td>\n",
              "      <td>0.382021</td>\n",
              "      <td>0.438814</td>\n",
              "      <td>0.439318</td>\n",
              "      <td>...</td>\n",
              "      <td>6.601388</td>\n",
              "      <td>5.301770</td>\n",
              "      <td>4.021045</td>\n",
              "      <td>3.825714</td>\n",
              "      <td>2.177808</td>\n",
              "      <td>2.975960</td>\n",
              "      <td>5.191958</td>\n",
              "      <td>4.825273</td>\n",
              "      <td>3.771930</td>\n",
              "      <td>6.425722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1781</th>\n",
              "      <td>F0005937</td>\n",
              "      <td>1.116768</td>\n",
              "      <td>1.125264</td>\n",
              "      <td>0.485512</td>\n",
              "      <td>0.354130</td>\n",
              "      <td>1.154607</td>\n",
              "      <td>3.042722</td>\n",
              "      <td>4.939437</td>\n",
              "      <td>3.706196</td>\n",
              "      <td>2.409788</td>\n",
              "      <td>...</td>\n",
              "      <td>3.222061</td>\n",
              "      <td>3.858958</td>\n",
              "      <td>4.335448</td>\n",
              "      <td>1.635573</td>\n",
              "      <td>0.701351</td>\n",
              "      <td>0.478707</td>\n",
              "      <td>0.512853</td>\n",
              "      <td>1.914670</td>\n",
              "      <td>6.387129</td>\n",
              "      <td>11.925665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1782</th>\n",
              "      <td>F0005942</td>\n",
              "      <td>1.999618</td>\n",
              "      <td>1.056652</td>\n",
              "      <td>0.795645</td>\n",
              "      <td>0.706524</td>\n",
              "      <td>0.451899</td>\n",
              "      <td>0.418066</td>\n",
              "      <td>1.079368</td>\n",
              "      <td>3.284924</td>\n",
              "      <td>5.180380</td>\n",
              "      <td>...</td>\n",
              "      <td>0.865656</td>\n",
              "      <td>1.302519</td>\n",
              "      <td>0.926325</td>\n",
              "      <td>1.057391</td>\n",
              "      <td>0.782206</td>\n",
              "      <td>0.754341</td>\n",
              "      <td>1.310361</td>\n",
              "      <td>4.562486</td>\n",
              "      <td>3.545104</td>\n",
              "      <td>7.773433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1783</th>\n",
              "      <td>F0005946</td>\n",
              "      <td>1.027266</td>\n",
              "      <td>1.026623</td>\n",
              "      <td>0.857798</td>\n",
              "      <td>1.793024</td>\n",
              "      <td>2.197954</td>\n",
              "      <td>2.197917</td>\n",
              "      <td>2.270341</td>\n",
              "      <td>1.566499</td>\n",
              "      <td>1.670121</td>\n",
              "      <td>...</td>\n",
              "      <td>1.734436</td>\n",
              "      <td>2.563675</td>\n",
              "      <td>2.337415</td>\n",
              "      <td>2.531934</td>\n",
              "      <td>3.913168</td>\n",
              "      <td>2.907942</td>\n",
              "      <td>2.541398</td>\n",
              "      <td>1.149108</td>\n",
              "      <td>1.044398</td>\n",
              "      <td>1.011177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1784</th>\n",
              "      <td>F0005948</td>\n",
              "      <td>1.860262</td>\n",
              "      <td>1.437228</td>\n",
              "      <td>0.612339</td>\n",
              "      <td>0.457762</td>\n",
              "      <td>0.557145</td>\n",
              "      <td>0.727749</td>\n",
              "      <td>0.900840</td>\n",
              "      <td>1.168446</td>\n",
              "      <td>0.598219</td>\n",
              "      <td>...</td>\n",
              "      <td>3.239780</td>\n",
              "      <td>1.902029</td>\n",
              "      <td>0.780457</td>\n",
              "      <td>1.520281</td>\n",
              "      <td>1.655513</td>\n",
              "      <td>1.466943</td>\n",
              "      <td>1.694931</td>\n",
              "      <td>0.793160</td>\n",
              "      <td>1.123061</td>\n",
              "      <td>2.759417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1785</th>\n",
              "      <td>F0005950</td>\n",
              "      <td>0.134930</td>\n",
              "      <td>0.358618</td>\n",
              "      <td>0.891062</td>\n",
              "      <td>1.308883</td>\n",
              "      <td>1.202681</td>\n",
              "      <td>0.669300</td>\n",
              "      <td>0.621382</td>\n",
              "      <td>1.252377</td>\n",
              "      <td>2.280426</td>\n",
              "      <td>...</td>\n",
              "      <td>7.137218</td>\n",
              "      <td>6.398832</td>\n",
              "      <td>5.180192</td>\n",
              "      <td>3.517964</td>\n",
              "      <td>4.741568</td>\n",
              "      <td>3.754290</td>\n",
              "      <td>1.797403</td>\n",
              "      <td>2.176733</td>\n",
              "      <td>1.071721</td>\n",
              "      <td>1.128544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1786</th>\n",
              "      <td>F0005952</td>\n",
              "      <td>1.526733</td>\n",
              "      <td>2.632045</td>\n",
              "      <td>7.510481</td>\n",
              "      <td>5.281970</td>\n",
              "      <td>2.729181</td>\n",
              "      <td>3.904912</td>\n",
              "      <td>8.062960</td>\n",
              "      <td>3.712467</td>\n",
              "      <td>3.136803</td>\n",
              "      <td>...</td>\n",
              "      <td>2.749143</td>\n",
              "      <td>3.058265</td>\n",
              "      <td>2.378554</td>\n",
              "      <td>3.386085</td>\n",
              "      <td>5.385571</td>\n",
              "      <td>5.340822</td>\n",
              "      <td>2.236870</td>\n",
              "      <td>1.659592</td>\n",
              "      <td>0.990404</td>\n",
              "      <td>0.901925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1787</th>\n",
              "      <td>F0005955</td>\n",
              "      <td>3.890497</td>\n",
              "      <td>3.962250</td>\n",
              "      <td>3.083796</td>\n",
              "      <td>1.340853</td>\n",
              "      <td>1.436267</td>\n",
              "      <td>2.652954</td>\n",
              "      <td>1.781050</td>\n",
              "      <td>1.483055</td>\n",
              "      <td>3.559106</td>\n",
              "      <td>...</td>\n",
              "      <td>4.885341</td>\n",
              "      <td>2.184284</td>\n",
              "      <td>2.119546</td>\n",
              "      <td>2.796220</td>\n",
              "      <td>1.850744</td>\n",
              "      <td>0.836297</td>\n",
              "      <td>0.891153</td>\n",
              "      <td>1.067829</td>\n",
              "      <td>1.885773</td>\n",
              "      <td>2.311086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1788</th>\n",
              "      <td>F0005956</td>\n",
              "      <td>3.130267</td>\n",
              "      <td>2.281266</td>\n",
              "      <td>1.638254</td>\n",
              "      <td>1.549735</td>\n",
              "      <td>2.230529</td>\n",
              "      <td>1.306892</td>\n",
              "      <td>1.091675</td>\n",
              "      <td>1.188698</td>\n",
              "      <td>1.046413</td>\n",
              "      <td>...</td>\n",
              "      <td>1.368070</td>\n",
              "      <td>1.671874</td>\n",
              "      <td>3.514567</td>\n",
              "      <td>7.493694</td>\n",
              "      <td>5.789693</td>\n",
              "      <td>4.361060</td>\n",
              "      <td>8.863407</td>\n",
              "      <td>5.340484</td>\n",
              "      <td>3.854682</td>\n",
              "      <td>5.021036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1789</th>\n",
              "      <td>F0005959</td>\n",
              "      <td>23.326921</td>\n",
              "      <td>28.488647</td>\n",
              "      <td>45.502148</td>\n",
              "      <td>15.296210</td>\n",
              "      <td>10.865436</td>\n",
              "      <td>6.963024</td>\n",
              "      <td>5.583030</td>\n",
              "      <td>5.447122</td>\n",
              "      <td>8.040642</td>\n",
              "      <td>...</td>\n",
              "      <td>3.093215</td>\n",
              "      <td>3.797435</td>\n",
              "      <td>4.416099</td>\n",
              "      <td>6.080209</td>\n",
              "      <td>7.273672</td>\n",
              "      <td>7.806881</td>\n",
              "      <td>2.414357</td>\n",
              "      <td>2.722545</td>\n",
              "      <td>1.655113</td>\n",
              "      <td>0.957200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1790</th>\n",
              "      <td>F0005961</td>\n",
              "      <td>0.706635</td>\n",
              "      <td>0.841394</td>\n",
              "      <td>0.419348</td>\n",
              "      <td>0.221148</td>\n",
              "      <td>0.492536</td>\n",
              "      <td>1.049918</td>\n",
              "      <td>0.764957</td>\n",
              "      <td>0.459775</td>\n",
              "      <td>0.563182</td>\n",
              "      <td>...</td>\n",
              "      <td>4.409362</td>\n",
              "      <td>4.895787</td>\n",
              "      <td>3.912370</td>\n",
              "      <td>2.656031</td>\n",
              "      <td>2.856354</td>\n",
              "      <td>4.010345</td>\n",
              "      <td>13.404252</td>\n",
              "      <td>24.783630</td>\n",
              "      <td>22.772015</td>\n",
              "      <td>27.581194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1791</th>\n",
              "      <td>F0005964</td>\n",
              "      <td>0.690461</td>\n",
              "      <td>0.818102</td>\n",
              "      <td>0.435939</td>\n",
              "      <td>0.419264</td>\n",
              "      <td>0.898035</td>\n",
              "      <td>2.271854</td>\n",
              "      <td>1.431377</td>\n",
              "      <td>1.365225</td>\n",
              "      <td>2.764257</td>\n",
              "      <td>...</td>\n",
              "      <td>1.505241</td>\n",
              "      <td>1.218970</td>\n",
              "      <td>1.354566</td>\n",
              "      <td>1.936176</td>\n",
              "      <td>1.732354</td>\n",
              "      <td>1.796404</td>\n",
              "      <td>1.419424</td>\n",
              "      <td>1.370859</td>\n",
              "      <td>1.918672</td>\n",
              "      <td>2.887728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1792</th>\n",
              "      <td>F0005965</td>\n",
              "      <td>5.417135</td>\n",
              "      <td>4.019561</td>\n",
              "      <td>3.426266</td>\n",
              "      <td>2.289643</td>\n",
              "      <td>1.505351</td>\n",
              "      <td>2.420017</td>\n",
              "      <td>1.446742</td>\n",
              "      <td>0.801799</td>\n",
              "      <td>0.793399</td>\n",
              "      <td>...</td>\n",
              "      <td>1.110762</td>\n",
              "      <td>0.790540</td>\n",
              "      <td>0.881395</td>\n",
              "      <td>0.786984</td>\n",
              "      <td>0.923787</td>\n",
              "      <td>1.764753</td>\n",
              "      <td>2.305818</td>\n",
              "      <td>1.591236</td>\n",
              "      <td>2.073917</td>\n",
              "      <td>2.156827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1793</th>\n",
              "      <td>F0005968</td>\n",
              "      <td>2.353701</td>\n",
              "      <td>2.081669</td>\n",
              "      <td>2.275276</td>\n",
              "      <td>2.440298</td>\n",
              "      <td>1.453248</td>\n",
              "      <td>1.266195</td>\n",
              "      <td>0.998716</td>\n",
              "      <td>1.043930</td>\n",
              "      <td>1.306492</td>\n",
              "      <td>...</td>\n",
              "      <td>1.857343</td>\n",
              "      <td>1.308526</td>\n",
              "      <td>0.701224</td>\n",
              "      <td>1.853004</td>\n",
              "      <td>1.776184</td>\n",
              "      <td>0.792995</td>\n",
              "      <td>0.626827</td>\n",
              "      <td>1.250852</td>\n",
              "      <td>1.429829</td>\n",
              "      <td>0.997534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1794</th>\n",
              "      <td>F0005973</td>\n",
              "      <td>0.414213</td>\n",
              "      <td>0.545782</td>\n",
              "      <td>0.448600</td>\n",
              "      <td>0.326854</td>\n",
              "      <td>0.274612</td>\n",
              "      <td>0.392778</td>\n",
              "      <td>0.637624</td>\n",
              "      <td>0.460694</td>\n",
              "      <td>0.530263</td>\n",
              "      <td>...</td>\n",
              "      <td>7.854734</td>\n",
              "      <td>10.525514</td>\n",
              "      <td>10.372069</td>\n",
              "      <td>5.984866</td>\n",
              "      <td>2.052454</td>\n",
              "      <td>1.326070</td>\n",
              "      <td>3.136443</td>\n",
              "      <td>4.032788</td>\n",
              "      <td>3.178754</td>\n",
              "      <td>3.387637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1795</th>\n",
              "      <td>F0005979</td>\n",
              "      <td>1.040663</td>\n",
              "      <td>1.284618</td>\n",
              "      <td>1.099170</td>\n",
              "      <td>0.664099</td>\n",
              "      <td>0.604902</td>\n",
              "      <td>0.820490</td>\n",
              "      <td>1.483253</td>\n",
              "      <td>0.765457</td>\n",
              "      <td>1.476914</td>\n",
              "      <td>...</td>\n",
              "      <td>9.877131</td>\n",
              "      <td>14.893926</td>\n",
              "      <td>16.550638</td>\n",
              "      <td>5.936255</td>\n",
              "      <td>7.014977</td>\n",
              "      <td>4.755193</td>\n",
              "      <td>3.853685</td>\n",
              "      <td>3.796204</td>\n",
              "      <td>1.744708</td>\n",
              "      <td>1.348829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1796</th>\n",
              "      <td>F0005982</td>\n",
              "      <td>0.623600</td>\n",
              "      <td>0.639799</td>\n",
              "      <td>1.029405</td>\n",
              "      <td>0.905027</td>\n",
              "      <td>0.632461</td>\n",
              "      <td>0.685374</td>\n",
              "      <td>1.351671</td>\n",
              "      <td>1.946942</td>\n",
              "      <td>2.264127</td>\n",
              "      <td>...</td>\n",
              "      <td>1.651571</td>\n",
              "      <td>1.745623</td>\n",
              "      <td>1.716137</td>\n",
              "      <td>3.352444</td>\n",
              "      <td>2.795984</td>\n",
              "      <td>3.564191</td>\n",
              "      <td>5.784504</td>\n",
              "      <td>3.273253</td>\n",
              "      <td>2.668455</td>\n",
              "      <td>3.121544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1797</th>\n",
              "      <td>F0005994</td>\n",
              "      <td>0.937487</td>\n",
              "      <td>0.593661</td>\n",
              "      <td>0.306112</td>\n",
              "      <td>0.431475</td>\n",
              "      <td>0.824567</td>\n",
              "      <td>1.398811</td>\n",
              "      <td>1.749571</td>\n",
              "      <td>1.620790</td>\n",
              "      <td>3.000057</td>\n",
              "      <td>...</td>\n",
              "      <td>9.087567</td>\n",
              "      <td>6.055478</td>\n",
              "      <td>5.754460</td>\n",
              "      <td>3.576048</td>\n",
              "      <td>7.038605</td>\n",
              "      <td>3.246110</td>\n",
              "      <td>1.555152</td>\n",
              "      <td>2.391660</td>\n",
              "      <td>6.500049</td>\n",
              "      <td>6.424990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1798</th>\n",
              "      <td>F0005995</td>\n",
              "      <td>3.354477</td>\n",
              "      <td>6.365976</td>\n",
              "      <td>5.308424</td>\n",
              "      <td>3.864825</td>\n",
              "      <td>1.422130</td>\n",
              "      <td>1.564374</td>\n",
              "      <td>4.567979</td>\n",
              "      <td>2.279386</td>\n",
              "      <td>2.480613</td>\n",
              "      <td>...</td>\n",
              "      <td>1.687940</td>\n",
              "      <td>1.070761</td>\n",
              "      <td>0.890398</td>\n",
              "      <td>0.586215</td>\n",
              "      <td>0.690286</td>\n",
              "      <td>0.678785</td>\n",
              "      <td>1.683590</td>\n",
              "      <td>2.352469</td>\n",
              "      <td>2.948082</td>\n",
              "      <td>2.060496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1799</th>\n",
              "      <td>F0005999</td>\n",
              "      <td>0.621699</td>\n",
              "      <td>0.520761</td>\n",
              "      <td>1.068800</td>\n",
              "      <td>3.859938</td>\n",
              "      <td>3.392767</td>\n",
              "      <td>1.810717</td>\n",
              "      <td>1.987533</td>\n",
              "      <td>0.679129</td>\n",
              "      <td>0.542923</td>\n",
              "      <td>...</td>\n",
              "      <td>11.724424</td>\n",
              "      <td>21.167513</td>\n",
              "      <td>5.204610</td>\n",
              "      <td>8.067691</td>\n",
              "      <td>4.598520</td>\n",
              "      <td>3.966576</td>\n",
              "      <td>3.266056</td>\n",
              "      <td>2.167287</td>\n",
              "      <td>2.578887</td>\n",
              "      <td>6.533395</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1800 rows × 401 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            ID       loc1       loc2       loc3       loc4       loc5  \\\n",
              "0     F0000009   2.303276   2.633120   0.967853   0.711195   0.612320   \n",
              "1     F0000011   6.329229   5.513133   3.744405   2.303595   1.706632   \n",
              "2     F0000012   3.611720   5.277518   3.246185   1.668042   0.475114   \n",
              "3     F0000014   2.683136   2.873166   1.564418   0.715690   0.310166   \n",
              "4     F0000017   0.234505   0.538168   0.824665   1.241247   0.794359   \n",
              "5     F0000018   3.106924   4.724935   3.024786   2.114809   1.122492   \n",
              "6     F0000024   1.509835   1.381073   2.387015   1.938448   1.519532   \n",
              "7     F0000036   4.549304   4.280723   2.687711   7.107157   5.854353   \n",
              "8     F0000046   5.534216   9.097797  25.130870  15.975117   8.560527   \n",
              "9     F0000047   1.926024   2.275108   2.933197   3.494335   2.513233   \n",
              "10    F0000048  20.652632  33.995995  52.926582  17.868387   7.314888   \n",
              "11    F0000051  12.437338   6.264972   2.177781   1.558816   1.509240   \n",
              "12    F0000053   0.910943   1.103605   0.767685   0.974606   1.286200   \n",
              "13    F0000054   0.750083   1.394417   3.304441   1.831993   0.756562   \n",
              "14    F0000057   0.741816   0.657264   0.695477   0.558093   0.701038   \n",
              "15    F0000061  33.306690  19.954653  10.333847   8.740312   3.814435   \n",
              "16    F0000064   2.144422   1.269146   0.591189   0.442271   1.035759   \n",
              "17    F0000066  11.645531   7.111301   3.323163   2.183275   2.512917   \n",
              "18    F0000067   3.080466   2.188396   1.882559   1.483453   1.545256   \n",
              "19    F0000077   1.500818   1.489934   1.635062   1.498792   1.547774   \n",
              "20    F0000078   2.713030   3.408247   4.660220   4.057150   3.293992   \n",
              "21    F0000079   6.621870   5.868481   3.552604   3.260004   5.629075   \n",
              "22    F0000088   0.518932   0.464432   0.612854   0.785798   0.438779   \n",
              "23    F0000092   0.269790   0.404652   0.366466   0.336078   0.435278   \n",
              "24    F0000100   1.038231   1.136319   0.885931   0.750078   1.665736   \n",
              "25    F0000103   0.477743   0.786121   1.645252   1.224810   1.019933   \n",
              "26    F0000104   1.388092   1.576286   0.858428   0.551944   1.830480   \n",
              "27    F0000105   0.714076   0.891166   1.478718   0.519320   0.711848   \n",
              "28    F0000107   1.905543   1.602184   1.044503   0.886893   1.467033   \n",
              "29    F0000109   0.149255   0.071287   0.178389   0.316939   0.597959   \n",
              "...        ...        ...        ...        ...        ...        ...   \n",
              "1770  F0005900   1.323523   1.295901   1.455428   0.753431   1.428795   \n",
              "1771  F0005905   3.209217   1.524227   2.423214   5.761286   6.936222   \n",
              "1772  F0005912   3.368048   2.576362   2.269005   2.212387   2.108429   \n",
              "1773  F0005915   0.961125   0.819193   0.647220   1.297425   0.925135   \n",
              "1774  F0005916   0.488487   0.539411   1.140720   1.281139   0.946188   \n",
              "1775  F0005917   2.601917   1.570904   0.722545   0.909375   0.741372   \n",
              "1776  F0005923   1.229617   1.345330   1.348163   2.388293   3.126362   \n",
              "1777  F0005926   1.132353   1.220101   0.850112   1.412708   1.596978   \n",
              "1778  F0005927   0.707323   0.664875   0.719043   1.259513   0.607309   \n",
              "1779  F0005928   6.045567  11.909250   7.740517   2.534117   2.461152   \n",
              "1780  F0005933   0.533600   0.392396   0.850073   1.531713   2.185963   \n",
              "1781  F0005937   1.116768   1.125264   0.485512   0.354130   1.154607   \n",
              "1782  F0005942   1.999618   1.056652   0.795645   0.706524   0.451899   \n",
              "1783  F0005946   1.027266   1.026623   0.857798   1.793024   2.197954   \n",
              "1784  F0005948   1.860262   1.437228   0.612339   0.457762   0.557145   \n",
              "1785  F0005950   0.134930   0.358618   0.891062   1.308883   1.202681   \n",
              "1786  F0005952   1.526733   2.632045   7.510481   5.281970   2.729181   \n",
              "1787  F0005955   3.890497   3.962250   3.083796   1.340853   1.436267   \n",
              "1788  F0005956   3.130267   2.281266   1.638254   1.549735   2.230529   \n",
              "1789  F0005959  23.326921  28.488647  45.502148  15.296210  10.865436   \n",
              "1790  F0005961   0.706635   0.841394   0.419348   0.221148   0.492536   \n",
              "1791  F0005964   0.690461   0.818102   0.435939   0.419264   0.898035   \n",
              "1792  F0005965   5.417135   4.019561   3.426266   2.289643   1.505351   \n",
              "1793  F0005968   2.353701   2.081669   2.275276   2.440298   1.453248   \n",
              "1794  F0005973   0.414213   0.545782   0.448600   0.326854   0.274612   \n",
              "1795  F0005979   1.040663   1.284618   1.099170   0.664099   0.604902   \n",
              "1796  F0005982   0.623600   0.639799   1.029405   0.905027   0.632461   \n",
              "1797  F0005994   0.937487   0.593661   0.306112   0.431475   0.824567   \n",
              "1798  F0005995   3.354477   6.365976   5.308424   3.864825   1.422130   \n",
              "1799  F0005999   0.621699   0.520761   1.068800   3.859938   3.392767   \n",
              "\n",
              "          loc6      loc7       loc8       loc9    ...         loc391  \\\n",
              "0     0.606436  0.659745   0.903510   1.337734    ...      14.198577   \n",
              "1     2.405296  1.752342   1.457627   0.967599    ...       2.244529   \n",
              "2     0.388565  0.637970   0.502681   0.655798    ...       1.773896   \n",
              "3     0.841908  2.362045   2.305695   1.127642    ...       2.207136   \n",
              "4     0.691293  2.202758   3.200006   2.721208    ...       6.542584   \n",
              "5     0.463340  0.880873   1.015929   1.780746    ...       2.085273   \n",
              "6     1.164685  1.023207   1.416044   1.884219    ...       0.844937   \n",
              "7     6.881705  8.659466   5.750557   8.395898    ...       8.959080   \n",
              "8     4.248939  2.432146   2.700563   4.332023    ...       4.914675   \n",
              "9     1.539907  1.563043   1.575975   2.180220    ...       5.074250   \n",
              "10    5.734422  4.157221   5.034347  15.480470    ...       3.063334   \n",
              "11    3.364384  3.158484   1.261669   2.506135    ...       7.489040   \n",
              "12    1.468730  1.025374   1.041733   1.424034    ...       0.668139   \n",
              "13    0.628499  0.816878   3.019567   5.123874    ...       1.721946   \n",
              "14    1.150451  1.679969   1.638696   2.425700    ...       2.144041   \n",
              "15    2.572137  2.151315   2.702223   1.683614    ...       0.471152   \n",
              "16    1.504987  1.264643   2.248283   2.694662    ...       1.940963   \n",
              "17    2.598790  1.745974   1.139034   1.535812    ...       1.111168   \n",
              "18    1.831146  2.026785   1.394369   1.169270    ...       0.841970   \n",
              "19    0.910363  1.886299   2.878254   2.859822    ...      18.453215   \n",
              "20    2.931879  1.991378   0.806612   0.948601    ...       0.611694   \n",
              "21    5.656404  5.099396   5.432373   5.272851    ...       0.906899   \n",
              "22    0.685559  1.624833   1.721921   1.228552    ...       2.578208   \n",
              "23    0.310754  0.456227   0.988275   0.675815    ...       5.928824   \n",
              "24    2.860888  1.548715   1.701221   2.488202    ...       7.957812   \n",
              "25    1.382263  0.995078   1.125195   1.225753    ...      10.511009   \n",
              "26    1.562317  1.816000   3.216226   2.353058    ...       1.428833   \n",
              "27    1.246055  0.783323   0.585090   0.453591    ...       2.320153   \n",
              "28    1.596956  1.561978   1.867255   2.737040    ...       4.258166   \n",
              "29    1.011843  0.883955   4.134008   3.133165    ...       0.654023   \n",
              "...        ...       ...        ...        ...    ...            ...   \n",
              "1770  2.009519  0.672810   0.888415   1.040625    ...       3.000496   \n",
              "1771  6.638852  8.835508   7.993683   7.503543    ...       0.678277   \n",
              "1772  1.106514  0.964778   0.696674   0.462625    ...       4.031189   \n",
              "1773  1.757982  1.187254   1.644746   3.944567    ...      17.345568   \n",
              "1774  0.796534  1.181490   3.118390   3.516017    ...       2.327692   \n",
              "1775  1.179749  2.833306   3.598268   4.436368    ...       2.975135   \n",
              "1776  3.293191  3.489855   2.391400   5.047786    ...       6.122244   \n",
              "1777  2.247459  6.328268  11.974008   4.930750    ...       3.248424   \n",
              "1778  0.380661  0.878974   0.869244   0.626905    ...       4.469632   \n",
              "1779  1.939848  3.434960  18.061798  36.109850    ...       2.875150   \n",
              "1780  0.868813  0.382021   0.438814   0.439318    ...       6.601388   \n",
              "1781  3.042722  4.939437   3.706196   2.409788    ...       3.222061   \n",
              "1782  0.418066  1.079368   3.284924   5.180380    ...       0.865656   \n",
              "1783  2.197917  2.270341   1.566499   1.670121    ...       1.734436   \n",
              "1784  0.727749  0.900840   1.168446   0.598219    ...       3.239780   \n",
              "1785  0.669300  0.621382   1.252377   2.280426    ...       7.137218   \n",
              "1786  3.904912  8.062960   3.712467   3.136803    ...       2.749143   \n",
              "1787  2.652954  1.781050   1.483055   3.559106    ...       4.885341   \n",
              "1788  1.306892  1.091675   1.188698   1.046413    ...       1.368070   \n",
              "1789  6.963024  5.583030   5.447122   8.040642    ...       3.093215   \n",
              "1790  1.049918  0.764957   0.459775   0.563182    ...       4.409362   \n",
              "1791  2.271854  1.431377   1.365225   2.764257    ...       1.505241   \n",
              "1792  2.420017  1.446742   0.801799   0.793399    ...       1.110762   \n",
              "1793  1.266195  0.998716   1.043930   1.306492    ...       1.857343   \n",
              "1794  0.392778  0.637624   0.460694   0.530263    ...       7.854734   \n",
              "1795  0.820490  1.483253   0.765457   1.476914    ...       9.877131   \n",
              "1796  0.685374  1.351671   1.946942   2.264127    ...       1.651571   \n",
              "1797  1.398811  1.749571   1.620790   3.000057    ...       9.087567   \n",
              "1798  1.564374  4.567979   2.279386   2.480613    ...       1.687940   \n",
              "1799  1.810717  1.987533   0.679129   0.542923    ...      11.724424   \n",
              "\n",
              "         loc392     loc393     loc394     loc395     loc396     loc397  \\\n",
              "0      8.562691   2.988225   5.268833   4.511308   2.368932   3.126402   \n",
              "1      2.195623   1.610037   1.782069   1.366208   0.615595   0.960376   \n",
              "2      2.336798   1.996541   1.867982   1.414425   0.676193   0.828465   \n",
              "3      2.887102   8.945212   9.453363  10.373502   6.186830   5.693193   \n",
              "4     22.577510  32.344440  18.713957  15.682641   5.173859   2.250664   \n",
              "5      2.548799   0.884403   1.468000   3.873965   5.753904   2.968388   \n",
              "6      1.718213   3.039999   2.531460   2.925235   4.045678   2.195813   \n",
              "7      4.130376   4.660564   7.090211  10.861546  14.283480  18.413473   \n",
              "8      6.751660   6.020052   3.641826   4.516870  10.352589  11.004499   \n",
              "9      6.098646   4.938331   3.050952   4.389782   5.872658   5.134856   \n",
              "10     1.055917   0.393314   0.385540   0.623621   1.025196   0.418536   \n",
              "11     4.709154   2.873084   1.953563   2.137640   4.766042  14.213503   \n",
              "12     0.236537   0.457683   1.008293   2.800187   3.738060   6.314335   \n",
              "13     2.220998   0.742731   0.330098   0.196398   0.629299   2.477335   \n",
              "14     3.300222   4.771925   6.796495   3.329586   2.940020   9.063610   \n",
              "15     0.841153   2.122401   3.926712   4.045723   4.517091   3.362950   \n",
              "16     1.301586   1.700055   1.908244   2.415176   4.074230   3.452168   \n",
              "17     0.502294   0.430600   0.967143   0.930188   4.759078   5.213235   \n",
              "18     0.883476   0.953351   1.408569   1.596759   1.974259   1.388235   \n",
              "19     6.355784   4.577859   9.068478   5.294091   4.772662   5.571238   \n",
              "20     1.523846   1.907247   3.170228   3.494537   2.894914   4.021257   \n",
              "21     1.727650   3.224317   7.051887  11.790594  16.643347   7.049145   \n",
              "22     1.937333   1.724551   3.365602   2.924775   3.078687   2.782127   \n",
              "23     5.870936   3.280750   1.706294   1.197328   2.347076   3.853735   \n",
              "24    11.172690   6.819529   3.797462   2.592548   1.742366   2.864707   \n",
              "25     6.419022   5.407313   2.035063   2.469227   2.116930   1.815346   \n",
              "26     1.245269   0.472492   1.225905   1.285642   1.005735   0.724017   \n",
              "27     2.394096   0.476611   1.266176   0.865764   0.722571   1.322277   \n",
              "28     6.741268  10.497833  10.431764   7.872065  11.145880   6.622912   \n",
              "29     1.257039   3.813466   7.821394   4.479205   5.301716   7.689736   \n",
              "...         ...        ...        ...        ...        ...        ...   \n",
              "1770   2.139266   1.611814   1.483957   1.450607   1.442716   1.521094   \n",
              "1771   0.815854   1.192992   0.446210   0.559226   1.476782   1.545443   \n",
              "1772   4.777591   4.667728   2.181439   0.982725   1.249798   1.109017   \n",
              "1773   8.764386   5.616642   2.226284   0.976604   1.439587   1.421871   \n",
              "1774   4.457804   2.626051   1.328008   2.163116   1.479488   2.761025   \n",
              "1775   4.605570   4.312715   4.348446   2.648516   2.593340   2.531201   \n",
              "1776   6.090610   3.748130   4.084436   5.214523   6.685337   7.968226   \n",
              "1777   3.912272   3.434962   7.491676  17.559752  28.621363  22.057838   \n",
              "1778   4.028175   3.718959   1.926531   1.365657   1.266352   1.624813   \n",
              "1779   3.679949   2.375665   2.557974   1.981741   1.048983   0.559934   \n",
              "1780   5.301770   4.021045   3.825714   2.177808   2.975960   5.191958   \n",
              "1781   3.858958   4.335448   1.635573   0.701351   0.478707   0.512853   \n",
              "1782   1.302519   0.926325   1.057391   0.782206   0.754341   1.310361   \n",
              "1783   2.563675   2.337415   2.531934   3.913168   2.907942   2.541398   \n",
              "1784   1.902029   0.780457   1.520281   1.655513   1.466943   1.694931   \n",
              "1785   6.398832   5.180192   3.517964   4.741568   3.754290   1.797403   \n",
              "1786   3.058265   2.378554   3.386085   5.385571   5.340822   2.236870   \n",
              "1787   2.184284   2.119546   2.796220   1.850744   0.836297   0.891153   \n",
              "1788   1.671874   3.514567   7.493694   5.789693   4.361060   8.863407   \n",
              "1789   3.797435   4.416099   6.080209   7.273672   7.806881   2.414357   \n",
              "1790   4.895787   3.912370   2.656031   2.856354   4.010345  13.404252   \n",
              "1791   1.218970   1.354566   1.936176   1.732354   1.796404   1.419424   \n",
              "1792   0.790540   0.881395   0.786984   0.923787   1.764753   2.305818   \n",
              "1793   1.308526   0.701224   1.853004   1.776184   0.792995   0.626827   \n",
              "1794  10.525514  10.372069   5.984866   2.052454   1.326070   3.136443   \n",
              "1795  14.893926  16.550638   5.936255   7.014977   4.755193   3.853685   \n",
              "1796   1.745623   1.716137   3.352444   2.795984   3.564191   5.784504   \n",
              "1797   6.055478   5.754460   3.576048   7.038605   3.246110   1.555152   \n",
              "1798   1.070761   0.890398   0.586215   0.690286   0.678785   1.683590   \n",
              "1799  21.167513   5.204610   8.067691   4.598520   3.966576   3.266056   \n",
              "\n",
              "         loc398     loc399     loc400  \n",
              "0      3.378817   4.378636   3.183275  \n",
              "1      0.557508   0.523693   0.649285  \n",
              "2      1.352033   1.516963   0.926687  \n",
              "3      4.179819   2.414889   3.788618  \n",
              "4      1.221537   5.205012   2.196197  \n",
              "5      1.870800   4.490972   9.098993  \n",
              "6      2.420788   4.041779   3.283170  \n",
              "7      7.699151   7.086923   8.154278  \n",
              "8      9.246391   8.224123  10.247271  \n",
              "9      6.942337  10.453601   8.602502  \n",
              "10     0.796182   4.360134   9.518021  \n",
              "11    29.409958  26.610662  13.395562  \n",
              "12     8.115027   3.156437   1.343943  \n",
              "13     3.594346   5.750381   2.457064  \n",
              "14    14.344852  29.723429  37.220543  \n",
              "15     1.696191   2.818758   2.435935  \n",
              "16     4.494447   5.617652   9.539695  \n",
              "17     2.600242   2.966337   1.876263  \n",
              "18     0.532499   0.679541   0.625160  \n",
              "19    13.266127   7.437178   1.510383  \n",
              "20     4.599502   3.673347   3.549517  \n",
              "21     8.583964  17.633648  16.254145  \n",
              "22     1.333418   1.685459   1.524903  \n",
              "23     3.917545   5.539401   3.259628  \n",
              "24     2.425534   2.244323   2.238122  \n",
              "25     1.955187   1.567535   1.478904  \n",
              "26     0.503107   0.512637   0.416839  \n",
              "27     2.253647   5.780131   4.890282  \n",
              "28     3.463826   1.891015   1.140295  \n",
              "29     2.943549   1.791588   2.820437  \n",
              "...         ...        ...        ...  \n",
              "1770   7.569211   4.721660   3.440695  \n",
              "1771   1.361798   2.982006   5.245597  \n",
              "1772   1.440018   2.662548   1.521867  \n",
              "1773   1.214840   1.680860   1.278092  \n",
              "1774   5.556460   6.655053   6.380516  \n",
              "1775   1.229766   0.925853   0.317516  \n",
              "1776   3.614980   4.437085   6.718983  \n",
              "1777  14.134838  11.995984   8.610427  \n",
              "1778   4.899743   3.889246   2.353610  \n",
              "1779   0.868441   2.852430   8.421205  \n",
              "1780   4.825273   3.771930   6.425722  \n",
              "1781   1.914670   6.387129  11.925665  \n",
              "1782   4.562486   3.545104   7.773433  \n",
              "1783   1.149108   1.044398   1.011177  \n",
              "1784   0.793160   1.123061   2.759417  \n",
              "1785   2.176733   1.071721   1.128544  \n",
              "1786   1.659592   0.990404   0.901925  \n",
              "1787   1.067829   1.885773   2.311086  \n",
              "1788   5.340484   3.854682   5.021036  \n",
              "1789   2.722545   1.655113   0.957200  \n",
              "1790  24.783630  22.772015  27.581194  \n",
              "1791   1.370859   1.918672   2.887728  \n",
              "1792   1.591236   2.073917   2.156827  \n",
              "1793   1.250852   1.429829   0.997534  \n",
              "1794   4.032788   3.178754   3.387637  \n",
              "1795   3.796204   1.744708   1.348829  \n",
              "1796   3.273253   2.668455   3.121544  \n",
              "1797   2.391660   6.500049   6.424990  \n",
              "1798   2.352469   2.948082   2.060496  \n",
              "1799   2.167287   2.578887   6.533395  \n",
              "\n",
              "[1800 rows x 401 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "H35ZKByQwx17",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp drive/aoi_test/t_brain/geo/test_submit.csv ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FBX-GE72RnE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('test_submit.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wm5M3X9z0enJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}