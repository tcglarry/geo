{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geo_stage_2_21x21x5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcglarry/geo/blob/master/geo_stage_2_21x21x5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kUbioL1FmGXN",
        "colab_type": "code",
        "outputId": "84dd3fd5-1d11-468c-81ff-e377fe7a3b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"ali_build_model_stage_2_MobileNetV2_aug_1000_dense.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1wSVEmcoWzRm-7B6UGkHB8ysfMDRT_xK5\n",
        "\"\"\"\n",
        "\n",
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8aoJd93CmGao",
        "colab_type": "code",
        "outputId": "694c8e6e-8950-4cf8-881d-29617f3f7049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cD6mcE4Zl4m1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "src=  'drive/My Drive/geo/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gUqwwxril4m5",
        "colab_type": "code",
        "outputId": "9ee1832c-3f65-4553-b1c9-a81c4b0ff44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import subprocess\n",
        "import os\n",
        "import pickle\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, merge, Lambda,UpSampling2D, concatenate, \\\n",
        "Conv2DTranspose,BatchNormalization\n",
        "from keras.models import Model, load_model\n",
        "import pandas as pd\n",
        "import sklearn \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras import regularizers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "IhIuNLS6l4nC",
        "colab_type": "code",
        "outputId": "3ed6a261-7dda-429a-a3e1-784d58c3339a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "data= np.load(src+'train_data.npy')\n",
        "label = np.load(src+'train_labels.npy')\n",
        "label = label[:,:,:,np.newaxis]\n",
        "print (data.shape)\n",
        "print (label.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4200, 21, 21, 20)\n",
            "(4200, 20, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wtZI1t8InGyu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reduce channel from 21 to 11"
      ]
    },
    {
      "metadata": {
        "id": "VIin_TiJl4nH",
        "colab_type": "code",
        "outputId": "a6956987-527a-45cf-861f-49ba074b9740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "index_list = list(range(20))\n",
        "select_list = [idx for idx in index_list if idx % 4  == 0 ]\n",
        "select_list\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 4, 8, 12, 16]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "gY3f1GFgn5gl",
        "colab_type": "code",
        "outputId": "7d3b37ee-137e-4de2-8e88-fd1f7e751922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data = data[:,:,:,select_list]\n",
        "data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4200, 21, 21, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "Zlc2JmA2w_RG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_mean = np.mean(data)\n",
        "data_std = np.std(data)\n",
        "label_max = np.max(label)\n",
        "label_min = np.min(label)\n",
        "\n",
        "train_data = (data - data_mean)/data_std\n",
        "#label_data = (label-label_min)/(label_max - label_min)\n",
        "label_data = np.log(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iGeIz5AWl4nL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_x,val_x,train_y,val_y = train_test_split(train_data,label_data,test_size=0.2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_F7YnLWqzBU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_block(ch, activation= 'relu', padding='same',kernel_regularizer=regularizers.l2(0.01)):       \n",
        "    return Conv2D(ch,(3,3),activation= activation, padding =padding )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C8sDm7fZl4nQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#def build_model_1(IMAGE_HEIGHT=IMAGE_HEIGHT,IMAGE_WIDTH=IMAGE_WIDTH,ch=ch):\n",
        "def build_model_1(ch=8):\n",
        "    #inputs = Input((IMAGE_HEIGHT,IMAGE_WIDTH,ch))\n",
        "    inputs = Input((21,21,5))\n",
        "\n",
        "    conv0 = Conv2D(32,(2,2),padding='valid')(inputs)\n",
        "    print ('conv0',conv0.get_shape())\n",
        "    conv1 =  conv_block(ch)(conv0)\n",
        "    conv1 = conv_block(ch)(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
        "    print ('pool1',pool1.get_shape())\n",
        "    \n",
        "    conv2 =  conv_block(ch*2)(pool1)\n",
        "    conv2 = conv_block(ch*2)(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
        "    print ('poo12',pool2.get_shape())\n",
        "    \n",
        "    conv_test = Conv2D(64,(2,2),padding='valid')(pool2)\n",
        "    print ('conv_test',conv_test.get_shape())\n",
        "    \n",
        "    \n",
        "    \n",
        "    conv3 =  conv_block(ch*4)(conv_test)\n",
        "    conv3 = conv_block(ch*4)(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
        "    print ('poo13',pool3.get_shape())\n",
        "    \n",
        "    conv4 =  conv_block(ch*8)(pool3)\n",
        "    conv4 = conv_block(ch*8)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2,2))(conv4)\n",
        "    print ('poo14',pool4.get_shape())\n",
        "    \n",
        "    conv5 =  conv_block(ch*16)(pool4)\n",
        "    conv5 = conv_block(ch*16)(conv5)\n",
        "\n",
        "    \n",
        "    up6 = concatenate([UpSampling2D(size=(2,2))(conv5), conv4], axis=3)\n",
        "    conv6 = conv_block(ch*8)(up6)\n",
        "    conv6 = conv_block(ch*8)(conv6)\n",
        "    \n",
        "    up7 = concatenate ([UpSampling2D(size=(2,2))(conv6), conv3],  axis=3)\n",
        "    conv7 = conv_block(ch*4)(up7)\n",
        "    conv7 = conv_block(ch*4)(conv7)\n",
        "    print ('conv7',conv7.get_shape())\n",
        "    conv7 = Conv2DTranspose(ch*4,(2,2),padding='valid')(conv7)\n",
        "    print ('conv7',conv7.get_shape())\n",
        "    \n",
        "    up8 = concatenate([UpSampling2D(size=(2,2))(conv7), conv2], axis=3)\n",
        "    conv8 = conv_block(ch*2)(up8)\n",
        "    conv8 = conv_block(ch*2)(conv8)\n",
        "    \n",
        "    up9 = concatenate([UpSampling2D(size=(2,2))(conv8), conv1],  axis=3)\n",
        "    conv9 = conv_block(ch)(up9)\n",
        "    conv9 = conv_block(ch)(conv9)\n",
        "    \n",
        "    \n",
        "    \n",
        "    conv10 = Conv2D(1,(1,1))(conv9)\n",
        "    \n",
        "  \n",
        "    model = Model(inputs= inputs, outputs=conv10)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WqpLnCCCoXnF",
        "colab_type": "code",
        "outputId": "57a790ab-80fc-4023-f5b0-15ff4ad28b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1598
        }
      },
      "cell_type": "code",
      "source": [
        "model = build_model_1()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv0 (?, 20, 20, 32)\n",
            "pool1 (?, 10, 10, 8)\n",
            "poo12 (?, 5, 5, 16)\n",
            "conv_test (?, 4, 4, 64)\n",
            "poo13 (?, 2, 2, 32)\n",
            "poo14 (?, 1, 1, 64)\n",
            "conv7 (?, 4, 4, 32)\n",
            "conv7 (?, ?, ?, 32)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 21, 21, 5)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 20, 20, 32)   672         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 20, 20, 8)    2312        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 20, 20, 8)    584         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 10, 10, 8)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 10, 10, 16)   1168        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 10, 10, 16)   2320        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 16)     0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 4, 4, 64)     4160        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 4, 4, 32)     18464       conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 4, 4, 32)     9248        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 2, 2, 32)     0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 2, 2, 64)     18496       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 2, 2, 64)     36928       conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 64)     0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 1, 1, 128)    73856       max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 1, 1, 128)    147584      conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 2, 2, 128)    0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2, 2, 192)    0           up_sampling2d_1[0][0]            \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 2, 2, 64)     110656      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 2, 2, 64)     36928       conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 4, 4, 64)     0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 4, 4, 96)     0           up_sampling2d_2[0][0]            \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 32)     27680       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 32)     9248        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 5, 5, 32)     4128        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 10, 10, 32)   0           conv2d_transpose_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 10, 10, 48)   0           up_sampling2d_3[0][0]            \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 10, 10, 16)   6928        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 10, 10, 16)   2320        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 20, 20, 16)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 20, 20, 24)   0           up_sampling2d_4[0][0]            \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 20, 20, 8)    1736        concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 20, 20, 8)    584         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 20, 20, 1)    9           conv2d_20[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 516,009\n",
            "Trainable params: 516,009\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mTSiLPYtl4nU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile (optimizer='Adam', loss = 'mse', metrics =['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bQ1YM-aml4nZ",
        "colab_type": "code",
        "outputId": "bb34b1d8-4634-4ea5-877b-55d3ea2ea1fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# train the model on the new data for a few epochs\n",
        "\n",
        "use_model= False\n",
        "if use_model:\n",
        "    \n",
        "\n",
        "    if os.path.isfile(src+'water_first_try.h5'):\n",
        "\n",
        "      model = load_model(src+'water_first_try.h5')\n",
        "      print ('model laoded')\n",
        "    else:\n",
        "      print ('model not exist')\n",
        "else:\n",
        "  print ('not to use model')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "not to use model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U2NKk1wFl4nc",
        "colab_type": "code",
        "outputId": "fcd40caa-6a3c-4551-b2c1-1939f4109b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16372
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=src+'geo_stage2_21x21x5_nrmse_loss.h5', monitor = 'val_loss', save_best_only=True, mode= 'auto')\n",
        "earlystop = EarlyStopping(patience=30)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=15, min_lr=0.00001,verbose=1)\n",
        "callback_list = [checkpoint, earlystop]\n",
        "model.fit(x=train_x, y=train_y, batch_size=32, epochs=1000, callbacks = callback_list, verbose=1, validation_split=0.2,  shuffle=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2688 samples, validate on 672 samples\n",
            "Epoch 1/1000\n",
            "2688/2688 [==============================] - 5s 2ms/step - loss: 0.9732 - mean_absolute_error: 0.7781 - val_loss: 0.6264 - val_mean_absolute_error: 0.6274\n",
            "Epoch 2/1000\n",
            "2688/2688 [==============================] - 3s 933us/step - loss: 0.5352 - mean_absolute_error: 0.5787 - val_loss: 0.4629 - val_mean_absolute_error: 0.5369\n",
            "Epoch 3/1000\n",
            "2688/2688 [==============================] - 3s 934us/step - loss: 0.4357 - mean_absolute_error: 0.5192 - val_loss: 0.3902 - val_mean_absolute_error: 0.4903\n",
            "Epoch 4/1000\n",
            "2688/2688 [==============================] - 3s 937us/step - loss: 0.3535 - mean_absolute_error: 0.4648 - val_loss: 0.3300 - val_mean_absolute_error: 0.4474\n",
            "Epoch 5/1000\n",
            "2688/2688 [==============================] - 2s 927us/step - loss: 0.3106 - mean_absolute_error: 0.4350 - val_loss: 0.2871 - val_mean_absolute_error: 0.4174\n",
            "Epoch 6/1000\n",
            "2688/2688 [==============================] - 3s 943us/step - loss: 0.2808 - mean_absolute_error: 0.4132 - val_loss: 0.2829 - val_mean_absolute_error: 0.4171\n",
            "Epoch 7/1000\n",
            "2688/2688 [==============================] - 2s 927us/step - loss: 0.2617 - mean_absolute_error: 0.3984 - val_loss: 0.2496 - val_mean_absolute_error: 0.3884\n",
            "Epoch 8/1000\n",
            "2688/2688 [==============================] - 2s 926us/step - loss: 0.2466 - mean_absolute_error: 0.3863 - val_loss: 0.2418 - val_mean_absolute_error: 0.3830\n",
            "Epoch 9/1000\n",
            "2688/2688 [==============================] - 3s 938us/step - loss: 0.2336 - mean_absolute_error: 0.3756 - val_loss: 0.2328 - val_mean_absolute_error: 0.3746\n",
            "Epoch 10/1000\n",
            "2688/2688 [==============================] - 3s 935us/step - loss: 0.2228 - mean_absolute_error: 0.3668 - val_loss: 0.2155 - val_mean_absolute_error: 0.3613\n",
            "Epoch 11/1000\n",
            "2688/2688 [==============================] - 2s 924us/step - loss: 0.2146 - mean_absolute_error: 0.3597 - val_loss: 0.2071 - val_mean_absolute_error: 0.3524\n",
            "Epoch 12/1000\n",
            "2688/2688 [==============================] - 3s 937us/step - loss: 0.2068 - mean_absolute_error: 0.3527 - val_loss: 0.2035 - val_mean_absolute_error: 0.3497\n",
            "Epoch 13/1000\n",
            "2688/2688 [==============================] - 2s 923us/step - loss: 0.1995 - mean_absolute_error: 0.3463 - val_loss: 0.1991 - val_mean_absolute_error: 0.3486\n",
            "Epoch 14/1000\n",
            "2688/2688 [==============================] - 3s 979us/step - loss: 0.1937 - mean_absolute_error: 0.3410 - val_loss: 0.1939 - val_mean_absolute_error: 0.3409\n",
            "Epoch 15/1000\n",
            "2688/2688 [==============================] - 2s 924us/step - loss: 0.1889 - mean_absolute_error: 0.3369 - val_loss: 0.1883 - val_mean_absolute_error: 0.3380\n",
            "Epoch 16/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.1846 - mean_absolute_error: 0.3330 - val_loss: 0.1860 - val_mean_absolute_error: 0.3337\n",
            "Epoch 17/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.1783 - mean_absolute_error: 0.3272 - val_loss: 0.1807 - val_mean_absolute_error: 0.3280\n",
            "Epoch 18/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.1732 - mean_absolute_error: 0.3222 - val_loss: 0.1698 - val_mean_absolute_error: 0.3194\n",
            "Epoch 19/1000\n",
            "2688/2688 [==============================] - 2s 908us/step - loss: 0.1706 - mean_absolute_error: 0.3200 - val_loss: 0.1711 - val_mean_absolute_error: 0.3213\n",
            "Epoch 20/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.1648 - mean_absolute_error: 0.3144 - val_loss: 0.1661 - val_mean_absolute_error: 0.3157\n",
            "Epoch 21/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.1614 - mean_absolute_error: 0.3111 - val_loss: 0.1633 - val_mean_absolute_error: 0.3131\n",
            "Epoch 22/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.1572 - mean_absolute_error: 0.3070 - val_loss: 0.1672 - val_mean_absolute_error: 0.3170\n",
            "Epoch 23/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.1536 - mean_absolute_error: 0.3035 - val_loss: 0.1574 - val_mean_absolute_error: 0.3065\n",
            "Epoch 24/1000\n",
            "2688/2688 [==============================] - 2s 920us/step - loss: 0.1498 - mean_absolute_error: 0.2997 - val_loss: 0.1523 - val_mean_absolute_error: 0.3010\n",
            "Epoch 25/1000\n",
            "2688/2688 [==============================] - 2s 919us/step - loss: 0.1462 - mean_absolute_error: 0.2960 - val_loss: 0.1507 - val_mean_absolute_error: 0.3011\n",
            "Epoch 26/1000\n",
            "2688/2688 [==============================] - 2s 921us/step - loss: 0.1438 - mean_absolute_error: 0.2936 - val_loss: 0.1504 - val_mean_absolute_error: 0.2991\n",
            "Epoch 27/1000\n",
            "2688/2688 [==============================] - 3s 931us/step - loss: 0.1425 - mean_absolute_error: 0.2923 - val_loss: 0.1509 - val_mean_absolute_error: 0.3007\n",
            "Epoch 28/1000\n",
            "2688/2688 [==============================] - 2s 925us/step - loss: 0.1407 - mean_absolute_error: 0.2903 - val_loss: 0.1456 - val_mean_absolute_error: 0.2942\n",
            "Epoch 29/1000\n",
            "2688/2688 [==============================] - 3s 936us/step - loss: 0.1362 - mean_absolute_error: 0.2858 - val_loss: 0.1401 - val_mean_absolute_error: 0.2886\n",
            "Epoch 30/1000\n",
            "2688/2688 [==============================] - 3s 982us/step - loss: 0.1329 - mean_absolute_error: 0.2822 - val_loss: 0.1426 - val_mean_absolute_error: 0.2914\n",
            "Epoch 31/1000\n",
            "2688/2688 [==============================] - 3s 938us/step - loss: 0.1308 - mean_absolute_error: 0.2798 - val_loss: 0.1406 - val_mean_absolute_error: 0.2892\n",
            "Epoch 32/1000\n",
            "2688/2688 [==============================] - 3s 939us/step - loss: 0.1283 - mean_absolute_error: 0.2772 - val_loss: 0.1400 - val_mean_absolute_error: 0.2883\n",
            "Epoch 33/1000\n",
            "2688/2688 [==============================] - 3s 943us/step - loss: 0.1269 - mean_absolute_error: 0.2758 - val_loss: 0.1382 - val_mean_absolute_error: 0.2864\n",
            "Epoch 34/1000\n",
            "2688/2688 [==============================] - 3s 932us/step - loss: 0.1251 - mean_absolute_error: 0.2737 - val_loss: 0.1355 - val_mean_absolute_error: 0.2836\n",
            "Epoch 35/1000\n",
            "2688/2688 [==============================] - 2s 926us/step - loss: 0.1228 - mean_absolute_error: 0.2713 - val_loss: 0.1389 - val_mean_absolute_error: 0.2865\n",
            "Epoch 36/1000\n",
            "2688/2688 [==============================] - 2s 920us/step - loss: 0.1192 - mean_absolute_error: 0.2674 - val_loss: 0.1362 - val_mean_absolute_error: 0.2846\n",
            "Epoch 37/1000\n",
            "2688/2688 [==============================] - 2s 927us/step - loss: 0.1180 - mean_absolute_error: 0.2661 - val_loss: 0.1322 - val_mean_absolute_error: 0.2800\n",
            "Epoch 38/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.1183 - mean_absolute_error: 0.2669 - val_loss: 0.1338 - val_mean_absolute_error: 0.2817\n",
            "Epoch 39/1000\n",
            "2688/2688 [==============================] - 2s 914us/step - loss: 0.1144 - mean_absolute_error: 0.2619 - val_loss: 0.1307 - val_mean_absolute_error: 0.2786\n",
            "Epoch 40/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.1122 - mean_absolute_error: 0.2595 - val_loss: 0.1319 - val_mean_absolute_error: 0.2800\n",
            "Epoch 41/1000\n",
            "2688/2688 [==============================] - 2s 924us/step - loss: 0.1103 - mean_absolute_error: 0.2574 - val_loss: 0.1264 - val_mean_absolute_error: 0.2749\n",
            "Epoch 42/1000\n",
            "2688/2688 [==============================] - 2s 921us/step - loss: 0.1087 - mean_absolute_error: 0.2554 - val_loss: 0.1298 - val_mean_absolute_error: 0.2782\n",
            "Epoch 43/1000\n",
            "2688/2688 [==============================] - 2s 917us/step - loss: 0.1077 - mean_absolute_error: 0.2544 - val_loss: 0.1274 - val_mean_absolute_error: 0.2751\n",
            "Epoch 44/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.1072 - mean_absolute_error: 0.2537 - val_loss: 0.1224 - val_mean_absolute_error: 0.2699\n",
            "Epoch 45/1000\n",
            "2688/2688 [==============================] - 2s 928us/step - loss: 0.1046 - mean_absolute_error: 0.2507 - val_loss: 0.1239 - val_mean_absolute_error: 0.2722\n",
            "Epoch 46/1000\n",
            "2688/2688 [==============================] - 2s 915us/step - loss: 0.1017 - mean_absolute_error: 0.2474 - val_loss: 0.1241 - val_mean_absolute_error: 0.2713\n",
            "Epoch 47/1000\n",
            "2688/2688 [==============================] - 2s 920us/step - loss: 0.1018 - mean_absolute_error: 0.2477 - val_loss: 0.1226 - val_mean_absolute_error: 0.2694\n",
            "Epoch 48/1000\n",
            "2688/2688 [==============================] - 2s 911us/step - loss: 0.0992 - mean_absolute_error: 0.2440 - val_loss: 0.1204 - val_mean_absolute_error: 0.2677\n",
            "Epoch 49/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0978 - mean_absolute_error: 0.2427 - val_loss: 0.1177 - val_mean_absolute_error: 0.2641\n",
            "Epoch 50/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0966 - mean_absolute_error: 0.2410 - val_loss: 0.1199 - val_mean_absolute_error: 0.2658\n",
            "Epoch 51/1000\n",
            "2688/2688 [==============================] - 2s 913us/step - loss: 0.0964 - mean_absolute_error: 0.2407 - val_loss: 0.1195 - val_mean_absolute_error: 0.2663\n",
            "Epoch 52/1000\n",
            "2688/2688 [==============================] - 2s 923us/step - loss: 0.0944 - mean_absolute_error: 0.2383 - val_loss: 0.1190 - val_mean_absolute_error: 0.2652\n",
            "Epoch 53/1000\n",
            "2688/2688 [==============================] - 2s 920us/step - loss: 0.0919 - mean_absolute_error: 0.2352 - val_loss: 0.1143 - val_mean_absolute_error: 0.2603\n",
            "Epoch 54/1000\n",
            "2688/2688 [==============================] - 2s 909us/step - loss: 0.0917 - mean_absolute_error: 0.2351 - val_loss: 0.1193 - val_mean_absolute_error: 0.2652\n",
            "Epoch 55/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0904 - mean_absolute_error: 0.2333 - val_loss: 0.1194 - val_mean_absolute_error: 0.2660\n",
            "Epoch 56/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0896 - mean_absolute_error: 0.2324 - val_loss: 0.1228 - val_mean_absolute_error: 0.2696\n",
            "Epoch 57/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.0885 - mean_absolute_error: 0.2311 - val_loss: 0.1170 - val_mean_absolute_error: 0.2640\n",
            "Epoch 58/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0857 - mean_absolute_error: 0.2272 - val_loss: 0.1186 - val_mean_absolute_error: 0.2626\n",
            "Epoch 59/1000\n",
            "2688/2688 [==============================] - 2s 914us/step - loss: 0.0866 - mean_absolute_error: 0.2284 - val_loss: 0.1175 - val_mean_absolute_error: 0.2621\n",
            "Epoch 60/1000\n",
            "2688/2688 [==============================] - 2s 929us/step - loss: 0.0863 - mean_absolute_error: 0.2280 - val_loss: 0.1148 - val_mean_absolute_error: 0.2601\n",
            "Epoch 61/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.0844 - mean_absolute_error: 0.2254 - val_loss: 0.1153 - val_mean_absolute_error: 0.2607\n",
            "Epoch 62/1000\n",
            "2688/2688 [==============================] - 3s 952us/step - loss: 0.0827 - mean_absolute_error: 0.2232 - val_loss: 0.1134 - val_mean_absolute_error: 0.2583\n",
            "Epoch 63/1000\n",
            "2688/2688 [==============================] - 2s 907us/step - loss: 0.0806 - mean_absolute_error: 0.2206 - val_loss: 0.1142 - val_mean_absolute_error: 0.2594\n",
            "Epoch 64/1000\n",
            "2688/2688 [==============================] - 2s 898us/step - loss: 0.0791 - mean_absolute_error: 0.2184 - val_loss: 0.1187 - val_mean_absolute_error: 0.2632\n",
            "Epoch 65/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.0813 - mean_absolute_error: 0.2216 - val_loss: 0.1137 - val_mean_absolute_error: 0.2582\n",
            "Epoch 66/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0800 - mean_absolute_error: 0.2195 - val_loss: 0.1111 - val_mean_absolute_error: 0.2557\n",
            "Epoch 67/1000\n",
            "2688/2688 [==============================] - 2s 893us/step - loss: 0.0781 - mean_absolute_error: 0.2171 - val_loss: 0.1235 - val_mean_absolute_error: 0.2705\n",
            "Epoch 68/1000\n",
            "2688/2688 [==============================] - 2s 888us/step - loss: 0.0766 - mean_absolute_error: 0.2150 - val_loss: 0.1115 - val_mean_absolute_error: 0.2558\n",
            "Epoch 69/1000\n",
            "2688/2688 [==============================] - 2s 889us/step - loss: 0.0745 - mean_absolute_error: 0.2120 - val_loss: 0.1110 - val_mean_absolute_error: 0.2552\n",
            "Epoch 70/1000\n",
            "2688/2688 [==============================] - 2s 880us/step - loss: 0.0748 - mean_absolute_error: 0.2123 - val_loss: 0.1098 - val_mean_absolute_error: 0.2533\n",
            "Epoch 71/1000\n",
            "2688/2688 [==============================] - 2s 883us/step - loss: 0.0726 - mean_absolute_error: 0.2092 - val_loss: 0.1096 - val_mean_absolute_error: 0.2546\n",
            "Epoch 72/1000\n",
            "2688/2688 [==============================] - 2s 882us/step - loss: 0.0727 - mean_absolute_error: 0.2095 - val_loss: 0.1080 - val_mean_absolute_error: 0.2518\n",
            "Epoch 73/1000\n",
            "2688/2688 [==============================] - 2s 883us/step - loss: 0.0723 - mean_absolute_error: 0.2088 - val_loss: 0.1121 - val_mean_absolute_error: 0.2556\n",
            "Epoch 74/1000\n",
            "2688/2688 [==============================] - 2s 882us/step - loss: 0.0704 - mean_absolute_error: 0.2059 - val_loss: 0.1067 - val_mean_absolute_error: 0.2508\n",
            "Epoch 75/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.0711 - mean_absolute_error: 0.2070 - val_loss: 0.1102 - val_mean_absolute_error: 0.2538\n",
            "Epoch 76/1000\n",
            "2688/2688 [==============================] - 2s 883us/step - loss: 0.0715 - mean_absolute_error: 0.2076 - val_loss: 0.1067 - val_mean_absolute_error: 0.2501\n",
            "Epoch 77/1000\n",
            "2688/2688 [==============================] - 2s 879us/step - loss: 0.0684 - mean_absolute_error: 0.2029 - val_loss: 0.1074 - val_mean_absolute_error: 0.2503\n",
            "Epoch 78/1000\n",
            "2688/2688 [==============================] - 2s 921us/step - loss: 0.0677 - mean_absolute_error: 0.2019 - val_loss: 0.1086 - val_mean_absolute_error: 0.2521\n",
            "Epoch 79/1000\n",
            "2688/2688 [==============================] - 2s 877us/step - loss: 0.0688 - mean_absolute_error: 0.2037 - val_loss: 0.1118 - val_mean_absolute_error: 0.2559\n",
            "Epoch 80/1000\n",
            "2688/2688 [==============================] - 2s 876us/step - loss: 0.0668 - mean_absolute_error: 0.2006 - val_loss: 0.1058 - val_mean_absolute_error: 0.2483\n",
            "Epoch 81/1000\n",
            "2688/2688 [==============================] - 2s 869us/step - loss: 0.0673 - mean_absolute_error: 0.2012 - val_loss: 0.1059 - val_mean_absolute_error: 0.2479\n",
            "Epoch 82/1000\n",
            "2688/2688 [==============================] - 2s 875us/step - loss: 0.0665 - mean_absolute_error: 0.2000 - val_loss: 0.1061 - val_mean_absolute_error: 0.2481\n",
            "Epoch 83/1000\n",
            "2688/2688 [==============================] - 2s 872us/step - loss: 0.0671 - mean_absolute_error: 0.2010 - val_loss: 0.1053 - val_mean_absolute_error: 0.2476\n",
            "Epoch 84/1000\n",
            "2688/2688 [==============================] - 2s 874us/step - loss: 0.0644 - mean_absolute_error: 0.1967 - val_loss: 0.1047 - val_mean_absolute_error: 0.2470\n",
            "Epoch 85/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.0641 - mean_absolute_error: 0.1965 - val_loss: 0.1057 - val_mean_absolute_error: 0.2472\n",
            "Epoch 86/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0636 - mean_absolute_error: 0.1957 - val_loss: 0.1075 - val_mean_absolute_error: 0.2516\n",
            "Epoch 87/1000\n",
            "2688/2688 [==============================] - 2s 917us/step - loss: 0.0631 - mean_absolute_error: 0.1947 - val_loss: 0.1047 - val_mean_absolute_error: 0.2469\n",
            "Epoch 88/1000\n",
            "2688/2688 [==============================] - 2s 927us/step - loss: 0.0629 - mean_absolute_error: 0.1946 - val_loss: 0.1033 - val_mean_absolute_error: 0.2448\n",
            "Epoch 89/1000\n",
            "2688/2688 [==============================] - 3s 936us/step - loss: 0.0625 - mean_absolute_error: 0.1941 - val_loss: 0.1026 - val_mean_absolute_error: 0.2449\n",
            "Epoch 90/1000\n",
            "2688/2688 [==============================] - 2s 922us/step - loss: 0.0621 - mean_absolute_error: 0.1934 - val_loss: 0.1042 - val_mean_absolute_error: 0.2470\n",
            "Epoch 91/1000\n",
            "2688/2688 [==============================] - 3s 937us/step - loss: 0.0615 - mean_absolute_error: 0.1923 - val_loss: 0.1022 - val_mean_absolute_error: 0.2441\n",
            "Epoch 92/1000\n",
            "2688/2688 [==============================] - 3s 947us/step - loss: 0.0618 - mean_absolute_error: 0.1928 - val_loss: 0.1057 - val_mean_absolute_error: 0.2488\n",
            "Epoch 93/1000\n",
            "2688/2688 [==============================] - 2s 927us/step - loss: 0.0640 - mean_absolute_error: 0.1960 - val_loss: 0.1030 - val_mean_absolute_error: 0.2443\n",
            "Epoch 94/1000\n",
            "2688/2688 [==============================] - 3s 976us/step - loss: 0.0594 - mean_absolute_error: 0.1890 - val_loss: 0.1019 - val_mean_absolute_error: 0.2438\n",
            "Epoch 95/1000\n",
            "2688/2688 [==============================] - 2s 930us/step - loss: 0.0592 - mean_absolute_error: 0.1886 - val_loss: 0.1030 - val_mean_absolute_error: 0.2443\n",
            "Epoch 96/1000\n",
            "2688/2688 [==============================] - 3s 933us/step - loss: 0.0578 - mean_absolute_error: 0.1863 - val_loss: 0.1012 - val_mean_absolute_error: 0.2430\n",
            "Epoch 97/1000\n",
            "2688/2688 [==============================] - 2s 917us/step - loss: 0.0580 - mean_absolute_error: 0.1868 - val_loss: 0.1021 - val_mean_absolute_error: 0.2442\n",
            "Epoch 98/1000\n",
            "2688/2688 [==============================] - 2s 930us/step - loss: 0.0579 - mean_absolute_error: 0.1865 - val_loss: 0.1037 - val_mean_absolute_error: 0.2451\n",
            "Epoch 99/1000\n",
            "2688/2688 [==============================] - 2s 920us/step - loss: 0.0586 - mean_absolute_error: 0.1877 - val_loss: 0.1022 - val_mean_absolute_error: 0.2442\n",
            "Epoch 100/1000\n",
            "2688/2688 [==============================] - 2s 926us/step - loss: 0.0581 - mean_absolute_error: 0.1869 - val_loss: 0.1013 - val_mean_absolute_error: 0.2424\n",
            "Epoch 101/1000\n",
            "2688/2688 [==============================] - 2s 907us/step - loss: 0.0581 - mean_absolute_error: 0.1869 - val_loss: 0.1023 - val_mean_absolute_error: 0.2443\n",
            "Epoch 102/1000\n",
            "2688/2688 [==============================] - 2s 895us/step - loss: 0.0576 - mean_absolute_error: 0.1860 - val_loss: 0.1004 - val_mean_absolute_error: 0.2414\n",
            "Epoch 103/1000\n",
            "2688/2688 [==============================] - 2s 883us/step - loss: 0.0580 - mean_absolute_error: 0.1869 - val_loss: 0.1019 - val_mean_absolute_error: 0.2437\n",
            "Epoch 104/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.0555 - mean_absolute_error: 0.1826 - val_loss: 0.0985 - val_mean_absolute_error: 0.2393\n",
            "Epoch 105/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0547 - mean_absolute_error: 0.1812 - val_loss: 0.1002 - val_mean_absolute_error: 0.2410\n",
            "Epoch 106/1000\n",
            "2688/2688 [==============================] - 2s 913us/step - loss: 0.0546 - mean_absolute_error: 0.1810 - val_loss: 0.1010 - val_mean_absolute_error: 0.2420\n",
            "Epoch 107/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.0545 - mean_absolute_error: 0.1810 - val_loss: 0.1003 - val_mean_absolute_error: 0.2414\n",
            "Epoch 108/1000\n",
            "2688/2688 [==============================] - 2s 893us/step - loss: 0.0553 - mean_absolute_error: 0.1822 - val_loss: 0.0997 - val_mean_absolute_error: 0.2400\n",
            "Epoch 109/1000\n",
            "2688/2688 [==============================] - 2s 902us/step - loss: 0.0549 - mean_absolute_error: 0.1818 - val_loss: 0.0990 - val_mean_absolute_error: 0.2397\n",
            "Epoch 110/1000\n",
            "2688/2688 [==============================] - 2s 929us/step - loss: 0.0531 - mean_absolute_error: 0.1783 - val_loss: 0.0979 - val_mean_absolute_error: 0.2381\n",
            "Epoch 111/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0540 - mean_absolute_error: 0.1801 - val_loss: 0.1024 - val_mean_absolute_error: 0.2440\n",
            "Epoch 112/1000\n",
            "2688/2688 [==============================] - 2s 907us/step - loss: 0.0550 - mean_absolute_error: 0.1816 - val_loss: 0.0980 - val_mean_absolute_error: 0.2380\n",
            "Epoch 113/1000\n",
            "2688/2688 [==============================] - 2s 908us/step - loss: 0.0533 - mean_absolute_error: 0.1788 - val_loss: 0.1000 - val_mean_absolute_error: 0.2408\n",
            "Epoch 114/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.0544 - mean_absolute_error: 0.1808 - val_loss: 0.0988 - val_mean_absolute_error: 0.2389\n",
            "Epoch 115/1000\n",
            "2688/2688 [==============================] - 2s 914us/step - loss: 0.0525 - mean_absolute_error: 0.1775 - val_loss: 0.0984 - val_mean_absolute_error: 0.2382\n",
            "Epoch 116/1000\n",
            "2688/2688 [==============================] - 2s 907us/step - loss: 0.0515 - mean_absolute_error: 0.1759 - val_loss: 0.0964 - val_mean_absolute_error: 0.2364\n",
            "Epoch 117/1000\n",
            "2688/2688 [==============================] - 2s 907us/step - loss: 0.0504 - mean_absolute_error: 0.1737 - val_loss: 0.0976 - val_mean_absolute_error: 0.2377\n",
            "Epoch 118/1000\n",
            "2688/2688 [==============================] - 2s 909us/step - loss: 0.0507 - mean_absolute_error: 0.1743 - val_loss: 0.0972 - val_mean_absolute_error: 0.2369\n",
            "Epoch 119/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0508 - mean_absolute_error: 0.1746 - val_loss: 0.0969 - val_mean_absolute_error: 0.2373\n",
            "Epoch 120/1000\n",
            "2688/2688 [==============================] - 2s 923us/step - loss: 0.0521 - mean_absolute_error: 0.1768 - val_loss: 0.0976 - val_mean_absolute_error: 0.2381\n",
            "Epoch 121/1000\n",
            "2688/2688 [==============================] - 2s 919us/step - loss: 0.0528 - mean_absolute_error: 0.1779 - val_loss: 0.1004 - val_mean_absolute_error: 0.2411\n",
            "Epoch 122/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.0518 - mean_absolute_error: 0.1762 - val_loss: 0.0965 - val_mean_absolute_error: 0.2362\n",
            "Epoch 123/1000\n",
            "2688/2688 [==============================] - 2s 911us/step - loss: 0.0507 - mean_absolute_error: 0.1745 - val_loss: 0.0976 - val_mean_absolute_error: 0.2370\n",
            "Epoch 124/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0505 - mean_absolute_error: 0.1739 - val_loss: 0.0973 - val_mean_absolute_error: 0.2365\n",
            "Epoch 125/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.0489 - mean_absolute_error: 0.1710 - val_loss: 0.0960 - val_mean_absolute_error: 0.2358\n",
            "Epoch 126/1000\n",
            "2688/2688 [==============================] - 3s 945us/step - loss: 0.0489 - mean_absolute_error: 0.1711 - val_loss: 0.0937 - val_mean_absolute_error: 0.2325\n",
            "Epoch 127/1000\n",
            "2688/2688 [==============================] - 2s 893us/step - loss: 0.0488 - mean_absolute_error: 0.1711 - val_loss: 0.0966 - val_mean_absolute_error: 0.2358\n",
            "Epoch 128/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0482 - mean_absolute_error: 0.1701 - val_loss: 0.0970 - val_mean_absolute_error: 0.2368\n",
            "Epoch 129/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0484 - mean_absolute_error: 0.1703 - val_loss: 0.0959 - val_mean_absolute_error: 0.2353\n",
            "Epoch 130/1000\n",
            "2688/2688 [==============================] - 2s 894us/step - loss: 0.0484 - mean_absolute_error: 0.1705 - val_loss: 0.0944 - val_mean_absolute_error: 0.2338\n",
            "Epoch 131/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0475 - mean_absolute_error: 0.1689 - val_loss: 0.0936 - val_mean_absolute_error: 0.2321\n",
            "Epoch 132/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0469 - mean_absolute_error: 0.1675 - val_loss: 0.0958 - val_mean_absolute_error: 0.2346\n",
            "Epoch 133/1000\n",
            "2688/2688 [==============================] - 2s 900us/step - loss: 0.0469 - mean_absolute_error: 0.1676 - val_loss: 0.0962 - val_mean_absolute_error: 0.2358\n",
            "Epoch 134/1000\n",
            "2688/2688 [==============================] - 2s 902us/step - loss: 0.0485 - mean_absolute_error: 0.1706 - val_loss: 0.0959 - val_mean_absolute_error: 0.2350\n",
            "Epoch 135/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.0475 - mean_absolute_error: 0.1687 - val_loss: 0.0952 - val_mean_absolute_error: 0.2343\n",
            "Epoch 136/1000\n",
            "2688/2688 [==============================] - 2s 890us/step - loss: 0.0458 - mean_absolute_error: 0.1657 - val_loss: 0.0943 - val_mean_absolute_error: 0.2335\n",
            "Epoch 137/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.0459 - mean_absolute_error: 0.1657 - val_loss: 0.0932 - val_mean_absolute_error: 0.2318\n",
            "Epoch 138/1000\n",
            "2688/2688 [==============================] - 2s 894us/step - loss: 0.0462 - mean_absolute_error: 0.1663 - val_loss: 0.0960 - val_mean_absolute_error: 0.2356\n",
            "Epoch 139/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.0463 - mean_absolute_error: 0.1664 - val_loss: 0.0950 - val_mean_absolute_error: 0.2345\n",
            "Epoch 140/1000\n",
            "2688/2688 [==============================] - 2s 876us/step - loss: 0.0471 - mean_absolute_error: 0.1682 - val_loss: 0.0941 - val_mean_absolute_error: 0.2324\n",
            "Epoch 141/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.0466 - mean_absolute_error: 0.1670 - val_loss: 0.0928 - val_mean_absolute_error: 0.2312\n",
            "Epoch 142/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.0465 - mean_absolute_error: 0.1669 - val_loss: 0.0951 - val_mean_absolute_error: 0.2339\n",
            "Epoch 143/1000\n",
            "2688/2688 [==============================] - 3s 931us/step - loss: 0.0457 - mean_absolute_error: 0.1654 - val_loss: 0.0934 - val_mean_absolute_error: 0.2318\n",
            "Epoch 144/1000\n",
            "2688/2688 [==============================] - 2s 909us/step - loss: 0.0439 - mean_absolute_error: 0.1620 - val_loss: 0.0915 - val_mean_absolute_error: 0.2288\n",
            "Epoch 145/1000\n",
            "2688/2688 [==============================] - 2s 908us/step - loss: 0.0446 - mean_absolute_error: 0.1635 - val_loss: 0.0918 - val_mean_absolute_error: 0.2295\n",
            "Epoch 146/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0435 - mean_absolute_error: 0.1613 - val_loss: 0.0918 - val_mean_absolute_error: 0.2293\n",
            "Epoch 147/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0440 - mean_absolute_error: 0.1622 - val_loss: 0.0929 - val_mean_absolute_error: 0.2304\n",
            "Epoch 148/1000\n",
            "2688/2688 [==============================] - 2s 917us/step - loss: 0.0444 - mean_absolute_error: 0.1628 - val_loss: 0.0958 - val_mean_absolute_error: 0.2337\n",
            "Epoch 149/1000\n",
            "2688/2688 [==============================] - 2s 920us/step - loss: 0.0467 - mean_absolute_error: 0.1673 - val_loss: 0.0980 - val_mean_absolute_error: 0.2399\n",
            "Epoch 150/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.0477 - mean_absolute_error: 0.1692 - val_loss: 0.0920 - val_mean_absolute_error: 0.2297\n",
            "Epoch 151/1000\n",
            "2688/2688 [==============================] - 2s 897us/step - loss: 0.0444 - mean_absolute_error: 0.1630 - val_loss: 0.0933 - val_mean_absolute_error: 0.2324\n",
            "Epoch 152/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0433 - mean_absolute_error: 0.1608 - val_loss: 0.0904 - val_mean_absolute_error: 0.2278\n",
            "Epoch 153/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.0425 - mean_absolute_error: 0.1595 - val_loss: 0.0926 - val_mean_absolute_error: 0.2306\n",
            "Epoch 154/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0431 - mean_absolute_error: 0.1606 - val_loss: 0.0924 - val_mean_absolute_error: 0.2297\n",
            "Epoch 155/1000\n",
            "2688/2688 [==============================] - 2s 907us/step - loss: 0.0415 - mean_absolute_error: 0.1576 - val_loss: 0.0905 - val_mean_absolute_error: 0.2279\n",
            "Epoch 156/1000\n",
            "2688/2688 [==============================] - 2s 909us/step - loss: 0.0417 - mean_absolute_error: 0.1579 - val_loss: 0.0918 - val_mean_absolute_error: 0.2300\n",
            "Epoch 157/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0433 - mean_absolute_error: 0.1613 - val_loss: 0.0910 - val_mean_absolute_error: 0.2291\n",
            "Epoch 158/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.0421 - mean_absolute_error: 0.1586 - val_loss: 0.0900 - val_mean_absolute_error: 0.2272\n",
            "Epoch 159/1000\n",
            "2688/2688 [==============================] - 3s 952us/step - loss: 0.0428 - mean_absolute_error: 0.1598 - val_loss: 0.0914 - val_mean_absolute_error: 0.2287\n",
            "Epoch 160/1000\n",
            "2688/2688 [==============================] - 2s 895us/step - loss: 0.0441 - mean_absolute_error: 0.1623 - val_loss: 0.0929 - val_mean_absolute_error: 0.2329\n",
            "Epoch 161/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0422 - mean_absolute_error: 0.1588 - val_loss: 0.0902 - val_mean_absolute_error: 0.2269\n",
            "Epoch 162/1000\n",
            "2688/2688 [==============================] - 2s 916us/step - loss: 0.0410 - mean_absolute_error: 0.1565 - val_loss: 0.0918 - val_mean_absolute_error: 0.2295\n",
            "Epoch 163/1000\n",
            "2688/2688 [==============================] - 2s 898us/step - loss: 0.0415 - mean_absolute_error: 0.1576 - val_loss: 0.0894 - val_mean_absolute_error: 0.2260\n",
            "Epoch 164/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.0411 - mean_absolute_error: 0.1568 - val_loss: 0.0912 - val_mean_absolute_error: 0.2290\n",
            "Epoch 165/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0429 - mean_absolute_error: 0.1604 - val_loss: 0.0907 - val_mean_absolute_error: 0.2287\n",
            "Epoch 166/1000\n",
            "2688/2688 [==============================] - 2s 919us/step - loss: 0.0414 - mean_absolute_error: 0.1572 - val_loss: 0.0909 - val_mean_absolute_error: 0.2280\n",
            "Epoch 167/1000\n",
            "2688/2688 [==============================] - 3s 932us/step - loss: 0.0405 - mean_absolute_error: 0.1557 - val_loss: 0.0936 - val_mean_absolute_error: 0.2332\n",
            "Epoch 168/1000\n",
            "2688/2688 [==============================] - 3s 939us/step - loss: 0.0413 - mean_absolute_error: 0.1574 - val_loss: 0.0901 - val_mean_absolute_error: 0.2278\n",
            "Epoch 169/1000\n",
            "2688/2688 [==============================] - 3s 942us/step - loss: 0.0398 - mean_absolute_error: 0.1543 - val_loss: 0.0900 - val_mean_absolute_error: 0.2283\n",
            "Epoch 170/1000\n",
            "2688/2688 [==============================] - 3s 945us/step - loss: 0.0401 - mean_absolute_error: 0.1545 - val_loss: 0.0894 - val_mean_absolute_error: 0.2269\n",
            "Epoch 171/1000\n",
            "2688/2688 [==============================] - 3s 935us/step - loss: 0.0407 - mean_absolute_error: 0.1559 - val_loss: 0.0909 - val_mean_absolute_error: 0.2288\n",
            "Epoch 172/1000\n",
            "2688/2688 [==============================] - 2s 907us/step - loss: 0.0400 - mean_absolute_error: 0.1545 - val_loss: 0.0871 - val_mean_absolute_error: 0.2233\n",
            "Epoch 173/1000\n",
            "2688/2688 [==============================] - 2s 908us/step - loss: 0.0396 - mean_absolute_error: 0.1540 - val_loss: 0.0894 - val_mean_absolute_error: 0.2267\n",
            "Epoch 174/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0395 - mean_absolute_error: 0.1538 - val_loss: 0.0881 - val_mean_absolute_error: 0.2248\n",
            "Epoch 175/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.0384 - mean_absolute_error: 0.1515 - val_loss: 0.0887 - val_mean_absolute_error: 0.2256\n",
            "Epoch 176/1000\n",
            "2688/2688 [==============================] - 3s 973us/step - loss: 0.0389 - mean_absolute_error: 0.1525 - val_loss: 0.0887 - val_mean_absolute_error: 0.2254\n",
            "Epoch 177/1000\n",
            "2688/2688 [==============================] - 2s 909us/step - loss: 0.0395 - mean_absolute_error: 0.1536 - val_loss: 0.0883 - val_mean_absolute_error: 0.2250\n",
            "Epoch 178/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0407 - mean_absolute_error: 0.1558 - val_loss: 0.0876 - val_mean_absolute_error: 0.2236\n",
            "Epoch 179/1000\n",
            "2688/2688 [==============================] - 2s 909us/step - loss: 0.0391 - mean_absolute_error: 0.1527 - val_loss: 0.0895 - val_mean_absolute_error: 0.2254\n",
            "Epoch 180/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0392 - mean_absolute_error: 0.1529 - val_loss: 0.0880 - val_mean_absolute_error: 0.2241\n",
            "Epoch 181/1000\n",
            "2688/2688 [==============================] - 2s 908us/step - loss: 0.0384 - mean_absolute_error: 0.1514 - val_loss: 0.0871 - val_mean_absolute_error: 0.2235\n",
            "Epoch 182/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0379 - mean_absolute_error: 0.1505 - val_loss: 0.0885 - val_mean_absolute_error: 0.2256\n",
            "Epoch 183/1000\n",
            "2688/2688 [==============================] - 2s 908us/step - loss: 0.0392 - mean_absolute_error: 0.1531 - val_loss: 0.0874 - val_mean_absolute_error: 0.2234\n",
            "Epoch 184/1000\n",
            "2688/2688 [==============================] - 3s 936us/step - loss: 0.0399 - mean_absolute_error: 0.1544 - val_loss: 0.0880 - val_mean_absolute_error: 0.2246\n",
            "Epoch 185/1000\n",
            "2688/2688 [==============================] - 2s 924us/step - loss: 0.0410 - mean_absolute_error: 0.1566 - val_loss: 0.0887 - val_mean_absolute_error: 0.2249\n",
            "Epoch 186/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0403 - mean_absolute_error: 0.1554 - val_loss: 0.0881 - val_mean_absolute_error: 0.2247\n",
            "Epoch 187/1000\n",
            "2688/2688 [==============================] - 2s 902us/step - loss: 0.0378 - mean_absolute_error: 0.1503 - val_loss: 0.0868 - val_mean_absolute_error: 0.2225\n",
            "Epoch 188/1000\n",
            "2688/2688 [==============================] - 2s 916us/step - loss: 0.0368 - mean_absolute_error: 0.1483 - val_loss: 0.0877 - val_mean_absolute_error: 0.2232\n",
            "Epoch 189/1000\n",
            "2688/2688 [==============================] - 2s 922us/step - loss: 0.0373 - mean_absolute_error: 0.1494 - val_loss: 0.0866 - val_mean_absolute_error: 0.2230\n",
            "Epoch 190/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0367 - mean_absolute_error: 0.1481 - val_loss: 0.0874 - val_mean_absolute_error: 0.2225\n",
            "Epoch 191/1000\n",
            "2688/2688 [==============================] - 2s 919us/step - loss: 0.0368 - mean_absolute_error: 0.1484 - val_loss: 0.0878 - val_mean_absolute_error: 0.2235\n",
            "Epoch 192/1000\n",
            "2688/2688 [==============================] - 2s 916us/step - loss: 0.0369 - mean_absolute_error: 0.1485 - val_loss: 0.0868 - val_mean_absolute_error: 0.2222\n",
            "Epoch 193/1000\n",
            "2688/2688 [==============================] - 2s 922us/step - loss: 0.0375 - mean_absolute_error: 0.1496 - val_loss: 0.0863 - val_mean_absolute_error: 0.2221\n",
            "Epoch 194/1000\n",
            "2688/2688 [==============================] - 2s 915us/step - loss: 0.0371 - mean_absolute_error: 0.1488 - val_loss: 0.0872 - val_mean_absolute_error: 0.2235\n",
            "Epoch 195/1000\n",
            "2688/2688 [==============================] - 2s 915us/step - loss: 0.0380 - mean_absolute_error: 0.1509 - val_loss: 0.0876 - val_mean_absolute_error: 0.2240\n",
            "Epoch 196/1000\n",
            "2688/2688 [==============================] - 2s 908us/step - loss: 0.0369 - mean_absolute_error: 0.1483 - val_loss: 0.0860 - val_mean_absolute_error: 0.2217\n",
            "Epoch 197/1000\n",
            "2688/2688 [==============================] - 2s 911us/step - loss: 0.0397 - mean_absolute_error: 0.1543 - val_loss: 0.0945 - val_mean_absolute_error: 0.2322\n",
            "Epoch 198/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0389 - mean_absolute_error: 0.1526 - val_loss: 0.0897 - val_mean_absolute_error: 0.2278\n",
            "Epoch 199/1000\n",
            "2688/2688 [==============================] - 3s 938us/step - loss: 0.0368 - mean_absolute_error: 0.1482 - val_loss: 0.0859 - val_mean_absolute_error: 0.2211\n",
            "Epoch 200/1000\n",
            "2688/2688 [==============================] - 3s 959us/step - loss: 0.0359 - mean_absolute_error: 0.1463 - val_loss: 0.0871 - val_mean_absolute_error: 0.2230\n",
            "Epoch 201/1000\n",
            "2688/2688 [==============================] - 2s 929us/step - loss: 0.0356 - mean_absolute_error: 0.1458 - val_loss: 0.0881 - val_mean_absolute_error: 0.2248\n",
            "Epoch 202/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0362 - mean_absolute_error: 0.1472 - val_loss: 0.0866 - val_mean_absolute_error: 0.2220\n",
            "Epoch 203/1000\n",
            "2688/2688 [==============================] - 2s 920us/step - loss: 0.0355 - mean_absolute_error: 0.1455 - val_loss: 0.0869 - val_mean_absolute_error: 0.2232\n",
            "Epoch 204/1000\n",
            "2688/2688 [==============================] - 2s 914us/step - loss: 0.0359 - mean_absolute_error: 0.1465 - val_loss: 0.0854 - val_mean_absolute_error: 0.2206\n",
            "Epoch 205/1000\n",
            "2688/2688 [==============================] - 2s 902us/step - loss: 0.0352 - mean_absolute_error: 0.1449 - val_loss: 0.0864 - val_mean_absolute_error: 0.2219\n",
            "Epoch 206/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.0349 - mean_absolute_error: 0.1442 - val_loss: 0.0850 - val_mean_absolute_error: 0.2201\n",
            "Epoch 207/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.0351 - mean_absolute_error: 0.1447 - val_loss: 0.0872 - val_mean_absolute_error: 0.2231\n",
            "Epoch 208/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0369 - mean_absolute_error: 0.1487 - val_loss: 0.0867 - val_mean_absolute_error: 0.2228\n",
            "Epoch 209/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.0356 - mean_absolute_error: 0.1459 - val_loss: 0.0853 - val_mean_absolute_error: 0.2206\n",
            "Epoch 210/1000\n",
            "2688/2688 [==============================] - 2s 915us/step - loss: 0.0358 - mean_absolute_error: 0.1462 - val_loss: 0.0853 - val_mean_absolute_error: 0.2214\n",
            "Epoch 211/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0352 - mean_absolute_error: 0.1449 - val_loss: 0.0851 - val_mean_absolute_error: 0.2203\n",
            "Epoch 212/1000\n",
            "2688/2688 [==============================] - 2s 900us/step - loss: 0.0344 - mean_absolute_error: 0.1434 - val_loss: 0.0845 - val_mean_absolute_error: 0.2193\n",
            "Epoch 213/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.0335 - mean_absolute_error: 0.1413 - val_loss: 0.0863 - val_mean_absolute_error: 0.2228\n",
            "Epoch 214/1000\n",
            "2688/2688 [==============================] - 2s 888us/step - loss: 0.0340 - mean_absolute_error: 0.1423 - val_loss: 0.0873 - val_mean_absolute_error: 0.2234\n",
            "Epoch 215/1000\n",
            "2688/2688 [==============================] - 2s 886us/step - loss: 0.0358 - mean_absolute_error: 0.1463 - val_loss: 0.0859 - val_mean_absolute_error: 0.2218\n",
            "Epoch 216/1000\n",
            "2688/2688 [==============================] - 2s 892us/step - loss: 0.0349 - mean_absolute_error: 0.1444 - val_loss: 0.0855 - val_mean_absolute_error: 0.2200\n",
            "Epoch 217/1000\n",
            "2688/2688 [==============================] - 3s 949us/step - loss: 0.0353 - mean_absolute_error: 0.1453 - val_loss: 0.0876 - val_mean_absolute_error: 0.2234\n",
            "Epoch 218/1000\n",
            "2688/2688 [==============================] - 2s 882us/step - loss: 0.0352 - mean_absolute_error: 0.1451 - val_loss: 0.0846 - val_mean_absolute_error: 0.2192\n",
            "Epoch 219/1000\n",
            "2688/2688 [==============================] - 2s 873us/step - loss: 0.0345 - mean_absolute_error: 0.1434 - val_loss: 0.0848 - val_mean_absolute_error: 0.2198\n",
            "Epoch 220/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0351 - mean_absolute_error: 0.1449 - val_loss: 0.0835 - val_mean_absolute_error: 0.2184\n",
            "Epoch 221/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.0343 - mean_absolute_error: 0.1431 - val_loss: 0.0857 - val_mean_absolute_error: 0.2210\n",
            "Epoch 222/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.0343 - mean_absolute_error: 0.1431 - val_loss: 0.0853 - val_mean_absolute_error: 0.2217\n",
            "Epoch 223/1000\n",
            "2688/2688 [==============================] - 2s 877us/step - loss: 0.0346 - mean_absolute_error: 0.1437 - val_loss: 0.0854 - val_mean_absolute_error: 0.2202\n",
            "Epoch 224/1000\n",
            "2688/2688 [==============================] - 2s 874us/step - loss: 0.0351 - mean_absolute_error: 0.1451 - val_loss: 0.0858 - val_mean_absolute_error: 0.2210\n",
            "Epoch 225/1000\n",
            "2688/2688 [==============================] - 2s 883us/step - loss: 0.0333 - mean_absolute_error: 0.1409 - val_loss: 0.0842 - val_mean_absolute_error: 0.2192\n",
            "Epoch 226/1000\n",
            "2688/2688 [==============================] - 2s 869us/step - loss: 0.0331 - mean_absolute_error: 0.1405 - val_loss: 0.0862 - val_mean_absolute_error: 0.2215\n",
            "Epoch 227/1000\n",
            "2688/2688 [==============================] - 2s 870us/step - loss: 0.0336 - mean_absolute_error: 0.1416 - val_loss: 0.0845 - val_mean_absolute_error: 0.2190\n",
            "Epoch 228/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0342 - mean_absolute_error: 0.1430 - val_loss: 0.0888 - val_mean_absolute_error: 0.2251\n",
            "Epoch 229/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.0343 - mean_absolute_error: 0.1432 - val_loss: 0.0853 - val_mean_absolute_error: 0.2202\n",
            "Epoch 230/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.0337 - mean_absolute_error: 0.1418 - val_loss: 0.0839 - val_mean_absolute_error: 0.2186\n",
            "Epoch 231/1000\n",
            "2688/2688 [==============================] - 2s 872us/step - loss: 0.0336 - mean_absolute_error: 0.1415 - val_loss: 0.0853 - val_mean_absolute_error: 0.2200\n",
            "Epoch 232/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0331 - mean_absolute_error: 0.1405 - val_loss: 0.0840 - val_mean_absolute_error: 0.2191\n",
            "Epoch 233/1000\n",
            "2688/2688 [==============================] - 2s 869us/step - loss: 0.0326 - mean_absolute_error: 0.1395 - val_loss: 0.0846 - val_mean_absolute_error: 0.2193\n",
            "Epoch 234/1000\n",
            "2688/2688 [==============================] - 3s 939us/step - loss: 0.0328 - mean_absolute_error: 0.1399 - val_loss: 0.0840 - val_mean_absolute_error: 0.2181\n",
            "Epoch 235/1000\n",
            "2688/2688 [==============================] - 2s 886us/step - loss: 0.0325 - mean_absolute_error: 0.1392 - val_loss: 0.0856 - val_mean_absolute_error: 0.2228\n",
            "Epoch 236/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.0332 - mean_absolute_error: 0.1408 - val_loss: 0.0853 - val_mean_absolute_error: 0.2200\n",
            "Epoch 237/1000\n",
            "2688/2688 [==============================] - 2s 916us/step - loss: 0.0331 - mean_absolute_error: 0.1408 - val_loss: 0.0847 - val_mean_absolute_error: 0.2191\n",
            "Epoch 238/1000\n",
            "2688/2688 [==============================] - 2s 893us/step - loss: 0.0322 - mean_absolute_error: 0.1386 - val_loss: 0.0839 - val_mean_absolute_error: 0.2183\n",
            "Epoch 239/1000\n",
            "2688/2688 [==============================] - 2s 886us/step - loss: 0.0327 - mean_absolute_error: 0.1397 - val_loss: 0.0845 - val_mean_absolute_error: 0.2193\n",
            "Epoch 240/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.0325 - mean_absolute_error: 0.1392 - val_loss: 0.0848 - val_mean_absolute_error: 0.2195\n",
            "Epoch 241/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.0325 - mean_absolute_error: 0.1393 - val_loss: 0.0845 - val_mean_absolute_error: 0.2198\n",
            "Epoch 242/1000\n",
            "2688/2688 [==============================] - 2s 880us/step - loss: 0.0323 - mean_absolute_error: 0.1390 - val_loss: 0.0858 - val_mean_absolute_error: 0.2225\n",
            "Epoch 243/1000\n",
            "2688/2688 [==============================] - 2s 914us/step - loss: 0.0322 - mean_absolute_error: 0.1385 - val_loss: 0.0854 - val_mean_absolute_error: 0.2200\n",
            "Epoch 244/1000\n",
            "2688/2688 [==============================] - 2s 894us/step - loss: 0.0317 - mean_absolute_error: 0.1375 - val_loss: 0.0834 - val_mean_absolute_error: 0.2183\n",
            "Epoch 245/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.0329 - mean_absolute_error: 0.1404 - val_loss: 0.0905 - val_mean_absolute_error: 0.2283\n",
            "Epoch 246/1000\n",
            "2688/2688 [==============================] - 2s 892us/step - loss: 0.0329 - mean_absolute_error: 0.1403 - val_loss: 0.0833 - val_mean_absolute_error: 0.2176\n",
            "Epoch 247/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.0318 - mean_absolute_error: 0.1378 - val_loss: 0.0829 - val_mean_absolute_error: 0.2169\n",
            "Epoch 248/1000\n",
            "2688/2688 [==============================] - 2s 893us/step - loss: 0.0319 - mean_absolute_error: 0.1378 - val_loss: 0.0834 - val_mean_absolute_error: 0.2178\n",
            "Epoch 249/1000\n",
            "2688/2688 [==============================] - 2s 886us/step - loss: 0.0312 - mean_absolute_error: 0.1365 - val_loss: 0.0831 - val_mean_absolute_error: 0.2175\n",
            "Epoch 250/1000\n",
            "2688/2688 [==============================] - 2s 900us/step - loss: 0.0315 - mean_absolute_error: 0.1370 - val_loss: 0.0832 - val_mean_absolute_error: 0.2170\n",
            "Epoch 251/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0317 - mean_absolute_error: 0.1375 - val_loss: 0.0830 - val_mean_absolute_error: 0.2170\n",
            "Epoch 252/1000\n",
            "2688/2688 [==============================] - 2s 875us/step - loss: 0.0319 - mean_absolute_error: 0.1381 - val_loss: 0.0844 - val_mean_absolute_error: 0.2187\n",
            "Epoch 253/1000\n",
            "2688/2688 [==============================] - 2s 879us/step - loss: 0.0321 - mean_absolute_error: 0.1383 - val_loss: 0.0825 - val_mean_absolute_error: 0.2164\n",
            "Epoch 254/1000\n",
            "2688/2688 [==============================] - 2s 882us/step - loss: 0.0320 - mean_absolute_error: 0.1381 - val_loss: 0.0840 - val_mean_absolute_error: 0.2180\n",
            "Epoch 255/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.0326 - mean_absolute_error: 0.1394 - val_loss: 0.0839 - val_mean_absolute_error: 0.2181\n",
            "Epoch 256/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.0311 - mean_absolute_error: 0.1363 - val_loss: 0.0849 - val_mean_absolute_error: 0.2193\n",
            "Epoch 257/1000\n",
            "2688/2688 [==============================] - 2s 895us/step - loss: 0.0310 - mean_absolute_error: 0.1359 - val_loss: 0.0832 - val_mean_absolute_error: 0.2178\n",
            "Epoch 258/1000\n",
            "2688/2688 [==============================] - 2s 879us/step - loss: 0.0307 - mean_absolute_error: 0.1353 - val_loss: 0.0831 - val_mean_absolute_error: 0.2170\n",
            "Epoch 259/1000\n",
            "2688/2688 [==============================] - 2s 923us/step - loss: 0.0312 - mean_absolute_error: 0.1363 - val_loss: 0.0836 - val_mean_absolute_error: 0.2180\n",
            "Epoch 260/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.0310 - mean_absolute_error: 0.1361 - val_loss: 0.0850 - val_mean_absolute_error: 0.2202\n",
            "Epoch 261/1000\n",
            "2688/2688 [==============================] - 2s 883us/step - loss: 0.0311 - mean_absolute_error: 0.1362 - val_loss: 0.0825 - val_mean_absolute_error: 0.2164\n",
            "Epoch 262/1000\n",
            "2688/2688 [==============================] - 2s 882us/step - loss: 0.0315 - mean_absolute_error: 0.1372 - val_loss: 0.0834 - val_mean_absolute_error: 0.2179\n",
            "Epoch 263/1000\n",
            "2688/2688 [==============================] - 2s 873us/step - loss: 0.0315 - mean_absolute_error: 0.1370 - val_loss: 0.0831 - val_mean_absolute_error: 0.2176\n",
            "Epoch 264/1000\n",
            "2688/2688 [==============================] - 2s 888us/step - loss: 0.0310 - mean_absolute_error: 0.1360 - val_loss: 0.0837 - val_mean_absolute_error: 0.2177\n",
            "Epoch 265/1000\n",
            "2688/2688 [==============================] - 2s 874us/step - loss: 0.0317 - mean_absolute_error: 0.1375 - val_loss: 0.0832 - val_mean_absolute_error: 0.2172\n",
            "Epoch 266/1000\n",
            "2688/2688 [==============================] - 2s 894us/step - loss: 0.0310 - mean_absolute_error: 0.1360 - val_loss: 0.0830 - val_mean_absolute_error: 0.2168\n",
            "Epoch 267/1000\n",
            "2688/2688 [==============================] - 2s 882us/step - loss: 0.0315 - mean_absolute_error: 0.1372 - val_loss: 0.0850 - val_mean_absolute_error: 0.2212\n",
            "Epoch 268/1000\n",
            "2688/2688 [==============================] - 2s 890us/step - loss: 0.0306 - mean_absolute_error: 0.1350 - val_loss: 0.0814 - val_mean_absolute_error: 0.2150\n",
            "Epoch 269/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.0306 - mean_absolute_error: 0.1351 - val_loss: 0.0841 - val_mean_absolute_error: 0.2181\n",
            "Epoch 270/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.0313 - mean_absolute_error: 0.1366 - val_loss: 0.0830 - val_mean_absolute_error: 0.2169\n",
            "Epoch 271/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.0308 - mean_absolute_error: 0.1356 - val_loss: 0.0826 - val_mean_absolute_error: 0.2167\n",
            "Epoch 272/1000\n",
            "2688/2688 [==============================] - 2s 885us/step - loss: 0.0294 - mean_absolute_error: 0.1322 - val_loss: 0.0819 - val_mean_absolute_error: 0.2158\n",
            "Epoch 273/1000\n",
            "2688/2688 [==============================] - 2s 895us/step - loss: 0.0302 - mean_absolute_error: 0.1343 - val_loss: 0.0836 - val_mean_absolute_error: 0.2176\n",
            "Epoch 274/1000\n",
            "2688/2688 [==============================] - 2s 872us/step - loss: 0.0300 - mean_absolute_error: 0.1337 - val_loss: 0.0813 - val_mean_absolute_error: 0.2148\n",
            "Epoch 275/1000\n",
            "2688/2688 [==============================] - 2s 879us/step - loss: 0.0299 - mean_absolute_error: 0.1335 - val_loss: 0.0841 - val_mean_absolute_error: 0.2195\n",
            "Epoch 276/1000\n",
            "2688/2688 [==============================] - 3s 938us/step - loss: 0.0295 - mean_absolute_error: 0.1325 - val_loss: 0.0834 - val_mean_absolute_error: 0.2179\n",
            "Epoch 277/1000\n",
            "2688/2688 [==============================] - 2s 873us/step - loss: 0.0296 - mean_absolute_error: 0.1331 - val_loss: 0.0827 - val_mean_absolute_error: 0.2167\n",
            "Epoch 278/1000\n",
            "2688/2688 [==============================] - 2s 877us/step - loss: 0.0304 - mean_absolute_error: 0.1347 - val_loss: 0.0818 - val_mean_absolute_error: 0.2154\n",
            "Epoch 279/1000\n",
            "2688/2688 [==============================] - 2s 889us/step - loss: 0.0302 - mean_absolute_error: 0.1341 - val_loss: 0.0819 - val_mean_absolute_error: 0.2156\n",
            "Epoch 280/1000\n",
            "2688/2688 [==============================] - 2s 890us/step - loss: 0.0304 - mean_absolute_error: 0.1348 - val_loss: 0.0819 - val_mean_absolute_error: 0.2154\n",
            "Epoch 281/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.0300 - mean_absolute_error: 0.1337 - val_loss: 0.0825 - val_mean_absolute_error: 0.2163\n",
            "Epoch 282/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0316 - mean_absolute_error: 0.1374 - val_loss: 0.0829 - val_mean_absolute_error: 0.2175\n",
            "Epoch 283/1000\n",
            "2688/2688 [==============================] - 2s 882us/step - loss: 0.0308 - mean_absolute_error: 0.1355 - val_loss: 0.0832 - val_mean_absolute_error: 0.2171\n",
            "Epoch 284/1000\n",
            "2688/2688 [==============================] - 2s 880us/step - loss: 0.0302 - mean_absolute_error: 0.1343 - val_loss: 0.0825 - val_mean_absolute_error: 0.2177\n",
            "Epoch 285/1000\n",
            "2688/2688 [==============================] - 2s 880us/step - loss: 0.0294 - mean_absolute_error: 0.1323 - val_loss: 0.0816 - val_mean_absolute_error: 0.2155\n",
            "Epoch 286/1000\n",
            "2688/2688 [==============================] - 2s 890us/step - loss: 0.0287 - mean_absolute_error: 0.1307 - val_loss: 0.0821 - val_mean_absolute_error: 0.2160\n",
            "Epoch 287/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0290 - mean_absolute_error: 0.1316 - val_loss: 0.0820 - val_mean_absolute_error: 0.2162\n",
            "Epoch 288/1000\n",
            "2688/2688 [==============================] - 2s 882us/step - loss: 0.0285 - mean_absolute_error: 0.1301 - val_loss: 0.0830 - val_mean_absolute_error: 0.2168\n",
            "Epoch 289/1000\n",
            "2688/2688 [==============================] - 2s 885us/step - loss: 0.0284 - mean_absolute_error: 0.1300 - val_loss: 0.0817 - val_mean_absolute_error: 0.2150\n",
            "Epoch 290/1000\n",
            "2688/2688 [==============================] - 2s 885us/step - loss: 0.0287 - mean_absolute_error: 0.1309 - val_loss: 0.0828 - val_mean_absolute_error: 0.2161\n",
            "Epoch 291/1000\n",
            "2688/2688 [==============================] - 2s 888us/step - loss: 0.0305 - mean_absolute_error: 0.1352 - val_loss: 0.0836 - val_mean_absolute_error: 0.2186\n",
            "Epoch 292/1000\n",
            "2688/2688 [==============================] - 2s 886us/step - loss: 0.0295 - mean_absolute_error: 0.1326 - val_loss: 0.0821 - val_mean_absolute_error: 0.2156\n",
            "Epoch 293/1000\n",
            "2688/2688 [==============================] - 3s 941us/step - loss: 0.0291 - mean_absolute_error: 0.1315 - val_loss: 0.0834 - val_mean_absolute_error: 0.2178\n",
            "Epoch 294/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.0287 - mean_absolute_error: 0.1307 - val_loss: 0.0850 - val_mean_absolute_error: 0.2191\n",
            "Epoch 295/1000\n",
            "2688/2688 [==============================] - 2s 880us/step - loss: 0.0292 - mean_absolute_error: 0.1320 - val_loss: 0.0832 - val_mean_absolute_error: 0.2174\n",
            "Epoch 296/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.0297 - mean_absolute_error: 0.1332 - val_loss: 0.0834 - val_mean_absolute_error: 0.2169\n",
            "Epoch 297/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.0302 - mean_absolute_error: 0.1342 - val_loss: 0.0839 - val_mean_absolute_error: 0.2185\n",
            "Epoch 298/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.0292 - mean_absolute_error: 0.1320 - val_loss: 0.0823 - val_mean_absolute_error: 0.2156\n",
            "Epoch 299/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.0294 - mean_absolute_error: 0.1322 - val_loss: 0.0826 - val_mean_absolute_error: 0.2162\n",
            "Epoch 300/1000\n",
            "2688/2688 [==============================] - 2s 879us/step - loss: 0.0299 - mean_absolute_error: 0.1336 - val_loss: 0.0818 - val_mean_absolute_error: 0.2151\n",
            "Epoch 301/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0290 - mean_absolute_error: 0.1314 - val_loss: 0.0810 - val_mean_absolute_error: 0.2140\n",
            "Epoch 302/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.0287 - mean_absolute_error: 0.1307 - val_loss: 0.0818 - val_mean_absolute_error: 0.2150\n",
            "Epoch 303/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0281 - mean_absolute_error: 0.1295 - val_loss: 0.0815 - val_mean_absolute_error: 0.2145\n",
            "Epoch 304/1000\n",
            "2688/2688 [==============================] - 2s 897us/step - loss: 0.0282 - mean_absolute_error: 0.1296 - val_loss: 0.0843 - val_mean_absolute_error: 0.2206\n",
            "Epoch 305/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0292 - mean_absolute_error: 0.1320 - val_loss: 0.0822 - val_mean_absolute_error: 0.2161\n",
            "Epoch 306/1000\n",
            "2688/2688 [==============================] - 2s 921us/step - loss: 0.0293 - mean_absolute_error: 0.1323 - val_loss: 0.0832 - val_mean_absolute_error: 0.2172\n",
            "Epoch 307/1000\n",
            "2688/2688 [==============================] - 2s 897us/step - loss: 0.0287 - mean_absolute_error: 0.1308 - val_loss: 0.0840 - val_mean_absolute_error: 0.2174\n",
            "Epoch 308/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.0288 - mean_absolute_error: 0.1312 - val_loss: 0.0811 - val_mean_absolute_error: 0.2142\n",
            "Epoch 309/1000\n",
            "2688/2688 [==============================] - 2s 902us/step - loss: 0.0288 - mean_absolute_error: 0.1310 - val_loss: 0.0815 - val_mean_absolute_error: 0.2147\n",
            "Epoch 310/1000\n",
            "2688/2688 [==============================] - 3s 933us/step - loss: 0.0276 - mean_absolute_error: 0.1282 - val_loss: 0.0807 - val_mean_absolute_error: 0.2137\n",
            "Epoch 311/1000\n",
            "2688/2688 [==============================] - 2s 914us/step - loss: 0.0281 - mean_absolute_error: 0.1295 - val_loss: 0.0809 - val_mean_absolute_error: 0.2140\n",
            "Epoch 312/1000\n",
            "2688/2688 [==============================] - 2s 915us/step - loss: 0.0277 - mean_absolute_error: 0.1284 - val_loss: 0.0833 - val_mean_absolute_error: 0.2180\n",
            "Epoch 313/1000\n",
            "2688/2688 [==============================] - 2s 920us/step - loss: 0.0284 - mean_absolute_error: 0.1300 - val_loss: 0.0838 - val_mean_absolute_error: 0.2176\n",
            "Epoch 314/1000\n",
            "2688/2688 [==============================] - 2s 918us/step - loss: 0.0288 - mean_absolute_error: 0.1311 - val_loss: 0.0808 - val_mean_absolute_error: 0.2139\n",
            "Epoch 315/1000\n",
            "2688/2688 [==============================] - 3s 934us/step - loss: 0.0287 - mean_absolute_error: 0.1307 - val_loss: 0.0837 - val_mean_absolute_error: 0.2184\n",
            "Epoch 316/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0292 - mean_absolute_error: 0.1320 - val_loss: 0.0816 - val_mean_absolute_error: 0.2143\n",
            "Epoch 317/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0283 - mean_absolute_error: 0.1297 - val_loss: 0.0812 - val_mean_absolute_error: 0.2144\n",
            "Epoch 318/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0273 - mean_absolute_error: 0.1275 - val_loss: 0.0807 - val_mean_absolute_error: 0.2138\n",
            "Epoch 319/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0268 - mean_absolute_error: 0.1262 - val_loss: 0.0815 - val_mean_absolute_error: 0.2143\n",
            "Epoch 320/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0271 - mean_absolute_error: 0.1270 - val_loss: 0.0821 - val_mean_absolute_error: 0.2152\n",
            "Epoch 321/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.0278 - mean_absolute_error: 0.1288 - val_loss: 0.0820 - val_mean_absolute_error: 0.2156\n",
            "Epoch 322/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0286 - mean_absolute_error: 0.1305 - val_loss: 0.0807 - val_mean_absolute_error: 0.2137\n",
            "Epoch 323/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0293 - mean_absolute_error: 0.1322 - val_loss: 0.0818 - val_mean_absolute_error: 0.2150\n",
            "Epoch 324/1000\n",
            "2688/2688 [==============================] - 2s 915us/step - loss: 0.0287 - mean_absolute_error: 0.1308 - val_loss: 0.0812 - val_mean_absolute_error: 0.2139\n",
            "Epoch 325/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.0276 - mean_absolute_error: 0.1283 - val_loss: 0.0809 - val_mean_absolute_error: 0.2137\n",
            "Epoch 326/1000\n",
            "2688/2688 [==============================] - 2s 930us/step - loss: 0.0268 - mean_absolute_error: 0.1264 - val_loss: 0.0812 - val_mean_absolute_error: 0.2146\n",
            "Epoch 327/1000\n",
            "2688/2688 [==============================] - 2s 893us/step - loss: 0.0275 - mean_absolute_error: 0.1280 - val_loss: 0.0803 - val_mean_absolute_error: 0.2129\n",
            "Epoch 328/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0268 - mean_absolute_error: 0.1263 - val_loss: 0.0807 - val_mean_absolute_error: 0.2142\n",
            "Epoch 329/1000\n",
            "2688/2688 [==============================] - 2s 897us/step - loss: 0.0272 - mean_absolute_error: 0.1272 - val_loss: 0.0812 - val_mean_absolute_error: 0.2142\n",
            "Epoch 330/1000\n",
            "2688/2688 [==============================] - 2s 909us/step - loss: 0.0281 - mean_absolute_error: 0.1294 - val_loss: 0.0803 - val_mean_absolute_error: 0.2130\n",
            "Epoch 331/1000\n",
            "2688/2688 [==============================] - 2s 895us/step - loss: 0.0273 - mean_absolute_error: 0.1275 - val_loss: 0.0813 - val_mean_absolute_error: 0.2145\n",
            "Epoch 332/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.0279 - mean_absolute_error: 0.1290 - val_loss: 0.0819 - val_mean_absolute_error: 0.2159\n",
            "Epoch 333/1000\n",
            "2688/2688 [==============================] - 2s 894us/step - loss: 0.0292 - mean_absolute_error: 0.1320 - val_loss: 0.0815 - val_mean_absolute_error: 0.2149\n",
            "Epoch 334/1000\n",
            "2688/2688 [==============================] - 2s 895us/step - loss: 0.0293 - mean_absolute_error: 0.1321 - val_loss: 0.0808 - val_mean_absolute_error: 0.2139\n",
            "Epoch 335/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.0287 - mean_absolute_error: 0.1308 - val_loss: 0.0810 - val_mean_absolute_error: 0.2141\n",
            "Epoch 336/1000\n",
            "2688/2688 [==============================] - 2s 894us/step - loss: 0.0274 - mean_absolute_error: 0.1276 - val_loss: 0.0824 - val_mean_absolute_error: 0.2157\n",
            "Epoch 337/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0266 - mean_absolute_error: 0.1258 - val_loss: 0.0809 - val_mean_absolute_error: 0.2133\n",
            "Epoch 338/1000\n",
            "2688/2688 [==============================] - 2s 926us/step - loss: 0.0263 - mean_absolute_error: 0.1252 - val_loss: 0.0798 - val_mean_absolute_error: 0.2120\n",
            "Epoch 339/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0265 - mean_absolute_error: 0.1256 - val_loss: 0.0832 - val_mean_absolute_error: 0.2165\n",
            "Epoch 340/1000\n",
            "2688/2688 [==============================] - 2s 914us/step - loss: 0.0281 - mean_absolute_error: 0.1296 - val_loss: 0.0796 - val_mean_absolute_error: 0.2123\n",
            "Epoch 341/1000\n",
            "2688/2688 [==============================] - 2s 918us/step - loss: 0.0265 - mean_absolute_error: 0.1254 - val_loss: 0.0805 - val_mean_absolute_error: 0.2131\n",
            "Epoch 342/1000\n",
            "2688/2688 [==============================] - 3s 937us/step - loss: 0.0267 - mean_absolute_error: 0.1260 - val_loss: 0.0804 - val_mean_absolute_error: 0.2135\n",
            "Epoch 343/1000\n",
            "2688/2688 [==============================] - 2s 920us/step - loss: 0.0274 - mean_absolute_error: 0.1279 - val_loss: 0.0805 - val_mean_absolute_error: 0.2136\n",
            "Epoch 344/1000\n",
            "2688/2688 [==============================] - 3s 930us/step - loss: 0.0282 - mean_absolute_error: 0.1299 - val_loss: 0.0816 - val_mean_absolute_error: 0.2146\n",
            "Epoch 345/1000\n",
            "2688/2688 [==============================] - 2s 929us/step - loss: 0.0270 - mean_absolute_error: 0.1269 - val_loss: 0.0830 - val_mean_absolute_error: 0.2172\n",
            "Epoch 346/1000\n",
            "2688/2688 [==============================] - 2s 913us/step - loss: 0.0276 - mean_absolute_error: 0.1283 - val_loss: 0.0805 - val_mean_absolute_error: 0.2136\n",
            "Epoch 347/1000\n",
            "2688/2688 [==============================] - 2s 918us/step - loss: 0.0268 - mean_absolute_error: 0.1264 - val_loss: 0.0821 - val_mean_absolute_error: 0.2154\n",
            "Epoch 348/1000\n",
            "2688/2688 [==============================] - 2s 913us/step - loss: 0.0269 - mean_absolute_error: 0.1265 - val_loss: 0.0792 - val_mean_absolute_error: 0.2116\n",
            "Epoch 349/1000\n",
            "2688/2688 [==============================] - 2s 902us/step - loss: 0.0264 - mean_absolute_error: 0.1253 - val_loss: 0.0797 - val_mean_absolute_error: 0.2119\n",
            "Epoch 350/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0264 - mean_absolute_error: 0.1253 - val_loss: 0.0799 - val_mean_absolute_error: 0.2129\n",
            "Epoch 351/1000\n",
            "2688/2688 [==============================] - 2s 909us/step - loss: 0.0264 - mean_absolute_error: 0.1255 - val_loss: 0.0807 - val_mean_absolute_error: 0.2140\n",
            "Epoch 352/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0263 - mean_absolute_error: 0.1250 - val_loss: 0.0806 - val_mean_absolute_error: 0.2138\n",
            "Epoch 353/1000\n",
            "2688/2688 [==============================] - 3s 932us/step - loss: 0.0263 - mean_absolute_error: 0.1252 - val_loss: 0.0803 - val_mean_absolute_error: 0.2131\n",
            "Epoch 354/1000\n",
            "2688/2688 [==============================] - 2s 909us/step - loss: 0.0262 - mean_absolute_error: 0.1250 - val_loss: 0.0796 - val_mean_absolute_error: 0.2120\n",
            "Epoch 355/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.0268 - mean_absolute_error: 0.1264 - val_loss: 0.0812 - val_mean_absolute_error: 0.2138\n",
            "Epoch 356/1000\n",
            "2688/2688 [==============================] - 2s 898us/step - loss: 0.0268 - mean_absolute_error: 0.1262 - val_loss: 0.0798 - val_mean_absolute_error: 0.2122\n",
            "Epoch 357/1000\n",
            "2688/2688 [==============================] - 2s 914us/step - loss: 0.0276 - mean_absolute_error: 0.1283 - val_loss: 0.0809 - val_mean_absolute_error: 0.2138\n",
            "Epoch 358/1000\n",
            "2688/2688 [==============================] - 2s 916us/step - loss: 0.0277 - mean_absolute_error: 0.1286 - val_loss: 0.0800 - val_mean_absolute_error: 0.2125\n",
            "Epoch 359/1000\n",
            "2688/2688 [==============================] - 2s 913us/step - loss: 0.0264 - mean_absolute_error: 0.1253 - val_loss: 0.0799 - val_mean_absolute_error: 0.2130\n",
            "Epoch 360/1000\n",
            "2688/2688 [==============================] - 2s 902us/step - loss: 0.0265 - mean_absolute_error: 0.1258 - val_loss: 0.0787 - val_mean_absolute_error: 0.2108\n",
            "Epoch 361/1000\n",
            "2688/2688 [==============================] - 2s 902us/step - loss: 0.0256 - mean_absolute_error: 0.1233 - val_loss: 0.0803 - val_mean_absolute_error: 0.2130\n",
            "Epoch 362/1000\n",
            "2688/2688 [==============================] - 2s 908us/step - loss: 0.0258 - mean_absolute_error: 0.1239 - val_loss: 0.0808 - val_mean_absolute_error: 0.2143\n",
            "Epoch 363/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0261 - mean_absolute_error: 0.1247 - val_loss: 0.0795 - val_mean_absolute_error: 0.2119\n",
            "Epoch 364/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.0262 - mean_absolute_error: 0.1251 - val_loss: 0.0799 - val_mean_absolute_error: 0.2126\n",
            "Epoch 365/1000\n",
            "2688/2688 [==============================] - 2s 919us/step - loss: 0.0259 - mean_absolute_error: 0.1243 - val_loss: 0.0807 - val_mean_absolute_error: 0.2127\n",
            "Epoch 366/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0258 - mean_absolute_error: 0.1239 - val_loss: 0.0794 - val_mean_absolute_error: 0.2119\n",
            "Epoch 367/1000\n",
            "2688/2688 [==============================] - 2s 898us/step - loss: 0.0266 - mean_absolute_error: 0.1259 - val_loss: 0.0795 - val_mean_absolute_error: 0.2117\n",
            "Epoch 368/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0276 - mean_absolute_error: 0.1283 - val_loss: 0.0811 - val_mean_absolute_error: 0.2139\n",
            "Epoch 369/1000\n",
            "2688/2688 [==============================] - 2s 902us/step - loss: 0.0264 - mean_absolute_error: 0.1253 - val_loss: 0.0801 - val_mean_absolute_error: 0.2125\n",
            "Epoch 370/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0259 - mean_absolute_error: 0.1241 - val_loss: 0.0788 - val_mean_absolute_error: 0.2110\n",
            "Epoch 371/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0259 - mean_absolute_error: 0.1243 - val_loss: 0.0791 - val_mean_absolute_error: 0.2114\n",
            "Epoch 372/1000\n",
            "2688/2688 [==============================] - 2s 897us/step - loss: 0.0257 - mean_absolute_error: 0.1238 - val_loss: 0.0806 - val_mean_absolute_error: 0.2139\n",
            "Epoch 373/1000\n",
            "2688/2688 [==============================] - 2s 897us/step - loss: 0.0266 - mean_absolute_error: 0.1261 - val_loss: 0.0802 - val_mean_absolute_error: 0.2122\n",
            "Epoch 374/1000\n",
            "2688/2688 [==============================] - 2s 898us/step - loss: 0.0255 - mean_absolute_error: 0.1232 - val_loss: 0.0798 - val_mean_absolute_error: 0.2119\n",
            "Epoch 375/1000\n",
            "2688/2688 [==============================] - 2s 908us/step - loss: 0.0263 - mean_absolute_error: 0.1253 - val_loss: 0.0794 - val_mean_absolute_error: 0.2113\n",
            "Epoch 376/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0260 - mean_absolute_error: 0.1244 - val_loss: 0.0805 - val_mean_absolute_error: 0.2131\n",
            "Epoch 377/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0265 - mean_absolute_error: 0.1257 - val_loss: 0.0787 - val_mean_absolute_error: 0.2107\n",
            "Epoch 378/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.0257 - mean_absolute_error: 0.1237 - val_loss: 0.0792 - val_mean_absolute_error: 0.2113\n",
            "Epoch 379/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0259 - mean_absolute_error: 0.1242 - val_loss: 0.0803 - val_mean_absolute_error: 0.2124\n",
            "Epoch 380/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0261 - mean_absolute_error: 0.1247 - val_loss: 0.0792 - val_mean_absolute_error: 0.2115\n",
            "Epoch 381/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.0254 - mean_absolute_error: 0.1231 - val_loss: 0.0781 - val_mean_absolute_error: 0.2101\n",
            "Epoch 382/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.0254 - mean_absolute_error: 0.1228 - val_loss: 0.0787 - val_mean_absolute_error: 0.2108\n",
            "Epoch 383/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0251 - mean_absolute_error: 0.1222 - val_loss: 0.0789 - val_mean_absolute_error: 0.2111\n",
            "Epoch 384/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.0258 - mean_absolute_error: 0.1242 - val_loss: 0.0799 - val_mean_absolute_error: 0.2131\n",
            "Epoch 385/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.0258 - mean_absolute_error: 0.1242 - val_loss: 0.0804 - val_mean_absolute_error: 0.2127\n",
            "Epoch 386/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.0256 - mean_absolute_error: 0.1234 - val_loss: 0.0791 - val_mean_absolute_error: 0.2113\n",
            "Epoch 387/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0260 - mean_absolute_error: 0.1246 - val_loss: 0.0803 - val_mean_absolute_error: 0.2125\n",
            "Epoch 388/1000\n",
            "2688/2688 [==============================] - 2s 895us/step - loss: 0.0260 - mean_absolute_error: 0.1244 - val_loss: 0.0795 - val_mean_absolute_error: 0.2127\n",
            "Epoch 389/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.0266 - mean_absolute_error: 0.1262 - val_loss: 0.0803 - val_mean_absolute_error: 0.2134\n",
            "Epoch 390/1000\n",
            "2688/2688 [==============================] - 2s 890us/step - loss: 0.0261 - mean_absolute_error: 0.1247 - val_loss: 0.0785 - val_mean_absolute_error: 0.2102\n",
            "Epoch 391/1000\n",
            "2688/2688 [==============================] - 2s 915us/step - loss: 0.0262 - mean_absolute_error: 0.1251 - val_loss: 0.0803 - val_mean_absolute_error: 0.2140\n",
            "Epoch 392/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0254 - mean_absolute_error: 0.1231 - val_loss: 0.0789 - val_mean_absolute_error: 0.2107\n",
            "Epoch 393/1000\n",
            "2688/2688 [==============================] - 2s 913us/step - loss: 0.0263 - mean_absolute_error: 0.1250 - val_loss: 0.0794 - val_mean_absolute_error: 0.2117\n",
            "Epoch 394/1000\n",
            "2688/2688 [==============================] - 2s 915us/step - loss: 0.0252 - mean_absolute_error: 0.1225 - val_loss: 0.0789 - val_mean_absolute_error: 0.2109\n",
            "Epoch 395/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0247 - mean_absolute_error: 0.1213 - val_loss: 0.0804 - val_mean_absolute_error: 0.2134\n",
            "Epoch 396/1000\n",
            "2688/2688 [==============================] - 2s 916us/step - loss: 0.0246 - mean_absolute_error: 0.1210 - val_loss: 0.0784 - val_mean_absolute_error: 0.2107\n",
            "Epoch 397/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0244 - mean_absolute_error: 0.1205 - val_loss: 0.0791 - val_mean_absolute_error: 0.2112\n",
            "Epoch 398/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.0252 - mean_absolute_error: 0.1226 - val_loss: 0.0791 - val_mean_absolute_error: 0.2111\n",
            "Epoch 399/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.0248 - mean_absolute_error: 0.1216 - val_loss: 0.0803 - val_mean_absolute_error: 0.2126\n",
            "Epoch 400/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.0257 - mean_absolute_error: 0.1237 - val_loss: 0.0784 - val_mean_absolute_error: 0.2109\n",
            "Epoch 401/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.0255 - mean_absolute_error: 0.1232 - val_loss: 0.0788 - val_mean_absolute_error: 0.2112\n",
            "Epoch 402/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0262 - mean_absolute_error: 0.1250 - val_loss: 0.0793 - val_mean_absolute_error: 0.2113\n",
            "Epoch 403/1000\n",
            "2688/2688 [==============================] - 2s 892us/step - loss: 0.0257 - mean_absolute_error: 0.1238 - val_loss: 0.0785 - val_mean_absolute_error: 0.2111\n",
            "Epoch 404/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.0251 - mean_absolute_error: 0.1223 - val_loss: 0.0793 - val_mean_absolute_error: 0.2112\n",
            "Epoch 405/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.0257 - mean_absolute_error: 0.1239 - val_loss: 0.0792 - val_mean_absolute_error: 0.2115\n",
            "Epoch 406/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.0255 - mean_absolute_error: 0.1232 - val_loss: 0.0778 - val_mean_absolute_error: 0.2094\n",
            "Epoch 407/1000\n",
            "2688/2688 [==============================] - 2s 898us/step - loss: 0.0246 - mean_absolute_error: 0.1210 - val_loss: 0.0786 - val_mean_absolute_error: 0.2106\n",
            "Epoch 408/1000\n",
            "2688/2688 [==============================] - 2s 894us/step - loss: 0.0240 - mean_absolute_error: 0.1195 - val_loss: 0.0785 - val_mean_absolute_error: 0.2102\n",
            "Epoch 409/1000\n",
            "2688/2688 [==============================] - 2s 897us/step - loss: 0.0246 - mean_absolute_error: 0.1212 - val_loss: 0.0795 - val_mean_absolute_error: 0.2120\n",
            "Epoch 410/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.0249 - mean_absolute_error: 0.1218 - val_loss: 0.0789 - val_mean_absolute_error: 0.2116\n",
            "Epoch 411/1000\n",
            "2688/2688 [==============================] - 2s 888us/step - loss: 0.0242 - mean_absolute_error: 0.1201 - val_loss: 0.0782 - val_mean_absolute_error: 0.2100\n",
            "Epoch 412/1000\n",
            "2688/2688 [==============================] - 2s 888us/step - loss: 0.0242 - mean_absolute_error: 0.1199 - val_loss: 0.0782 - val_mean_absolute_error: 0.2096\n",
            "Epoch 413/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.0239 - mean_absolute_error: 0.1192 - val_loss: 0.0789 - val_mean_absolute_error: 0.2106\n",
            "Epoch 414/1000\n",
            "2688/2688 [==============================] - 2s 880us/step - loss: 0.0248 - mean_absolute_error: 0.1216 - val_loss: 0.0780 - val_mean_absolute_error: 0.2096\n",
            "Epoch 415/1000\n",
            "2688/2688 [==============================] - 2s 880us/step - loss: 0.0248 - mean_absolute_error: 0.1215 - val_loss: 0.0775 - val_mean_absolute_error: 0.2090\n",
            "Epoch 416/1000\n",
            "2688/2688 [==============================] - 2s 879us/step - loss: 0.0244 - mean_absolute_error: 0.1206 - val_loss: 0.0783 - val_mean_absolute_error: 0.2101\n",
            "Epoch 417/1000\n",
            "2688/2688 [==============================] - 2s 874us/step - loss: 0.0247 - mean_absolute_error: 0.1215 - val_loss: 0.0780 - val_mean_absolute_error: 0.2095\n",
            "Epoch 418/1000\n",
            "2688/2688 [==============================] - 2s 877us/step - loss: 0.0244 - mean_absolute_error: 0.1206 - val_loss: 0.0778 - val_mean_absolute_error: 0.2093\n",
            "Epoch 419/1000\n",
            "2688/2688 [==============================] - 2s 873us/step - loss: 0.0240 - mean_absolute_error: 0.1194 - val_loss: 0.0776 - val_mean_absolute_error: 0.2096\n",
            "Epoch 420/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.0262 - mean_absolute_error: 0.1249 - val_loss: 0.0789 - val_mean_absolute_error: 0.2107\n",
            "Epoch 421/1000\n",
            "2688/2688 [==============================] - 2s 872us/step - loss: 0.0257 - mean_absolute_error: 0.1237 - val_loss: 0.0783 - val_mean_absolute_error: 0.2107\n",
            "Epoch 422/1000\n",
            "2688/2688 [==============================] - 2s 872us/step - loss: 0.0245 - mean_absolute_error: 0.1209 - val_loss: 0.0775 - val_mean_absolute_error: 0.2089\n",
            "Epoch 423/1000\n",
            "2688/2688 [==============================] - 2s 866us/step - loss: 0.0243 - mean_absolute_error: 0.1202 - val_loss: 0.0776 - val_mean_absolute_error: 0.2095\n",
            "Epoch 424/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0247 - mean_absolute_error: 0.1214 - val_loss: 0.0799 - val_mean_absolute_error: 0.2119\n",
            "Epoch 425/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0264 - mean_absolute_error: 0.1255 - val_loss: 0.0825 - val_mean_absolute_error: 0.2150\n",
            "Epoch 426/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.0275 - mean_absolute_error: 0.1281 - val_loss: 0.0779 - val_mean_absolute_error: 0.2100\n",
            "Epoch 427/1000\n",
            "2688/2688 [==============================] - 2s 890us/step - loss: 0.0259 - mean_absolute_error: 0.1242 - val_loss: 0.0777 - val_mean_absolute_error: 0.2099\n",
            "Epoch 428/1000\n",
            "2688/2688 [==============================] - 2s 880us/step - loss: 0.0250 - mean_absolute_error: 0.1222 - val_loss: 0.0772 - val_mean_absolute_error: 0.2085\n",
            "Epoch 429/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.0234 - mean_absolute_error: 0.1180 - val_loss: 0.0768 - val_mean_absolute_error: 0.2080\n",
            "Epoch 430/1000\n",
            "2688/2688 [==============================] - 2s 897us/step - loss: 0.0229 - mean_absolute_error: 0.1165 - val_loss: 0.0774 - val_mean_absolute_error: 0.2088\n",
            "Epoch 431/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.0232 - mean_absolute_error: 0.1174 - val_loss: 0.0774 - val_mean_absolute_error: 0.2087\n",
            "Epoch 432/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.0234 - mean_absolute_error: 0.1181 - val_loss: 0.0778 - val_mean_absolute_error: 0.2090\n",
            "Epoch 433/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0230 - mean_absolute_error: 0.1169 - val_loss: 0.0786 - val_mean_absolute_error: 0.2105\n",
            "Epoch 434/1000\n",
            "2688/2688 [==============================] - 2s 879us/step - loss: 0.0239 - mean_absolute_error: 0.1194 - val_loss: 0.0777 - val_mean_absolute_error: 0.2092\n",
            "Epoch 435/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.0238 - mean_absolute_error: 0.1193 - val_loss: 0.0777 - val_mean_absolute_error: 0.2089\n",
            "Epoch 436/1000\n",
            "2688/2688 [==============================] - 2s 888us/step - loss: 0.0258 - mean_absolute_error: 0.1241 - val_loss: 0.0776 - val_mean_absolute_error: 0.2094\n",
            "Epoch 437/1000\n",
            "2688/2688 [==============================] - 2s 893us/step - loss: 0.0253 - mean_absolute_error: 0.1228 - val_loss: 0.0798 - val_mean_absolute_error: 0.2114\n",
            "Epoch 438/1000\n",
            "2688/2688 [==============================] - 2s 882us/step - loss: 0.0247 - mean_absolute_error: 0.1212 - val_loss: 0.0787 - val_mean_absolute_error: 0.2107\n",
            "Epoch 439/1000\n",
            "2688/2688 [==============================] - 2s 877us/step - loss: 0.0238 - mean_absolute_error: 0.1191 - val_loss: 0.0779 - val_mean_absolute_error: 0.2094\n",
            "Epoch 440/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.0239 - mean_absolute_error: 0.1192 - val_loss: 0.0776 - val_mean_absolute_error: 0.2091\n",
            "Epoch 441/1000\n",
            "2688/2688 [==============================] - 2s 894us/step - loss: 0.0238 - mean_absolute_error: 0.1191 - val_loss: 0.0779 - val_mean_absolute_error: 0.2097\n",
            "Epoch 442/1000\n",
            "2688/2688 [==============================] - 2s 874us/step - loss: 0.0238 - mean_absolute_error: 0.1190 - val_loss: 0.0782 - val_mean_absolute_error: 0.2102\n",
            "Epoch 443/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0235 - mean_absolute_error: 0.1182 - val_loss: 0.0771 - val_mean_absolute_error: 0.2086\n",
            "Epoch 444/1000\n",
            "2688/2688 [==============================] - 2s 868us/step - loss: 0.0239 - mean_absolute_error: 0.1192 - val_loss: 0.0783 - val_mean_absolute_error: 0.2099\n",
            "Epoch 445/1000\n",
            "2688/2688 [==============================] - 2s 872us/step - loss: 0.0241 - mean_absolute_error: 0.1199 - val_loss: 0.0793 - val_mean_absolute_error: 0.2115\n",
            "Epoch 446/1000\n",
            "2688/2688 [==============================] - 2s 868us/step - loss: 0.0246 - mean_absolute_error: 0.1211 - val_loss: 0.0772 - val_mean_absolute_error: 0.2084\n",
            "Epoch 447/1000\n",
            "2688/2688 [==============================] - 2s 875us/step - loss: 0.0243 - mean_absolute_error: 0.1202 - val_loss: 0.0771 - val_mean_absolute_error: 0.2081\n",
            "Epoch 448/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.0239 - mean_absolute_error: 0.1193 - val_loss: 0.0771 - val_mean_absolute_error: 0.2090\n",
            "Epoch 449/1000\n",
            "2688/2688 [==============================] - 2s 866us/step - loss: 0.0245 - mean_absolute_error: 0.1210 - val_loss: 0.0790 - val_mean_absolute_error: 0.2117\n",
            "Epoch 450/1000\n",
            "2688/2688 [==============================] - 2s 873us/step - loss: 0.0242 - mean_absolute_error: 0.1201 - val_loss: 0.0780 - val_mean_absolute_error: 0.2094\n",
            "Epoch 451/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.0243 - mean_absolute_error: 0.1202 - val_loss: 0.0783 - val_mean_absolute_error: 0.2106\n",
            "Epoch 452/1000\n",
            "2688/2688 [==============================] - 2s 879us/step - loss: 0.0237 - mean_absolute_error: 0.1187 - val_loss: 0.0779 - val_mean_absolute_error: 0.2094\n",
            "Epoch 453/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.0232 - mean_absolute_error: 0.1174 - val_loss: 0.0780 - val_mean_absolute_error: 0.2098\n",
            "Epoch 454/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.0235 - mean_absolute_error: 0.1182 - val_loss: 0.0776 - val_mean_absolute_error: 0.2099\n",
            "Epoch 455/1000\n",
            "2688/2688 [==============================] - 2s 882us/step - loss: 0.0240 - mean_absolute_error: 0.1194 - val_loss: 0.0778 - val_mean_absolute_error: 0.2105\n",
            "Epoch 456/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.0239 - mean_absolute_error: 0.1192 - val_loss: 0.0791 - val_mean_absolute_error: 0.2112\n",
            "Epoch 457/1000\n",
            "2688/2688 [==============================] - 2s 890us/step - loss: 0.0243 - mean_absolute_error: 0.1204 - val_loss: 0.0787 - val_mean_absolute_error: 0.2112\n",
            "Epoch 458/1000\n",
            "2688/2688 [==============================] - 2s 895us/step - loss: 0.0243 - mean_absolute_error: 0.1203 - val_loss: 0.0780 - val_mean_absolute_error: 0.2093\n",
            "Epoch 459/1000\n",
            "2688/2688 [==============================] - 2s 875us/step - loss: 0.0245 - mean_absolute_error: 0.1209 - val_loss: 0.0773 - val_mean_absolute_error: 0.2086\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcc35ac3390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "RfvCtJTd5bsr",
        "colab_type": "code",
        "outputId": "bd50bdde-22dc-4d4c-fb7a-8c1f01d4e852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "error = model.evaluate(val_x,val_y)\n",
        "print (error)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "840/840 [==============================] - 0s 257us/step\n",
            "[0.08205240950697944, 0.21424394803387778]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5CWRCpqll4ni",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3X8AU2h2xVai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing dataset NRMSE Loss \n"
      ]
    },
    {
      "metadata": {
        "id": "O3eB1StFxc8P",
        "colab_type": "code",
        "outputId": "98f5c4a3-8ecd-4ab3-e466-44df5a13b829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def nrmse(y_true,y_pred):\n",
        "\n",
        "    return (np.sqrt(np.mean(np.square(y_true - y_pred))))/np.mean(y_pred)\n",
        "def calculate_nrmse():\n",
        "  y_pred = np.exp(model.predict(val_x))\n",
        "  print (f'shape of y_pred {y_pred.shape}')\n",
        "  y_true = np.exp(val_y)\n",
        "  print (f'shape of y_true {y_true.shape}')\n",
        "\n",
        "  loss =  nrmse(y_true,y_pred)\n",
        "\n",
        "  print (loss)\n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "nrmse_loss = calculate_nrmse()\n",
        "print ('claculate valuatdaion NRMSE loss', nrmse_loss)\n",
        "\n",
        "print ('\\n', 'start preparing test data')\n",
        "#print (os.path.isfile(src+'test2.npy'))\n",
        "test = np.load(src+'test2.npy')\n",
        "test_data = (test - data_mean)/data_std\n",
        "\n",
        "test_data = test_data[:,:,:,select_list]\n",
        "\n",
        "print ('test_data shape', test_data.shape)\n",
        "\n",
        "\n",
        "prediction = model.predict(test_data)\n",
        "prediction = np.exp(prediction)\n",
        "pred = prediction.reshape(1800,-1)\n",
        "#print (pred.shape)\n",
        "\n",
        "\n",
        "\n",
        "src_2 =  'drive/My Drive/geo/test_data/test_dataset/'\n",
        "\n",
        "test_label_df = pd.read_csv(src_2+'solution.csv')\n",
        "\n",
        "\n",
        "\n",
        "test_label_df = test_label_df.drop('ID', axis=1)\n",
        "\n",
        "\n",
        "test_label = test_label_df.values\n",
        "\n",
        "\n",
        "print ('test set NRMSE error', nrmse(test_label,pred))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of y_pred (840, 20, 20, 1)\n",
            "shape of y_true (840, 20, 20, 1)\n",
            "0.4127597365925097\n",
            "claculate valuatdaion NRMSE loss 0.4127597365925097\n",
            "\n",
            " start preparing test data\n",
            "test_data shape (1800, 21, 21, 5)\n",
            "test set NRMSE error 0.4015404241235492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0L0xOQ4D06yk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}