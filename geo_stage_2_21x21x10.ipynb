{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geo_stage_2_21x21x10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcglarry/geo/blob/master/geo_stage_2_21x21x10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kUbioL1FmGXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "c80ac373-6161-4938-cd4c-50443bae7365"
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"ali_build_model_stage_2_MobileNetV2_aug_1000_dense.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1wSVEmcoWzRm-7B6UGkHB8ysfMDRT_xK5\n",
        "\"\"\"\n",
        "\n",
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 110377 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8aoJd93CmGao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "366854bb-d08a-431b-df06-13739d9e2804"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TQX-eTtjmGhS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "c0eebe13-f93e-4b1a-fe1a-4dadb64f2d84"
      },
      "cell_type": "code",
      "source": [
        "!ls 'drive/My Drive/geo/'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "geo_Unet_colab.ipynb\t\tUntitled1.ipynb\n",
            "geo_Unet.ipynb\t\t\tUntitled2.ipynb\n",
            "test2.npy\t\t\tupload_sample.csv\n",
            "test_submit_0831_2.csv\t\twater_first_try_2_2.h5\n",
            "test_submit_0831_3.csv\t\twater_first_try_2.h5\n",
            "test_submit_0831_ch32.csv\twater_first_try_2_prepro_ch16.h5\n",
            "test_submit_0831_ch32_gram.csv\twater_first_try_2_prepro_ch32_gram.h5\n",
            "test_submit_0831_ch64_gram.csv\twater_first_try_2_prepro_ch32.h5\n",
            "test_submit_0831.csv\t\twater_first_try_2_prepro_ch64_gram.h5\n",
            "test_submit.csv\t\t\twater_first_try_2_prepro.h5\n",
            "train_data.npy\t\t\twater_first_try_2_prepro_twoloss.h5\n",
            "train_labels.npy\t\twater_first_try_3_prepro.h5\n",
            "Untitled0.ipynb\t\t\twater_first_try.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cD6mcE4Zl4m1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "src=  'drive/My Drive/geo/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gUqwwxril4m5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import subprocess\n",
        "import os\n",
        "import pickle\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, merge, Lambda,UpSampling2D, concatenate, \\\n",
        "Conv2DTranspose,BatchNormalization\n",
        "from keras.models import Model, load_model\n",
        "import pandas as pd\n",
        "import sklearn \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IhIuNLS6l4nC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3f375fae-aaed-48e6-cbea-24abd94697b6"
      },
      "cell_type": "code",
      "source": [
        "data= np.load(src+'train_data.npy')\n",
        "label = np.load(src+'train_labels.npy')\n",
        "label = label[:,:,:,np.newaxis]\n",
        "print (data.shape)\n",
        "print (label.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4200, 21, 21, 20)\n",
            "(4200, 20, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wtZI1t8InGyu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reduce channel from 21 to 11"
      ]
    },
    {
      "metadata": {
        "id": "VIin_TiJl4nH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffff586b-4365-451d-fd21-f7b36c168434"
      },
      "cell_type": "code",
      "source": [
        "index_list = list(range(20))\n",
        "select_list = [idx for idx in index_list if idx % 2  == 0 ]\n",
        "select_list\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "gY3f1GFgn5gl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e13fe5a-a6d1-4ff7-b62f-605afd0e7501"
      },
      "cell_type": "code",
      "source": [
        "data = data[:,:,:,select_list]\n",
        "data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4200, 21, 21, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Zlc2JmA2w_RG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_mean = np.mean(data)\n",
        "data_std = np.std(data)\n",
        "label_max = np.max(label)\n",
        "label_min = np.min(label)\n",
        "\n",
        "train_data = (data - data_mean)/data_std\n",
        "#label_data = (label-label_min)/(label_max - label_min)\n",
        "label_data = np.log(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iGeIz5AWl4nL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_x,val_x,train_y,val_y = train_test_split(train_data,label_data,test_size=0.2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_F7YnLWqzBU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_block(ch, activation= 'relu', padding='same',kernel_regularizer=regularizers.l2(0.01)):       \n",
        "    return Conv2D(ch,(3,3),activation= activation, padding =padding )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C8sDm7fZl4nQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#def build_model_1(IMAGE_HEIGHT=IMAGE_HEIGHT,IMAGE_WIDTH=IMAGE_WIDTH,ch=ch):\n",
        "def build_model_1(ch=8):\n",
        "    #inputs = Input((IMAGE_HEIGHT,IMAGE_WIDTH,ch))\n",
        "    inputs = Input((21,21,10))\n",
        "\n",
        "    conv0 = Conv2D(32,(2,2),padding='valid')(inputs)\n",
        "    print ('conv0',conv0.get_shape())\n",
        "    conv1 =  conv_block(ch)(conv0)\n",
        "    conv1 = conv_block(ch)(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
        "    print ('pool1',pool1.get_shape())\n",
        "    \n",
        "    conv2 =  conv_block(ch*2)(pool1)\n",
        "    conv2 = conv_block(ch*2)(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
        "    print ('poo12',pool2.get_shape())\n",
        "    \n",
        "    conv_test = Conv2D(64,(2,2),padding='valid')(pool2)\n",
        "    print ('conv_test',conv_test.get_shape())\n",
        "    \n",
        "    \n",
        "    \n",
        "    conv3 =  conv_block(ch*4)(conv_test)\n",
        "    conv3 = conv_block(ch*4)(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
        "    print ('poo13',pool3.get_shape())\n",
        "    \n",
        "    conv4 =  conv_block(ch*8)(pool3)\n",
        "    conv4 = conv_block(ch*8)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2,2))(conv4)\n",
        "    print ('poo14',pool4.get_shape())\n",
        "    \n",
        "    conv5 =  conv_block(ch*16)(pool4)\n",
        "    conv5 = conv_block(ch*16)(conv5)\n",
        "\n",
        "    \n",
        "    up6 = concatenate([UpSampling2D(size=(2,2))(conv5), conv4], axis=3)\n",
        "    conv6 = conv_block(ch*8)(up6)\n",
        "    conv6 = conv_block(ch*8)(conv6)\n",
        "    \n",
        "    up7 = concatenate ([UpSampling2D(size=(2,2))(conv6), conv3],  axis=3)\n",
        "    conv7 = conv_block(ch*4)(up7)\n",
        "    conv7 = conv_block(ch*4)(conv7)\n",
        "    print ('conv7',conv7.get_shape())\n",
        "    conv7 = Conv2DTranspose(ch*4,(2,2),padding='valid')(conv7)\n",
        "    print ('conv7',conv7.get_shape())\n",
        "    \n",
        "    up8 = concatenate([UpSampling2D(size=(2,2))(conv7), conv2], axis=3)\n",
        "    conv8 = conv_block(ch*2)(up8)\n",
        "    conv8 = conv_block(ch*2)(conv8)\n",
        "    \n",
        "    up9 = concatenate([UpSampling2D(size=(2,2))(conv8), conv1],  axis=3)\n",
        "    conv9 = conv_block(ch)(up9)\n",
        "    conv9 = conv_block(ch)(conv9)\n",
        "    \n",
        "    \n",
        "    \n",
        "    conv10 = Conv2D(1,(1,1))(conv9)\n",
        "    \n",
        "  \n",
        "    model = Model(inputs= inputs, outputs=conv10)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WqpLnCCCoXnF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1598
        },
        "outputId": "b2c8f44f-e32a-4381-f6e0-533a7ec61e2d"
      },
      "cell_type": "code",
      "source": [
        "model = build_model_1()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv0 (?, 20, 20, 32)\n",
            "pool1 (?, 10, 10, 8)\n",
            "poo12 (?, 5, 5, 16)\n",
            "conv_test (?, 4, 4, 64)\n",
            "poo13 (?, 2, 2, 32)\n",
            "poo14 (?, 1, 1, 64)\n",
            "conv7 (?, 4, 4, 32)\n",
            "conv7 (?, ?, ?, 32)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 21, 21, 10)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 20, 20, 32)   1312        input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 20, 20, 8)    2312        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 20, 20, 8)    584         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling2D) (None, 10, 10, 8)    0           conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 10, 10, 16)   1168        max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 10, 10, 16)   2320        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling2D) (None, 5, 5, 16)     0           conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 4, 4, 64)     4160        max_pooling2d_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 4, 4, 32)     18464       conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 4, 4, 32)     9248        conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling2D) (None, 2, 2, 32)     0           conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 2, 2, 64)     18496       max_pooling2d_19[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 2, 2, 64)     36928       conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling2D) (None, 1, 1, 64)     0           conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 1, 1, 128)    73856       max_pooling2d_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 1, 1, 128)    147584      conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_17 (UpSampling2D) (None, 2, 2, 128)    0           conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 2, 2, 192)    0           up_sampling2d_17[0][0]           \n",
            "                                                                 conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 2, 2, 64)     110656      concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 2, 2, 64)     36928       conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_18 (UpSampling2D) (None, 4, 4, 64)     0           conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 4, 4, 96)     0           up_sampling2d_18[0][0]           \n",
            "                                                                 conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 4, 4, 32)     27680       concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 4, 4, 32)     9248        conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTrans (None, 5, 5, 32)     4128        conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_19 (UpSampling2D) (None, 10, 10, 32)   0           conv2d_transpose_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 10, 10, 48)   0           up_sampling2d_19[0][0]           \n",
            "                                                                 conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 10, 10, 16)   6928        concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 10, 10, 16)   2320        conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_20 (UpSampling2D) (None, 20, 20, 16)   0           conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 20, 20, 24)   0           up_sampling2d_20[0][0]           \n",
            "                                                                 conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 20, 20, 8)    1736        concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 20, 20, 8)    584         conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 20, 20, 1)    9           conv2d_106[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 516,649\n",
            "Trainable params: 516,649\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mTSiLPYtl4nU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile (optimizer='Adam', loss = 'mse', metrics =['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bQ1YM-aml4nZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98411a65-d5e7-4614-b48f-235f5493b817"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# train the model on the new data for a few epochs\n",
        "\n",
        "use_model= False\n",
        "if use_model:\n",
        "    \n",
        "\n",
        "    if os.path.isfile(src+'water_first_try.h5'):\n",
        "\n",
        "      model = load_model(src+'water_first_try.h5')\n",
        "      print ('model laoded')\n",
        "    else:\n",
        "      print ('model not exist')\n",
        "else:\n",
        "  print ('not to use model')\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "not to use model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U2NKk1wFl4nc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10647
        },
        "outputId": "c4494e70-712c-4421-aee7-67f0fb8def43"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=src+'geo_stage2_first_try.h5', monitor = 'val_loss', save_best_only=True, mode= 'auto')\n",
        "earlystop = EarlyStopping(patience=30)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=15, min_lr=0.00001,verbose=1)\n",
        "callback_list = [checkpoint, earlystop,reduce_lr]\n",
        "model.fit(x=train_x, y=train_y, batch_size=32, epochs=1000, callbacks = callback_list, verbose=1, validation_split=0.2,  shuffle=True)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2688 samples, validate on 672 samples\n",
            "Epoch 1/1000\n",
            "2688/2688 [==============================] - 4s 2ms/step - loss: 0.9681 - mean_absolute_error: 0.7785 - val_loss: 0.7037 - val_mean_absolute_error: 0.6665\n",
            "Epoch 2/1000\n",
            "2688/2688 [==============================] - 3s 940us/step - loss: 0.6196 - mean_absolute_error: 0.6245 - val_loss: 0.5419 - val_mean_absolute_error: 0.5809\n",
            "Epoch 3/1000\n",
            "2688/2688 [==============================] - 3s 983us/step - loss: 0.4952 - mean_absolute_error: 0.5547 - val_loss: 0.4689 - val_mean_absolute_error: 0.5376\n",
            "Epoch 4/1000\n",
            "2688/2688 [==============================] - 3s 980us/step - loss: 0.4154 - mean_absolute_error: 0.5056 - val_loss: 0.3707 - val_mean_absolute_error: 0.4769\n",
            "Epoch 5/1000\n",
            "2688/2688 [==============================] - 3s 975us/step - loss: 0.3540 - mean_absolute_error: 0.4649 - val_loss: 0.3331 - val_mean_absolute_error: 0.4510\n",
            "Epoch 6/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.3153 - mean_absolute_error: 0.4382 - val_loss: 0.3051 - val_mean_absolute_error: 0.4321\n",
            "Epoch 7/1000\n",
            "2688/2688 [==============================] - 3s 966us/step - loss: 0.2843 - mean_absolute_error: 0.4155 - val_loss: 0.2710 - val_mean_absolute_error: 0.4039\n",
            "Epoch 8/1000\n",
            "2688/2688 [==============================] - 3s 978us/step - loss: 0.2639 - mean_absolute_error: 0.4000 - val_loss: 0.2587 - val_mean_absolute_error: 0.3960\n",
            "Epoch 9/1000\n",
            "2688/2688 [==============================] - 3s 984us/step - loss: 0.2483 - mean_absolute_error: 0.3876 - val_loss: 0.2407 - val_mean_absolute_error: 0.3799\n",
            "Epoch 10/1000\n",
            "2688/2688 [==============================] - 3s 986us/step - loss: 0.2335 - mean_absolute_error: 0.3755 - val_loss: 0.2247 - val_mean_absolute_error: 0.3689\n",
            "Epoch 11/1000\n",
            "2688/2688 [==============================] - 3s 981us/step - loss: 0.2213 - mean_absolute_error: 0.3658 - val_loss: 0.2132 - val_mean_absolute_error: 0.3570\n",
            "Epoch 12/1000\n",
            "2688/2688 [==============================] - 3s 990us/step - loss: 0.2086 - mean_absolute_error: 0.3543 - val_loss: 0.2047 - val_mean_absolute_error: 0.3508\n",
            "Epoch 13/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.2009 - mean_absolute_error: 0.3479 - val_loss: 0.2031 - val_mean_absolute_error: 0.3492\n",
            "Epoch 14/1000\n",
            "2688/2688 [==============================] - 3s 991us/step - loss: 0.1932 - mean_absolute_error: 0.3409 - val_loss: 0.1922 - val_mean_absolute_error: 0.3409\n",
            "Epoch 15/1000\n",
            "2688/2688 [==============================] - 3s 975us/step - loss: 0.1868 - mean_absolute_error: 0.3351 - val_loss: 0.1890 - val_mean_absolute_error: 0.3389\n",
            "Epoch 16/1000\n",
            "2688/2688 [==============================] - 3s 982us/step - loss: 0.1809 - mean_absolute_error: 0.3298 - val_loss: 0.1835 - val_mean_absolute_error: 0.3312\n",
            "Epoch 17/1000\n",
            "2688/2688 [==============================] - 3s 965us/step - loss: 0.1736 - mean_absolute_error: 0.3227 - val_loss: 0.1756 - val_mean_absolute_error: 0.3229\n",
            "Epoch 18/1000\n",
            "2688/2688 [==============================] - 3s 972us/step - loss: 0.1673 - mean_absolute_error: 0.3170 - val_loss: 0.1759 - val_mean_absolute_error: 0.3237\n",
            "Epoch 19/1000\n",
            "2688/2688 [==============================] - 3s 954us/step - loss: 0.1646 - mean_absolute_error: 0.3144 - val_loss: 0.1658 - val_mean_absolute_error: 0.3152\n",
            "Epoch 20/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.1592 - mean_absolute_error: 0.3092 - val_loss: 0.1588 - val_mean_absolute_error: 0.3075\n",
            "Epoch 21/1000\n",
            "2688/2688 [==============================] - 2s 897us/step - loss: 0.1531 - mean_absolute_error: 0.3029 - val_loss: 0.1583 - val_mean_absolute_error: 0.3061\n",
            "Epoch 22/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.1509 - mean_absolute_error: 0.3007 - val_loss: 0.1541 - val_mean_absolute_error: 0.3042\n",
            "Epoch 23/1000\n",
            "2688/2688 [==============================] - 3s 936us/step - loss: 0.1465 - mean_absolute_error: 0.2964 - val_loss: 0.1490 - val_mean_absolute_error: 0.2976\n",
            "Epoch 24/1000\n",
            "2688/2688 [==============================] - 2s 919us/step - loss: 0.1413 - mean_absolute_error: 0.2908 - val_loss: 0.1520 - val_mean_absolute_error: 0.3007\n",
            "Epoch 25/1000\n",
            "2688/2688 [==============================] - 2s 923us/step - loss: 0.1402 - mean_absolute_error: 0.2896 - val_loss: 0.1452 - val_mean_absolute_error: 0.2930\n",
            "Epoch 26/1000\n",
            "2688/2688 [==============================] - 2s 925us/step - loss: 0.1355 - mean_absolute_error: 0.2847 - val_loss: 0.1426 - val_mean_absolute_error: 0.2909\n",
            "Epoch 27/1000\n",
            "2688/2688 [==============================] - 2s 923us/step - loss: 0.1340 - mean_absolute_error: 0.2831 - val_loss: 0.1378 - val_mean_absolute_error: 0.2855\n",
            "Epoch 28/1000\n",
            "2688/2688 [==============================] - 2s 927us/step - loss: 0.1311 - mean_absolute_error: 0.2799 - val_loss: 0.1374 - val_mean_absolute_error: 0.2853\n",
            "Epoch 29/1000\n",
            "2688/2688 [==============================] - 2s 926us/step - loss: 0.1288 - mean_absolute_error: 0.2775 - val_loss: 0.1395 - val_mean_absolute_error: 0.2868\n",
            "Epoch 30/1000\n",
            "2688/2688 [==============================] - 3s 935us/step - loss: 0.1262 - mean_absolute_error: 0.2747 - val_loss: 0.1335 - val_mean_absolute_error: 0.2806\n",
            "Epoch 31/1000\n",
            "2688/2688 [==============================] - 2s 928us/step - loss: 0.1224 - mean_absolute_error: 0.2705 - val_loss: 0.1327 - val_mean_absolute_error: 0.2793\n",
            "Epoch 32/1000\n",
            "2688/2688 [==============================] - 2s 924us/step - loss: 0.1197 - mean_absolute_error: 0.2675 - val_loss: 0.1322 - val_mean_absolute_error: 0.2786\n",
            "Epoch 33/1000\n",
            "2688/2688 [==============================] - 2s 922us/step - loss: 0.1177 - mean_absolute_error: 0.2653 - val_loss: 0.1251 - val_mean_absolute_error: 0.2720\n",
            "Epoch 34/1000\n",
            "2688/2688 [==============================] - 3s 938us/step - loss: 0.1161 - mean_absolute_error: 0.2633 - val_loss: 0.1326 - val_mean_absolute_error: 0.2788\n",
            "Epoch 35/1000\n",
            "2688/2688 [==============================] - 3s 947us/step - loss: 0.1138 - mean_absolute_error: 0.2607 - val_loss: 0.1238 - val_mean_absolute_error: 0.2690\n",
            "Epoch 36/1000\n",
            "2688/2688 [==============================] - 3s 933us/step - loss: 0.1111 - mean_absolute_error: 0.2577 - val_loss: 0.1227 - val_mean_absolute_error: 0.2708\n",
            "Epoch 37/1000\n",
            "2688/2688 [==============================] - 2s 920us/step - loss: 0.1110 - mean_absolute_error: 0.2572 - val_loss: 0.1217 - val_mean_absolute_error: 0.2695\n",
            "Epoch 38/1000\n",
            "2688/2688 [==============================] - 2s 928us/step - loss: 0.1083 - mean_absolute_error: 0.2542 - val_loss: 0.1203 - val_mean_absolute_error: 0.2652\n",
            "Epoch 39/1000\n",
            "2688/2688 [==============================] - 3s 946us/step - loss: 0.1056 - mean_absolute_error: 0.2512 - val_loss: 0.1215 - val_mean_absolute_error: 0.2687\n",
            "Epoch 40/1000\n",
            "2688/2688 [==============================] - 2s 929us/step - loss: 0.1058 - mean_absolute_error: 0.2511 - val_loss: 0.1178 - val_mean_absolute_error: 0.2628\n",
            "Epoch 41/1000\n",
            "2688/2688 [==============================] - 2s 917us/step - loss: 0.1029 - mean_absolute_error: 0.2478 - val_loss: 0.1192 - val_mean_absolute_error: 0.2638\n",
            "Epoch 42/1000\n",
            "2688/2688 [==============================] - 2s 923us/step - loss: 0.1016 - mean_absolute_error: 0.2464 - val_loss: 0.1149 - val_mean_absolute_error: 0.2594\n",
            "Epoch 43/1000\n",
            "2688/2688 [==============================] - 2s 923us/step - loss: 0.1001 - mean_absolute_error: 0.2444 - val_loss: 0.1183 - val_mean_absolute_error: 0.2641\n",
            "Epoch 44/1000\n",
            "2688/2688 [==============================] - 3s 935us/step - loss: 0.0976 - mean_absolute_error: 0.2416 - val_loss: 0.1175 - val_mean_absolute_error: 0.2619\n",
            "Epoch 45/1000\n",
            "2688/2688 [==============================] - 3s 931us/step - loss: 0.0981 - mean_absolute_error: 0.2421 - val_loss: 0.1119 - val_mean_absolute_error: 0.2552\n",
            "Epoch 46/1000\n",
            "2688/2688 [==============================] - 2s 924us/step - loss: 0.0961 - mean_absolute_error: 0.2395 - val_loss: 0.1129 - val_mean_absolute_error: 0.2570\n",
            "Epoch 47/1000\n",
            "2688/2688 [==============================] - 3s 940us/step - loss: 0.0934 - mean_absolute_error: 0.2362 - val_loss: 0.1170 - val_mean_absolute_error: 0.2603\n",
            "Epoch 48/1000\n",
            "2688/2688 [==============================] - 3s 955us/step - loss: 0.0934 - mean_absolute_error: 0.2362 - val_loss: 0.1123 - val_mean_absolute_error: 0.2547\n",
            "Epoch 49/1000\n",
            "2688/2688 [==============================] - 3s 939us/step - loss: 0.0925 - mean_absolute_error: 0.2350 - val_loss: 0.1074 - val_mean_absolute_error: 0.2498\n",
            "Epoch 50/1000\n",
            "2688/2688 [==============================] - 2s 921us/step - loss: 0.0934 - mean_absolute_error: 0.2362 - val_loss: 0.1069 - val_mean_absolute_error: 0.2493\n",
            "Epoch 51/1000\n",
            "2688/2688 [==============================] - 3s 933us/step - loss: 0.0890 - mean_absolute_error: 0.2306 - val_loss: 0.1109 - val_mean_absolute_error: 0.2533\n",
            "Epoch 52/1000\n",
            "2688/2688 [==============================] - 3s 965us/step - loss: 0.0890 - mean_absolute_error: 0.2306 - val_loss: 0.1059 - val_mean_absolute_error: 0.2490\n",
            "Epoch 53/1000\n",
            "2688/2688 [==============================] - 3s 980us/step - loss: 0.0868 - mean_absolute_error: 0.2278 - val_loss: 0.1082 - val_mean_absolute_error: 0.2521\n",
            "Epoch 54/1000\n",
            "2688/2688 [==============================] - 3s 971us/step - loss: 0.0861 - mean_absolute_error: 0.2268 - val_loss: 0.1100 - val_mean_absolute_error: 0.2527\n",
            "Epoch 55/1000\n",
            "2688/2688 [==============================] - 3s 973us/step - loss: 0.0862 - mean_absolute_error: 0.2269 - val_loss: 0.1062 - val_mean_absolute_error: 0.2499\n",
            "Epoch 56/1000\n",
            "2688/2688 [==============================] - 3s 956us/step - loss: 0.0855 - mean_absolute_error: 0.2263 - val_loss: 0.1047 - val_mean_absolute_error: 0.2462\n",
            "Epoch 57/1000\n",
            "2688/2688 [==============================] - 3s 938us/step - loss: 0.0850 - mean_absolute_error: 0.2254 - val_loss: 0.1045 - val_mean_absolute_error: 0.2459\n",
            "Epoch 58/1000\n",
            "2688/2688 [==============================] - 3s 962us/step - loss: 0.0817 - mean_absolute_error: 0.2210 - val_loss: 0.1025 - val_mean_absolute_error: 0.2434\n",
            "Epoch 59/1000\n",
            "2688/2688 [==============================] - 3s 958us/step - loss: 0.0799 - mean_absolute_error: 0.2186 - val_loss: 0.1077 - val_mean_absolute_error: 0.2487\n",
            "Epoch 60/1000\n",
            "2688/2688 [==============================] - 3s 964us/step - loss: 0.0811 - mean_absolute_error: 0.2204 - val_loss: 0.1011 - val_mean_absolute_error: 0.2423\n",
            "Epoch 61/1000\n",
            "2688/2688 [==============================] - 3s 977us/step - loss: 0.0787 - mean_absolute_error: 0.2170 - val_loss: 0.1034 - val_mean_absolute_error: 0.2443\n",
            "Epoch 62/1000\n",
            "2688/2688 [==============================] - 3s 975us/step - loss: 0.0789 - mean_absolute_error: 0.2171 - val_loss: 0.1000 - val_mean_absolute_error: 0.2413\n",
            "Epoch 63/1000\n",
            "2688/2688 [==============================] - 3s 946us/step - loss: 0.0786 - mean_absolute_error: 0.2167 - val_loss: 0.1024 - val_mean_absolute_error: 0.2436\n",
            "Epoch 64/1000\n",
            "2688/2688 [==============================] - 3s 947us/step - loss: 0.0769 - mean_absolute_error: 0.2145 - val_loss: 0.1037 - val_mean_absolute_error: 0.2448\n",
            "Epoch 65/1000\n",
            "2688/2688 [==============================] - 3s 961us/step - loss: 0.0759 - mean_absolute_error: 0.2132 - val_loss: 0.0991 - val_mean_absolute_error: 0.2392\n",
            "Epoch 66/1000\n",
            "2688/2688 [==============================] - 3s 974us/step - loss: 0.0746 - mean_absolute_error: 0.2113 - val_loss: 0.1073 - val_mean_absolute_error: 0.2491\n",
            "Epoch 67/1000\n",
            "2688/2688 [==============================] - 3s 970us/step - loss: 0.0741 - mean_absolute_error: 0.2106 - val_loss: 0.1006 - val_mean_absolute_error: 0.2423\n",
            "Epoch 68/1000\n",
            "2688/2688 [==============================] - 3s 999us/step - loss: 0.0723 - mean_absolute_error: 0.2080 - val_loss: 0.0989 - val_mean_absolute_error: 0.2393\n",
            "Epoch 69/1000\n",
            "2688/2688 [==============================] - 3s 988us/step - loss: 0.0732 - mean_absolute_error: 0.2093 - val_loss: 0.1003 - val_mean_absolute_error: 0.2400\n",
            "Epoch 70/1000\n",
            "2688/2688 [==============================] - 3s 993us/step - loss: 0.0719 - mean_absolute_error: 0.2074 - val_loss: 0.1037 - val_mean_absolute_error: 0.2458\n",
            "Epoch 71/1000\n",
            "2688/2688 [==============================] - 3s 988us/step - loss: 0.0719 - mean_absolute_error: 0.2075 - val_loss: 0.1020 - val_mean_absolute_error: 0.2416\n",
            "Epoch 72/1000\n",
            "2688/2688 [==============================] - 3s 994us/step - loss: 0.0706 - mean_absolute_error: 0.2057 - val_loss: 0.1002 - val_mean_absolute_error: 0.2398\n",
            "Epoch 73/1000\n",
            "2688/2688 [==============================] - 3s 998us/step - loss: 0.0690 - mean_absolute_error: 0.2034 - val_loss: 0.0974 - val_mean_absolute_error: 0.2374\n",
            "Epoch 74/1000\n",
            "2688/2688 [==============================] - 3s 988us/step - loss: 0.0683 - mean_absolute_error: 0.2023 - val_loss: 0.0962 - val_mean_absolute_error: 0.2354\n",
            "Epoch 75/1000\n",
            "2688/2688 [==============================] - 3s 994us/step - loss: 0.0689 - mean_absolute_error: 0.2033 - val_loss: 0.0989 - val_mean_absolute_error: 0.2383\n",
            "Epoch 76/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0664 - mean_absolute_error: 0.1993 - val_loss: 0.0964 - val_mean_absolute_error: 0.2349\n",
            "Epoch 77/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0675 - mean_absolute_error: 0.2009 - val_loss: 0.0975 - val_mean_absolute_error: 0.2366\n",
            "Epoch 78/1000\n",
            "2688/2688 [==============================] - 3s 993us/step - loss: 0.0666 - mean_absolute_error: 0.1999 - val_loss: 0.0971 - val_mean_absolute_error: 0.2361\n",
            "Epoch 79/1000\n",
            "2688/2688 [==============================] - 3s 998us/step - loss: 0.0656 - mean_absolute_error: 0.1982 - val_loss: 0.0976 - val_mean_absolute_error: 0.2366\n",
            "Epoch 80/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0672 - mean_absolute_error: 0.2005 - val_loss: 0.0985 - val_mean_absolute_error: 0.2369\n",
            "Epoch 81/1000\n",
            "2688/2688 [==============================] - 3s 995us/step - loss: 0.0657 - mean_absolute_error: 0.1983 - val_loss: 0.0941 - val_mean_absolute_error: 0.2318\n",
            "Epoch 82/1000\n",
            "2688/2688 [==============================] - 3s 994us/step - loss: 0.0646 - mean_absolute_error: 0.1965 - val_loss: 0.0977 - val_mean_absolute_error: 0.2353\n",
            "Epoch 83/1000\n",
            "2688/2688 [==============================] - 3s 987us/step - loss: 0.0637 - mean_absolute_error: 0.1951 - val_loss: 0.0931 - val_mean_absolute_error: 0.2313\n",
            "Epoch 84/1000\n",
            "2688/2688 [==============================] - 3s 993us/step - loss: 0.0621 - mean_absolute_error: 0.1926 - val_loss: 0.0941 - val_mean_absolute_error: 0.2320\n",
            "Epoch 85/1000\n",
            "2688/2688 [==============================] - 3s 995us/step - loss: 0.0621 - mean_absolute_error: 0.1928 - val_loss: 0.0949 - val_mean_absolute_error: 0.2326\n",
            "Epoch 86/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0603 - mean_absolute_error: 0.1899 - val_loss: 0.0973 - val_mean_absolute_error: 0.2359\n",
            "Epoch 87/1000\n",
            "2688/2688 [==============================] - 3s 999us/step - loss: 0.0610 - mean_absolute_error: 0.1911 - val_loss: 0.0957 - val_mean_absolute_error: 0.2329\n",
            "Epoch 88/1000\n",
            "2688/2688 [==============================] - 3s 988us/step - loss: 0.0604 - mean_absolute_error: 0.1900 - val_loss: 0.0930 - val_mean_absolute_error: 0.2300\n",
            "Epoch 89/1000\n",
            "2688/2688 [==============================] - 3s 973us/step - loss: 0.0594 - mean_absolute_error: 0.1885 - val_loss: 0.0933 - val_mean_absolute_error: 0.2309\n",
            "Epoch 90/1000\n",
            "2688/2688 [==============================] - 3s 970us/step - loss: 0.0604 - mean_absolute_error: 0.1900 - val_loss: 0.0952 - val_mean_absolute_error: 0.2343\n",
            "Epoch 91/1000\n",
            "2688/2688 [==============================] - 3s 969us/step - loss: 0.0607 - mean_absolute_error: 0.1909 - val_loss: 0.0929 - val_mean_absolute_error: 0.2296\n",
            "Epoch 92/1000\n",
            "2688/2688 [==============================] - 3s 960us/step - loss: 0.0594 - mean_absolute_error: 0.1885 - val_loss: 0.0966 - val_mean_absolute_error: 0.2346\n",
            "Epoch 93/1000\n",
            "2688/2688 [==============================] - 3s 960us/step - loss: 0.0582 - mean_absolute_error: 0.1864 - val_loss: 0.0957 - val_mean_absolute_error: 0.2323\n",
            "Epoch 94/1000\n",
            "2688/2688 [==============================] - 3s 986us/step - loss: 0.0591 - mean_absolute_error: 0.1880 - val_loss: 0.0943 - val_mean_absolute_error: 0.2304\n",
            "Epoch 95/1000\n",
            "2688/2688 [==============================] - 3s 959us/step - loss: 0.0572 - mean_absolute_error: 0.1849 - val_loss: 0.0906 - val_mean_absolute_error: 0.2267\n",
            "Epoch 96/1000\n",
            "2688/2688 [==============================] - 3s 957us/step - loss: 0.0591 - mean_absolute_error: 0.1879 - val_loss: 0.0940 - val_mean_absolute_error: 0.2310\n",
            "Epoch 97/1000\n",
            "2688/2688 [==============================] - 3s 950us/step - loss: 0.0567 - mean_absolute_error: 0.1841 - val_loss: 0.0905 - val_mean_absolute_error: 0.2260\n",
            "Epoch 98/1000\n",
            "2688/2688 [==============================] - 3s 951us/step - loss: 0.0543 - mean_absolute_error: 0.1802 - val_loss: 0.0897 - val_mean_absolute_error: 0.2256\n",
            "Epoch 99/1000\n",
            "2688/2688 [==============================] - 3s 947us/step - loss: 0.0540 - mean_absolute_error: 0.1797 - val_loss: 0.0913 - val_mean_absolute_error: 0.2265\n",
            "Epoch 100/1000\n",
            "2688/2688 [==============================] - 3s 968us/step - loss: 0.0545 - mean_absolute_error: 0.1805 - val_loss: 0.0900 - val_mean_absolute_error: 0.2251\n",
            "Epoch 101/1000\n",
            "2688/2688 [==============================] - 3s 938us/step - loss: 0.0541 - mean_absolute_error: 0.1798 - val_loss: 0.0907 - val_mean_absolute_error: 0.2261\n",
            "Epoch 102/1000\n",
            "2688/2688 [==============================] - 2s 914us/step - loss: 0.0551 - mean_absolute_error: 0.1813 - val_loss: 0.0936 - val_mean_absolute_error: 0.2298\n",
            "Epoch 103/1000\n",
            "2688/2688 [==============================] - 2s 913us/step - loss: 0.0550 - mean_absolute_error: 0.1811 - val_loss: 0.0918 - val_mean_absolute_error: 0.2284\n",
            "Epoch 104/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0534 - mean_absolute_error: 0.1785 - val_loss: 0.0884 - val_mean_absolute_error: 0.2238\n",
            "Epoch 105/1000\n",
            "2688/2688 [==============================] - 2s 917us/step - loss: 0.0528 - mean_absolute_error: 0.1775 - val_loss: 0.0917 - val_mean_absolute_error: 0.2275\n",
            "Epoch 106/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.0527 - mean_absolute_error: 0.1773 - val_loss: 0.0902 - val_mean_absolute_error: 0.2270\n",
            "Epoch 107/1000\n",
            "2688/2688 [==============================] - 2s 924us/step - loss: 0.0535 - mean_absolute_error: 0.1787 - val_loss: 0.0893 - val_mean_absolute_error: 0.2244\n",
            "Epoch 108/1000\n",
            "2688/2688 [==============================] - 3s 947us/step - loss: 0.0534 - mean_absolute_error: 0.1785 - val_loss: 0.0912 - val_mean_absolute_error: 0.2266\n",
            "Epoch 109/1000\n",
            "2688/2688 [==============================] - 2s 906us/step - loss: 0.0529 - mean_absolute_error: 0.1777 - val_loss: 0.0885 - val_mean_absolute_error: 0.2230\n",
            "Epoch 110/1000\n",
            "2688/2688 [==============================] - 2s 900us/step - loss: 0.0519 - mean_absolute_error: 0.1758 - val_loss: 0.0905 - val_mean_absolute_error: 0.2251\n",
            "Epoch 111/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.0506 - mean_absolute_error: 0.1737 - val_loss: 0.0895 - val_mean_absolute_error: 0.2254\n",
            "Epoch 112/1000\n",
            "2688/2688 [==============================] - 2s 923us/step - loss: 0.0503 - mean_absolute_error: 0.1731 - val_loss: 0.0896 - val_mean_absolute_error: 0.2246\n",
            "Epoch 113/1000\n",
            "2688/2688 [==============================] - 3s 938us/step - loss: 0.0511 - mean_absolute_error: 0.1749 - val_loss: 0.0900 - val_mean_absolute_error: 0.2246\n",
            "Epoch 114/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0498 - mean_absolute_error: 0.1722 - val_loss: 0.0890 - val_mean_absolute_error: 0.2235\n",
            "Epoch 115/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.0502 - mean_absolute_error: 0.1731 - val_loss: 0.0875 - val_mean_absolute_error: 0.2219\n",
            "Epoch 116/1000\n",
            "2688/2688 [==============================] - 3s 941us/step - loss: 0.0497 - mean_absolute_error: 0.1722 - val_loss: 0.0880 - val_mean_absolute_error: 0.2231\n",
            "Epoch 117/1000\n",
            "2688/2688 [==============================] - 3s 971us/step - loss: 0.0498 - mean_absolute_error: 0.1722 - val_loss: 0.0899 - val_mean_absolute_error: 0.2240\n",
            "Epoch 118/1000\n",
            "2688/2688 [==============================] - 3s 991us/step - loss: 0.0495 - mean_absolute_error: 0.1719 - val_loss: 0.0878 - val_mean_absolute_error: 0.2224\n",
            "Epoch 119/1000\n",
            "2688/2688 [==============================] - 3s 984us/step - loss: 0.0491 - mean_absolute_error: 0.1711 - val_loss: 0.0875 - val_mean_absolute_error: 0.2216\n",
            "Epoch 120/1000\n",
            "2688/2688 [==============================] - 3s 957us/step - loss: 0.0490 - mean_absolute_error: 0.1709 - val_loss: 0.0873 - val_mean_absolute_error: 0.2212\n",
            "Epoch 121/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0481 - mean_absolute_error: 0.1693 - val_loss: 0.0858 - val_mean_absolute_error: 0.2198\n",
            "Epoch 122/1000\n",
            "2688/2688 [==============================] - 3s 955us/step - loss: 0.0471 - mean_absolute_error: 0.1675 - val_loss: 0.0881 - val_mean_absolute_error: 0.2220\n",
            "Epoch 123/1000\n",
            "2688/2688 [==============================] - 2s 921us/step - loss: 0.0464 - mean_absolute_error: 0.1662 - val_loss: 0.0859 - val_mean_absolute_error: 0.2193\n",
            "Epoch 124/1000\n",
            "2688/2688 [==============================] - 2s 927us/step - loss: 0.0459 - mean_absolute_error: 0.1652 - val_loss: 0.0894 - val_mean_absolute_error: 0.2237\n",
            "Epoch 125/1000\n",
            "2688/2688 [==============================] - 3s 941us/step - loss: 0.0473 - mean_absolute_error: 0.1683 - val_loss: 0.0853 - val_mean_absolute_error: 0.2184\n",
            "Epoch 126/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0471 - mean_absolute_error: 0.1674 - val_loss: 0.0864 - val_mean_absolute_error: 0.2205\n",
            "Epoch 127/1000\n",
            "2688/2688 [==============================] - 2s 920us/step - loss: 0.0468 - mean_absolute_error: 0.1671 - val_loss: 0.0845 - val_mean_absolute_error: 0.2173\n",
            "Epoch 128/1000\n",
            "2688/2688 [==============================] - 2s 895us/step - loss: 0.0451 - mean_absolute_error: 0.1639 - val_loss: 0.0856 - val_mean_absolute_error: 0.2184\n",
            "Epoch 129/1000\n",
            "2688/2688 [==============================] - 2s 909us/step - loss: 0.0445 - mean_absolute_error: 0.1628 - val_loss: 0.0853 - val_mean_absolute_error: 0.2180\n",
            "Epoch 130/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.0449 - mean_absolute_error: 0.1635 - val_loss: 0.0863 - val_mean_absolute_error: 0.2185\n",
            "Epoch 131/1000\n",
            "2688/2688 [==============================] - 2s 928us/step - loss: 0.0455 - mean_absolute_error: 0.1646 - val_loss: 0.0884 - val_mean_absolute_error: 0.2221\n",
            "Epoch 132/1000\n",
            "2688/2688 [==============================] - 2s 919us/step - loss: 0.0446 - mean_absolute_error: 0.1630 - val_loss: 0.0865 - val_mean_absolute_error: 0.2212\n",
            "Epoch 133/1000\n",
            "2688/2688 [==============================] - 3s 933us/step - loss: 0.0458 - mean_absolute_error: 0.1653 - val_loss: 0.0875 - val_mean_absolute_error: 0.2213\n",
            "Epoch 134/1000\n",
            "2688/2688 [==============================] - 3s 945us/step - loss: 0.0449 - mean_absolute_error: 0.1636 - val_loss: 0.0889 - val_mean_absolute_error: 0.2256\n",
            "Epoch 135/1000\n",
            "2688/2688 [==============================] - 3s 975us/step - loss: 0.0464 - mean_absolute_error: 0.1664 - val_loss: 0.0884 - val_mean_absolute_error: 0.2220\n",
            "Epoch 136/1000\n",
            "2688/2688 [==============================] - 3s 972us/step - loss: 0.0461 - mean_absolute_error: 0.1655 - val_loss: 0.0846 - val_mean_absolute_error: 0.2171\n",
            "Epoch 137/1000\n",
            "2688/2688 [==============================] - 3s 996us/step - loss: 0.0435 - mean_absolute_error: 0.1609 - val_loss: 0.0881 - val_mean_absolute_error: 0.2202\n",
            "Epoch 138/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0443 - mean_absolute_error: 0.1624 - val_loss: 0.0837 - val_mean_absolute_error: 0.2161\n",
            "Epoch 139/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0441 - mean_absolute_error: 0.1620 - val_loss: 0.0842 - val_mean_absolute_error: 0.2162\n",
            "Epoch 140/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0437 - mean_absolute_error: 0.1612 - val_loss: 0.0835 - val_mean_absolute_error: 0.2161\n",
            "Epoch 141/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0429 - mean_absolute_error: 0.1597 - val_loss: 0.0843 - val_mean_absolute_error: 0.2162\n",
            "Epoch 142/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0422 - mean_absolute_error: 0.1584 - val_loss: 0.0852 - val_mean_absolute_error: 0.2178\n",
            "Epoch 143/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0424 - mean_absolute_error: 0.1589 - val_loss: 0.0852 - val_mean_absolute_error: 0.2174\n",
            "Epoch 144/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0438 - mean_absolute_error: 0.1615 - val_loss: 0.0844 - val_mean_absolute_error: 0.2166\n",
            "Epoch 145/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0436 - mean_absolute_error: 0.1609 - val_loss: 0.0854 - val_mean_absolute_error: 0.2170\n",
            "Epoch 146/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0419 - mean_absolute_error: 0.1577 - val_loss: 0.0844 - val_mean_absolute_error: 0.2171\n",
            "Epoch 147/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0428 - mean_absolute_error: 0.1594 - val_loss: 0.0834 - val_mean_absolute_error: 0.2149\n",
            "Epoch 148/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0430 - mean_absolute_error: 0.1601 - val_loss: 0.0860 - val_mean_absolute_error: 0.2198\n",
            "Epoch 149/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0429 - mean_absolute_error: 0.1598 - val_loss: 0.0829 - val_mean_absolute_error: 0.2141\n",
            "Epoch 150/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0411 - mean_absolute_error: 0.1562 - val_loss: 0.0820 - val_mean_absolute_error: 0.2132\n",
            "Epoch 151/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0410 - mean_absolute_error: 0.1560 - val_loss: 0.0821 - val_mean_absolute_error: 0.2134\n",
            "Epoch 152/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0407 - mean_absolute_error: 0.1555 - val_loss: 0.0828 - val_mean_absolute_error: 0.2143\n",
            "Epoch 153/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0420 - mean_absolute_error: 0.1580 - val_loss: 0.0812 - val_mean_absolute_error: 0.2122\n",
            "Epoch 154/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0408 - mean_absolute_error: 0.1557 - val_loss: 0.0826 - val_mean_absolute_error: 0.2138\n",
            "Epoch 155/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0401 - mean_absolute_error: 0.1544 - val_loss: 0.0817 - val_mean_absolute_error: 0.2122\n",
            "Epoch 156/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0398 - mean_absolute_error: 0.1539 - val_loss: 0.0832 - val_mean_absolute_error: 0.2142\n",
            "Epoch 157/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0402 - mean_absolute_error: 0.1546 - val_loss: 0.0825 - val_mean_absolute_error: 0.2146\n",
            "Epoch 158/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0406 - mean_absolute_error: 0.1553 - val_loss: 0.0814 - val_mean_absolute_error: 0.2124\n",
            "Epoch 159/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0401 - mean_absolute_error: 0.1544 - val_loss: 0.0828 - val_mean_absolute_error: 0.2140\n",
            "Epoch 160/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0389 - mean_absolute_error: 0.1521 - val_loss: 0.0820 - val_mean_absolute_error: 0.2132\n",
            "Epoch 161/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0385 - mean_absolute_error: 0.1513 - val_loss: 0.0814 - val_mean_absolute_error: 0.2124\n",
            "Epoch 162/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0400 - mean_absolute_error: 0.1545 - val_loss: 0.0803 - val_mean_absolute_error: 0.2108\n",
            "Epoch 163/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0387 - mean_absolute_error: 0.1516 - val_loss: 0.0818 - val_mean_absolute_error: 0.2130\n",
            "Epoch 164/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0401 - mean_absolute_error: 0.1544 - val_loss: 0.0845 - val_mean_absolute_error: 0.2154\n",
            "Epoch 165/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0406 - mean_absolute_error: 0.1552 - val_loss: 0.0838 - val_mean_absolute_error: 0.2147\n",
            "Epoch 166/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0394 - mean_absolute_error: 0.1529 - val_loss: 0.0830 - val_mean_absolute_error: 0.2134\n",
            "Epoch 167/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0400 - mean_absolute_error: 0.1539 - val_loss: 0.0820 - val_mean_absolute_error: 0.2124\n",
            "Epoch 168/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0389 - mean_absolute_error: 0.1518 - val_loss: 0.0804 - val_mean_absolute_error: 0.2104\n",
            "Epoch 169/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0381 - mean_absolute_error: 0.1504 - val_loss: 0.0797 - val_mean_absolute_error: 0.2103\n",
            "Epoch 170/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0377 - mean_absolute_error: 0.1497 - val_loss: 0.0814 - val_mean_absolute_error: 0.2114\n",
            "Epoch 171/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0376 - mean_absolute_error: 0.1493 - val_loss: 0.0803 - val_mean_absolute_error: 0.2109\n",
            "Epoch 172/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0392 - mean_absolute_error: 0.1525 - val_loss: 0.0817 - val_mean_absolute_error: 0.2120\n",
            "Epoch 173/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0386 - mean_absolute_error: 0.1516 - val_loss: 0.0833 - val_mean_absolute_error: 0.2150\n",
            "Epoch 174/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0376 - mean_absolute_error: 0.1494 - val_loss: 0.0816 - val_mean_absolute_error: 0.2124\n",
            "Epoch 175/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0368 - mean_absolute_error: 0.1478 - val_loss: 0.0810 - val_mean_absolute_error: 0.2111\n",
            "Epoch 176/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0366 - mean_absolute_error: 0.1475 - val_loss: 0.0804 - val_mean_absolute_error: 0.2112\n",
            "Epoch 177/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0365 - mean_absolute_error: 0.1474 - val_loss: 0.0794 - val_mean_absolute_error: 0.2091\n",
            "Epoch 178/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0361 - mean_absolute_error: 0.1464 - val_loss: 0.0811 - val_mean_absolute_error: 0.2113\n",
            "Epoch 179/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0383 - mean_absolute_error: 0.1511 - val_loss: 0.0816 - val_mean_absolute_error: 0.2118\n",
            "Epoch 180/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0375 - mean_absolute_error: 0.1492 - val_loss: 0.0804 - val_mean_absolute_error: 0.2100\n",
            "Epoch 181/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0366 - mean_absolute_error: 0.1473 - val_loss: 0.0813 - val_mean_absolute_error: 0.2112\n",
            "Epoch 182/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0375 - mean_absolute_error: 0.1492 - val_loss: 0.0838 - val_mean_absolute_error: 0.2166\n",
            "Epoch 183/1000\n",
            "2688/2688 [==============================] - 3s 950us/step - loss: 0.0379 - mean_absolute_error: 0.1499 - val_loss: 0.0819 - val_mean_absolute_error: 0.2115\n",
            "Epoch 184/1000\n",
            "2688/2688 [==============================] - 3s 936us/step - loss: 0.0369 - mean_absolute_error: 0.1479 - val_loss: 0.0793 - val_mean_absolute_error: 0.2089\n",
            "Epoch 185/1000\n",
            "2688/2688 [==============================] - 2s 919us/step - loss: 0.0365 - mean_absolute_error: 0.1473 - val_loss: 0.0812 - val_mean_absolute_error: 0.2115\n",
            "Epoch 186/1000\n",
            "2688/2688 [==============================] - 2s 926us/step - loss: 0.0356 - mean_absolute_error: 0.1452 - val_loss: 0.0794 - val_mean_absolute_error: 0.2083\n",
            "Epoch 187/1000\n",
            "2688/2688 [==============================] - 2s 911us/step - loss: 0.0371 - mean_absolute_error: 0.1483 - val_loss: 0.0812 - val_mean_absolute_error: 0.2115\n",
            "Epoch 188/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.0367 - mean_absolute_error: 0.1474 - val_loss: 0.0793 - val_mean_absolute_error: 0.2088\n",
            "Epoch 189/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0376 - mean_absolute_error: 0.1495 - val_loss: 0.0811 - val_mean_absolute_error: 0.2111\n",
            "Epoch 190/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0362 - mean_absolute_error: 0.1465 - val_loss: 0.0802 - val_mean_absolute_error: 0.2106\n",
            "Epoch 191/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0357 - mean_absolute_error: 0.1454 - val_loss: 0.0803 - val_mean_absolute_error: 0.2106\n",
            "Epoch 192/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0347 - mean_absolute_error: 0.1434 - val_loss: 0.0812 - val_mean_absolute_error: 0.2102\n",
            "Epoch 193/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0338 - mean_absolute_error: 0.1416 - val_loss: 0.0793 - val_mean_absolute_error: 0.2086\n",
            "Epoch 194/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0339 - mean_absolute_error: 0.1416 - val_loss: 0.0804 - val_mean_absolute_error: 0.2095\n",
            "Epoch 195/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0342 - mean_absolute_error: 0.1425 - val_loss: 0.0791 - val_mean_absolute_error: 0.2082\n",
            "Epoch 196/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0348 - mean_absolute_error: 0.1436 - val_loss: 0.0808 - val_mean_absolute_error: 0.2102\n",
            "Epoch 197/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0360 - mean_absolute_error: 0.1460 - val_loss: 0.0811 - val_mean_absolute_error: 0.2116\n",
            "Epoch 198/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0364 - mean_absolute_error: 0.1470 - val_loss: 0.0803 - val_mean_absolute_error: 0.2094\n",
            "Epoch 199/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0358 - mean_absolute_error: 0.1458 - val_loss: 0.0826 - val_mean_absolute_error: 0.2119\n",
            "Epoch 200/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0351 - mean_absolute_error: 0.1442 - val_loss: 0.0836 - val_mean_absolute_error: 0.2134\n",
            "Epoch 201/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0358 - mean_absolute_error: 0.1459 - val_loss: 0.0785 - val_mean_absolute_error: 0.2080\n",
            "Epoch 202/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0349 - mean_absolute_error: 0.1438 - val_loss: 0.0797 - val_mean_absolute_error: 0.2092\n",
            "Epoch 203/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0337 - mean_absolute_error: 0.1413 - val_loss: 0.0786 - val_mean_absolute_error: 0.2076\n",
            "Epoch 204/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0338 - mean_absolute_error: 0.1415 - val_loss: 0.0791 - val_mean_absolute_error: 0.2081\n",
            "Epoch 205/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0329 - mean_absolute_error: 0.1397 - val_loss: 0.0783 - val_mean_absolute_error: 0.2067\n",
            "Epoch 206/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0357 - mean_absolute_error: 0.1459 - val_loss: 0.0843 - val_mean_absolute_error: 0.2157\n",
            "Epoch 207/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0348 - mean_absolute_error: 0.1438 - val_loss: 0.0794 - val_mean_absolute_error: 0.2084\n",
            "Epoch 208/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0334 - mean_absolute_error: 0.1406 - val_loss: 0.0792 - val_mean_absolute_error: 0.2083\n",
            "Epoch 209/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0344 - mean_absolute_error: 0.1427 - val_loss: 0.0804 - val_mean_absolute_error: 0.2102\n",
            "Epoch 210/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0348 - mean_absolute_error: 0.1434 - val_loss: 0.0786 - val_mean_absolute_error: 0.2067\n",
            "Epoch 211/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0337 - mean_absolute_error: 0.1412 - val_loss: 0.0801 - val_mean_absolute_error: 0.2081\n",
            "Epoch 212/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0336 - mean_absolute_error: 0.1411 - val_loss: 0.0780 - val_mean_absolute_error: 0.2062\n",
            "Epoch 213/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0335 - mean_absolute_error: 0.1407 - val_loss: 0.0778 - val_mean_absolute_error: 0.2057\n",
            "Epoch 214/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0329 - mean_absolute_error: 0.1395 - val_loss: 0.0781 - val_mean_absolute_error: 0.2065\n",
            "Epoch 215/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0329 - mean_absolute_error: 0.1395 - val_loss: 0.0779 - val_mean_absolute_error: 0.2057\n",
            "Epoch 216/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0321 - mean_absolute_error: 0.1379 - val_loss: 0.0780 - val_mean_absolute_error: 0.2063\n",
            "Epoch 217/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0324 - mean_absolute_error: 0.1385 - val_loss: 0.0791 - val_mean_absolute_error: 0.2069\n",
            "Epoch 218/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0336 - mean_absolute_error: 0.1412 - val_loss: 0.0804 - val_mean_absolute_error: 0.2091\n",
            "Epoch 219/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0342 - mean_absolute_error: 0.1423 - val_loss: 0.0782 - val_mean_absolute_error: 0.2066\n",
            "Epoch 220/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0337 - mean_absolute_error: 0.1413 - val_loss: 0.0792 - val_mean_absolute_error: 0.2081\n",
            "Epoch 221/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0345 - mean_absolute_error: 0.1429 - val_loss: 0.0782 - val_mean_absolute_error: 0.2058\n",
            "Epoch 222/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0331 - mean_absolute_error: 0.1400 - val_loss: 0.0783 - val_mean_absolute_error: 0.2071\n",
            "Epoch 223/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0330 - mean_absolute_error: 0.1397 - val_loss: 0.0774 - val_mean_absolute_error: 0.2048\n",
            "Epoch 224/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0333 - mean_absolute_error: 0.1407 - val_loss: 0.0788 - val_mean_absolute_error: 0.2071\n",
            "Epoch 225/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0316 - mean_absolute_error: 0.1367 - val_loss: 0.0780 - val_mean_absolute_error: 0.2058\n",
            "Epoch 226/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0313 - mean_absolute_error: 0.1362 - val_loss: 0.0783 - val_mean_absolute_error: 0.2062\n",
            "Epoch 227/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0314 - mean_absolute_error: 0.1364 - val_loss: 0.0798 - val_mean_absolute_error: 0.2091\n",
            "Epoch 228/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0316 - mean_absolute_error: 0.1368 - val_loss: 0.0781 - val_mean_absolute_error: 0.2058\n",
            "Epoch 229/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0313 - mean_absolute_error: 0.1361 - val_loss: 0.0820 - val_mean_absolute_error: 0.2133\n",
            "Epoch 230/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0327 - mean_absolute_error: 0.1389 - val_loss: 0.0785 - val_mean_absolute_error: 0.2063\n",
            "Epoch 231/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0339 - mean_absolute_error: 0.1417 - val_loss: 0.0781 - val_mean_absolute_error: 0.2058\n",
            "Epoch 232/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0324 - mean_absolute_error: 0.1384 - val_loss: 0.0790 - val_mean_absolute_error: 0.2086\n",
            "Epoch 233/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0330 - mean_absolute_error: 0.1399 - val_loss: 0.0785 - val_mean_absolute_error: 0.2059\n",
            "Epoch 234/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0310 - mean_absolute_error: 0.1353 - val_loss: 0.0777 - val_mean_absolute_error: 0.2054\n",
            "Epoch 235/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0309 - mean_absolute_error: 0.1351 - val_loss: 0.0765 - val_mean_absolute_error: 0.2036\n",
            "Epoch 236/1000\n",
            "2688/2688 [==============================] - 3s 940us/step - loss: 0.0307 - mean_absolute_error: 0.1347 - val_loss: 0.0795 - val_mean_absolute_error: 0.2078\n",
            "Epoch 237/1000\n",
            "2688/2688 [==============================] - 3s 945us/step - loss: 0.0313 - mean_absolute_error: 0.1362 - val_loss: 0.0787 - val_mean_absolute_error: 0.2061\n",
            "Epoch 238/1000\n",
            "2688/2688 [==============================] - 2s 929us/step - loss: 0.0314 - mean_absolute_error: 0.1363 - val_loss: 0.0787 - val_mean_absolute_error: 0.2058\n",
            "Epoch 239/1000\n",
            "2688/2688 [==============================] - 3s 933us/step - loss: 0.0313 - mean_absolute_error: 0.1360 - val_loss: 0.0772 - val_mean_absolute_error: 0.2042\n",
            "Epoch 240/1000\n",
            "2688/2688 [==============================] - 2s 925us/step - loss: 0.0314 - mean_absolute_error: 0.1363 - val_loss: 0.0790 - val_mean_absolute_error: 0.2070\n",
            "Epoch 241/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.0314 - mean_absolute_error: 0.1363 - val_loss: 0.0779 - val_mean_absolute_error: 0.2055\n",
            "Epoch 242/1000\n",
            "2688/2688 [==============================] - 2s 894us/step - loss: 0.0313 - mean_absolute_error: 0.1362 - val_loss: 0.0792 - val_mean_absolute_error: 0.2075\n",
            "Epoch 243/1000\n",
            "2688/2688 [==============================] - 3s 977us/step - loss: 0.0307 - mean_absolute_error: 0.1348 - val_loss: 0.0765 - val_mean_absolute_error: 0.2035\n",
            "Epoch 244/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0308 - mean_absolute_error: 0.1351 - val_loss: 0.0790 - val_mean_absolute_error: 0.2088\n",
            "Epoch 245/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0307 - mean_absolute_error: 0.1348 - val_loss: 0.0785 - val_mean_absolute_error: 0.2066\n",
            "Epoch 246/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0298 - mean_absolute_error: 0.1326 - val_loss: 0.0765 - val_mean_absolute_error: 0.2039\n",
            "Epoch 247/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0311 - mean_absolute_error: 0.1358 - val_loss: 0.0780 - val_mean_absolute_error: 0.2053\n",
            "Epoch 248/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0312 - mean_absolute_error: 0.1359 - val_loss: 0.0762 - val_mean_absolute_error: 0.2030\n",
            "Epoch 249/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0307 - mean_absolute_error: 0.1347 - val_loss: 0.0788 - val_mean_absolute_error: 0.2068\n",
            "Epoch 250/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0304 - mean_absolute_error: 0.1342 - val_loss: 0.0772 - val_mean_absolute_error: 0.2040\n",
            "Epoch 251/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0307 - mean_absolute_error: 0.1348 - val_loss: 0.0778 - val_mean_absolute_error: 0.2048\n",
            "Epoch 252/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0317 - mean_absolute_error: 0.1368 - val_loss: 0.0768 - val_mean_absolute_error: 0.2031\n",
            "Epoch 253/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0310 - mean_absolute_error: 0.1353 - val_loss: 0.0764 - val_mean_absolute_error: 0.2027\n",
            "Epoch 254/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0304 - mean_absolute_error: 0.1342 - val_loss: 0.0778 - val_mean_absolute_error: 0.2048\n",
            "Epoch 255/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0312 - mean_absolute_error: 0.1357 - val_loss: 0.0784 - val_mean_absolute_error: 0.2071\n",
            "Epoch 256/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0299 - mean_absolute_error: 0.1328 - val_loss: 0.0765 - val_mean_absolute_error: 0.2033\n",
            "Epoch 257/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0304 - mean_absolute_error: 0.1338 - val_loss: 0.0770 - val_mean_absolute_error: 0.2036\n",
            "Epoch 258/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0295 - mean_absolute_error: 0.1320 - val_loss: 0.0765 - val_mean_absolute_error: 0.2033\n",
            "Epoch 259/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0294 - mean_absolute_error: 0.1317 - val_loss: 0.0763 - val_mean_absolute_error: 0.2025\n",
            "Epoch 260/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0294 - mean_absolute_error: 0.1317 - val_loss: 0.0772 - val_mean_absolute_error: 0.2040\n",
            "Epoch 261/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0311 - mean_absolute_error: 0.1356 - val_loss: 0.0770 - val_mean_absolute_error: 0.2039\n",
            "Epoch 262/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0310 - mean_absolute_error: 0.1355 - val_loss: 0.0774 - val_mean_absolute_error: 0.2041\n",
            "Epoch 263/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0304 - mean_absolute_error: 0.1341 - val_loss: 0.0761 - val_mean_absolute_error: 0.2024\n",
            "\n",
            "Epoch 00263: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 264/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0267 - mean_absolute_error: 0.1255 - val_loss: 0.0751 - val_mean_absolute_error: 0.2009\n",
            "Epoch 265/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0253 - mean_absolute_error: 0.1219 - val_loss: 0.0749 - val_mean_absolute_error: 0.2007\n",
            "Epoch 266/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0248 - mean_absolute_error: 0.1208 - val_loss: 0.0750 - val_mean_absolute_error: 0.2007\n",
            "Epoch 267/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0246 - mean_absolute_error: 0.1201 - val_loss: 0.0750 - val_mean_absolute_error: 0.2012\n",
            "Epoch 268/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0244 - mean_absolute_error: 0.1197 - val_loss: 0.0753 - val_mean_absolute_error: 0.2014\n",
            "Epoch 269/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0242 - mean_absolute_error: 0.1193 - val_loss: 0.0759 - val_mean_absolute_error: 0.2020\n",
            "Epoch 270/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0241 - mean_absolute_error: 0.1190 - val_loss: 0.0759 - val_mean_absolute_error: 0.2023\n",
            "Epoch 271/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0241 - mean_absolute_error: 0.1189 - val_loss: 0.0761 - val_mean_absolute_error: 0.2025\n",
            "Epoch 272/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0241 - mean_absolute_error: 0.1190 - val_loss: 0.0762 - val_mean_absolute_error: 0.2022\n",
            "Epoch 273/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0242 - mean_absolute_error: 0.1192 - val_loss: 0.0757 - val_mean_absolute_error: 0.2018\n",
            "Epoch 274/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0242 - mean_absolute_error: 0.1192 - val_loss: 0.0767 - val_mean_absolute_error: 0.2028\n",
            "Epoch 275/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0242 - mean_absolute_error: 0.1193 - val_loss: 0.0765 - val_mean_absolute_error: 0.2026\n",
            "Epoch 276/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0241 - mean_absolute_error: 0.1190 - val_loss: 0.0765 - val_mean_absolute_error: 0.2025\n",
            "Epoch 277/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0241 - mean_absolute_error: 0.1190 - val_loss: 0.0775 - val_mean_absolute_error: 0.2039\n",
            "Epoch 278/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0242 - mean_absolute_error: 0.1193 - val_loss: 0.0766 - val_mean_absolute_error: 0.2030\n",
            "Epoch 279/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0242 - mean_absolute_error: 0.1192 - val_loss: 0.0768 - val_mean_absolute_error: 0.2032\n",
            "Epoch 280/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0244 - mean_absolute_error: 0.1197 - val_loss: 0.0774 - val_mean_absolute_error: 0.2040\n",
            "\n",
            "Epoch 00280: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 281/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0234 - mean_absolute_error: 0.1172 - val_loss: 0.0766 - val_mean_absolute_error: 0.2028\n",
            "Epoch 282/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0230 - mean_absolute_error: 0.1161 - val_loss: 0.0768 - val_mean_absolute_error: 0.2030\n",
            "Epoch 283/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0229 - mean_absolute_error: 0.1157 - val_loss: 0.0770 - val_mean_absolute_error: 0.2031\n",
            "Epoch 284/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0228 - mean_absolute_error: 0.1155 - val_loss: 0.0772 - val_mean_absolute_error: 0.2036\n",
            "Epoch 285/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0227 - mean_absolute_error: 0.1154 - val_loss: 0.0773 - val_mean_absolute_error: 0.2036\n",
            "Epoch 286/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0227 - mean_absolute_error: 0.1154 - val_loss: 0.0774 - val_mean_absolute_error: 0.2036\n",
            "Epoch 287/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0227 - mean_absolute_error: 0.1152 - val_loss: 0.0774 - val_mean_absolute_error: 0.2037\n",
            "Epoch 288/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0226 - mean_absolute_error: 0.1150 - val_loss: 0.0774 - val_mean_absolute_error: 0.2037\n",
            "Epoch 289/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0226 - mean_absolute_error: 0.1151 - val_loss: 0.0778 - val_mean_absolute_error: 0.2041\n",
            "Epoch 290/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0226 - mean_absolute_error: 0.1150 - val_loss: 0.0779 - val_mean_absolute_error: 0.2044\n",
            "Epoch 291/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0226 - mean_absolute_error: 0.1150 - val_loss: 0.0781 - val_mean_absolute_error: 0.2045\n",
            "Epoch 292/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0226 - mean_absolute_error: 0.1150 - val_loss: 0.0778 - val_mean_absolute_error: 0.2045\n",
            "Epoch 293/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0226 - mean_absolute_error: 0.1150 - val_loss: 0.0781 - val_mean_absolute_error: 0.2045\n",
            "Epoch 294/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0225 - mean_absolute_error: 0.1147 - val_loss: 0.0784 - val_mean_absolute_error: 0.2049\n",
            "Epoch 295/1000\n",
            "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0225 - mean_absolute_error: 0.1149 - val_loss: 0.0783 - val_mean_absolute_error: 0.2047\n",
            "\n",
            "Epoch 00295: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2a669c0668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "5CWRCpqll4ni",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(val_x,val_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ep90s0lsl4nn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "with open('train_data.pickle', 'wb') as f:\n",
        "    pickle.dump(train_data, f)"
      ]
    },
    {
      "metadata": {
        "id": "FEVS2wr1l4np",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "with open('train_data.pickle', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "\n",
        "print (train_data.shape)    \n"
      ]
    },
    {
      "metadata": {
        "id": "e1ilYkSil4nq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def shuffle_list(s,l):\n",
        "    final_list = []\n",
        "    n_list = np.arange(l)\n",
        "    np.random.shuffle(n_list)\n",
        "    b_s = int(np.floor(l/s))\n",
        "    #print (b_s)\n",
        "    \n",
        "    for i in range (b_s):\n",
        "        print (i)\n",
        "        final_list.append(n_list[i*s:(i+1)*s])\n",
        "    return final_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l5cIBF3Bl4nz",
        "colab_type": "code",
        "colab": {},
        "outputId": "fc1b8af7-23c1-48a9-d061-421225e0d9ea"
      },
      "cell_type": "code",
      "source": [
        "l = len(train_x)\n",
        "s= 32\n",
        "batch_list = shuffle_list(s,l)\n",
        "for batch in bacth_list:\n",
        "    X = train_x[batch]\n",
        "    Y = train_y[batch]\n",
        "    model.train_on_batch(X,Y)\n",
        "    print (np.argmax(model.predict(X),axis=-1))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_x' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1e88f2460e8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbacth_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_mgkvCKEl4n6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}