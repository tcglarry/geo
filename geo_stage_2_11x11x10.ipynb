{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "geo_stage_2_11x11x10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcglarry/geo/blob/master/geo_stage_2_11x11x10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kUbioL1FmGXN",
        "colab_type": "code",
        "outputId": "f598c6df-c2d8-4258-b47b-a4f0a799a755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"ali_build_model_stage_2_MobileNetV2_aug_1000_dense.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1wSVEmcoWzRm-7B6UGkHB8ysfMDRT_xK5\n",
        "\"\"\"\n",
        "\n",
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8aoJd93CmGao",
        "colab_type": "code",
        "outputId": "4d84cbf3-0be6-48bd-bc7e-b1aead6e5292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cD6mcE4Zl4m1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "src=  'drive/My Drive/geo/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gUqwwxril4m5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb9ef679-68ca-4ef7-9e93-42e4d5036824"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import subprocess\n",
        "import os\n",
        "import pickle\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, merge, Lambda,UpSampling2D, concatenate, \\\n",
        "Conv2DTranspose,BatchNormalization\n",
        "from keras.models import Model, load_model\n",
        "import pandas as pd\n",
        "import sklearn \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras import regularizers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "IhIuNLS6l4nC",
        "colab_type": "code",
        "outputId": "99f32ee9-5547-4a53-8390-ffcc69993afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "data= np.load(src+'train_data.npy')\n",
        "label = np.load(src+'train_labels.npy')\n",
        "label = label[:,:,:,np.newaxis]\n",
        "print (data.shape)\n",
        "print (label.shape)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4200, 21, 21, 20)\n",
            "(4200, 20, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wtZI1t8InGyu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reduce H,W from 21 to 11, Reduce channel from 21 to 11\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VIin_TiJl4nH",
        "colab_type": "code",
        "outputId": "17556eb4-1029-46a8-b66f-0d144304b1c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "index_list_2 = list(range(20))\n",
        "select_list_2 = [idx for idx in index_list_2 if idx % 2  == 0 ]\n",
        "\n",
        "index_list_2 = list(range(21))\n",
        "select_list_0 = [idx for idx in index_list_2 if idx % 2  == 0 ]\n",
        "print (select_list_2)\n",
        "print (select_list_0)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gY3f1GFgn5gl",
        "colab_type": "code",
        "outputId": "e7a3b30b-84bc-4e64-b4f1-a9061743425e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data = data[:,:,:,select_list_2]\n",
        "\n",
        "data = data[:,select_list_0,:,:]\n",
        "\n",
        "data = data[:,:,select_list_0,:]\n",
        "data.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4200, 11, 11, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Zlc2JmA2w_RG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_mean = np.mean(data)\n",
        "data_std = np.std(data)\n",
        "label_max = np.max(label)\n",
        "label_min = np.min(label)\n",
        "\n",
        "train_data = (data - data_mean)/data_std\n",
        "#label_data = (label-label_min)/(label_max - label_min)\n",
        "label_data = np.log(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iGeIz5AWl4nL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_x,val_x,train_y,val_y = train_test_split(train_data,label_data,test_size=0.2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_F7YnLWqzBU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_block(ch, activation= 'relu', padding='same',kernel_regularizer=regularizers.l2(0.01)):       \n",
        "    return Conv2D(ch,(3,3),activation= activation, padding =padding )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C8sDm7fZl4nQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#def build_model_1(IMAGE_HEIGHT=IMAGE_HEIGHT,IMAGE_WIDTH=IMAGE_WIDTH,ch=ch):\n",
        "def build_model_1(ch=8):\n",
        "    #inputs = Input((IMAGE_HEIGHT,IMAGE_WIDTH,ch))\n",
        "    inputs = Input((11,11,10))\n",
        "    \n",
        "    up_1 = UpSampling2D(size=(2,2))(inputs)\n",
        "\n",
        "    conv0 = Conv2D(32,(3,3),padding='valid')(up_1)\n",
        "    print ('conv0',conv0.get_shape())\n",
        "    conv1 =  conv_block(ch)(conv0)\n",
        "    conv1 = conv_block(ch)(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
        "    print ('pool1',pool1.get_shape())\n",
        "    \n",
        "    conv2 =  conv_block(ch*2)(pool1)\n",
        "    conv2 = conv_block(ch*2)(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
        "    print ('poo12',pool2.get_shape())\n",
        "    \n",
        "    conv_test = Conv2D(64,(2,2),padding='valid')(pool2)\n",
        "    print ('conv_test',conv_test.get_shape())\n",
        "    \n",
        "    \n",
        "    \n",
        "    conv3 =  conv_block(ch*4)(conv_test)\n",
        "    conv3 = conv_block(ch*4)(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
        "    print ('poo13',pool3.get_shape())\n",
        "    \n",
        "    conv4 =  conv_block(ch*8)(pool3)\n",
        "    conv4 = conv_block(ch*8)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2,2))(conv4)\n",
        "    print ('poo14',pool4.get_shape())\n",
        "    \n",
        "    conv5 =  conv_block(ch*16)(pool4)\n",
        "    conv5 = conv_block(ch*16)(conv5)\n",
        "\n",
        "    \n",
        "    up6 = concatenate([UpSampling2D(size=(2,2))(conv5), conv4], axis=3)\n",
        "    conv6 = conv_block(ch*8)(up6)\n",
        "    conv6 = conv_block(ch*8)(conv6)\n",
        "    \n",
        "    up7 = concatenate ([UpSampling2D(size=(2,2))(conv6), conv3],  axis=3)\n",
        "    conv7 = conv_block(ch*4)(up7)\n",
        "    conv7 = conv_block(ch*4)(conv7)\n",
        "    print ('conv7',conv7.get_shape())\n",
        "    conv7 = Conv2DTranspose(ch*4,(2,2),padding='valid')(conv7)\n",
        "    print ('conv7',conv7.get_shape())\n",
        "    \n",
        "    up8 = concatenate([UpSampling2D(size=(2,2))(conv7), conv2], axis=3)\n",
        "    conv8 = conv_block(ch*2)(up8)\n",
        "    conv8 = conv_block(ch*2)(conv8)\n",
        "    \n",
        "    up9 = concatenate([UpSampling2D(size=(2,2))(conv8), conv1],  axis=3)\n",
        "    conv9 = conv_block(ch)(up9)\n",
        "    conv9 = conv_block(ch)(conv9)\n",
        "    \n",
        "    \n",
        "    \n",
        "    conv10 = Conv2D(1,(1,1))(conv9)\n",
        "    \n",
        "  \n",
        "    model = Model(inputs= inputs, outputs=conv10)\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WqpLnCCCoXnF",
        "colab_type": "code",
        "outputId": "0d0b765b-b569-4af7-8df6-4c43a33e3f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1634
        }
      },
      "cell_type": "code",
      "source": [
        "model = build_model_1()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv0 (?, 20, 20, 32)\n",
            "pool1 (?, 10, 10, 8)\n",
            "poo12 (?, 5, 5, 16)\n",
            "conv_test (?, 4, 4, 64)\n",
            "poo13 (?, 2, 2, 32)\n",
            "poo14 (?, 1, 1, 64)\n",
            "conv7 (?, 4, 4, 32)\n",
            "conv7 (?, ?, ?, 32)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 11, 11, 10)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 22, 22, 10)   0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 20, 20, 32)   2912        up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 20, 20, 8)    2312        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 20, 20, 8)    584         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 10, 10, 8)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 10, 10, 16)   1168        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 10, 10, 16)   2320        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 16)     0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 4, 4, 64)     4160        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 4, 4, 32)     18464       conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 4, 4, 32)     9248        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 2, 2, 32)     0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 2, 2, 64)     18496       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 2, 2, 64)     36928       conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 64)     0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 1, 1, 128)    73856       max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 1, 1, 128)    147584      conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 2, 2, 128)    0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2, 2, 192)    0           up_sampling2d_2[0][0]            \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 2, 2, 64)     110656      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 2, 2, 64)     36928       conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 4, 4, 64)     0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 4, 4, 96)     0           up_sampling2d_3[0][0]            \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 32)     27680       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 32)     9248        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 5, 5, 32)     4128        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 10, 10, 32)   0           conv2d_transpose_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 10, 10, 48)   0           up_sampling2d_4[0][0]            \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 10, 10, 16)   6928        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 10, 10, 16)   2320        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2D)  (None, 20, 20, 16)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 20, 20, 24)   0           up_sampling2d_5[0][0]            \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 20, 20, 8)    1736        concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 20, 20, 8)    584         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 20, 20, 1)    9           conv2d_20[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 518,249\n",
            "Trainable params: 518,249\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mTSiLPYtl4nU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile (optimizer='Adam', loss = 'mse', metrics =['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bQ1YM-aml4nZ",
        "colab_type": "code",
        "outputId": "f0679717-6092-4bc2-e3f4-6e1c6e3dfff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# train the model on the new data for a few epochs\n",
        "\n",
        "use_model= False\n",
        "if use_model:\n",
        "    \n",
        "\n",
        "    if os.path.isfile(src+'water_first_try.h5'):\n",
        "\n",
        "      model = load_model(src+'water_first_try.h5')\n",
        "      print ('model laoded')\n",
        "    else:\n",
        "      print ('model not exist')\n",
        "else:\n",
        "  print ('not to use model')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "not to use model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U2NKk1wFl4nc",
        "colab_type": "code",
        "outputId": "a74ea5d9-6f0a-4919-87c4-7153d0ba0caa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4567
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=src+'geo_stage2_2nd_try_11_11_10.h5', monitor = 'val_loss', save_best_only=True, mode= 'auto')\n",
        "earlystop = EarlyStopping(patience=30)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=15, min_lr=0.00001,verbose=1)\n",
        "callback_list = [checkpoint, earlystop]\n",
        "model.fit(x=train_x, y=train_y, batch_size=32, epochs=1000, callbacks = callback_list, verbose=1, validation_split=0.2,  shuffle=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2688 samples, validate on 672 samples\n",
            "Epoch 1/1000\n",
            "2688/2688 [==============================] - 5s 2ms/step - loss: 1.0767 - mean_absolute_error: 0.8176 - val_loss: 0.7279 - val_mean_absolute_error: 0.6808\n",
            "Epoch 2/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.6075 - mean_absolute_error: 0.6167 - val_loss: 0.5000 - val_mean_absolute_error: 0.5559\n",
            "Epoch 3/1000\n",
            "2688/2688 [==============================] - 2s 908us/step - loss: 0.4388 - mean_absolute_error: 0.5194 - val_loss: 0.3833 - val_mean_absolute_error: 0.4864\n",
            "Epoch 4/1000\n",
            "2688/2688 [==============================] - 2s 921us/step - loss: 0.3453 - mean_absolute_error: 0.4596 - val_loss: 0.3257 - val_mean_absolute_error: 0.4457\n",
            "Epoch 5/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.3114 - mean_absolute_error: 0.4363 - val_loss: 0.3036 - val_mean_absolute_error: 0.4306\n",
            "Epoch 6/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.2843 - mean_absolute_error: 0.4166 - val_loss: 0.2861 - val_mean_absolute_error: 0.4172\n",
            "Epoch 7/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.2676 - mean_absolute_error: 0.4040 - val_loss: 0.2675 - val_mean_absolute_error: 0.4052\n",
            "Epoch 8/1000\n",
            "2688/2688 [==============================] - 2s 907us/step - loss: 0.2526 - mean_absolute_error: 0.3924 - val_loss: 0.2512 - val_mean_absolute_error: 0.3906\n",
            "Epoch 9/1000\n",
            "2688/2688 [==============================] - 2s 907us/step - loss: 0.2408 - mean_absolute_error: 0.3828 - val_loss: 0.2492 - val_mean_absolute_error: 0.3881\n",
            "Epoch 10/1000\n",
            "2688/2688 [==============================] - 2s 911us/step - loss: 0.2332 - mean_absolute_error: 0.3766 - val_loss: 0.2397 - val_mean_absolute_error: 0.3813\n",
            "Epoch 11/1000\n",
            "2688/2688 [==============================] - 2s 898us/step - loss: 0.2245 - mean_absolute_error: 0.3695 - val_loss: 0.2377 - val_mean_absolute_error: 0.3793\n",
            "Epoch 12/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.2200 - mean_absolute_error: 0.3655 - val_loss: 0.2264 - val_mean_absolute_error: 0.3706\n",
            "Epoch 13/1000\n",
            "2688/2688 [==============================] - 2s 909us/step - loss: 0.2137 - mean_absolute_error: 0.3605 - val_loss: 0.2208 - val_mean_absolute_error: 0.3650\n",
            "Epoch 14/1000\n",
            "2688/2688 [==============================] - 2s 916us/step - loss: 0.2085 - mean_absolute_error: 0.3557 - val_loss: 0.2208 - val_mean_absolute_error: 0.3659\n",
            "Epoch 15/1000\n",
            "2688/2688 [==============================] - 2s 895us/step - loss: 0.2055 - mean_absolute_error: 0.3532 - val_loss: 0.2137 - val_mean_absolute_error: 0.3593\n",
            "Epoch 16/1000\n",
            "2688/2688 [==============================] - 2s 883us/step - loss: 0.1995 - mean_absolute_error: 0.3479 - val_loss: 0.2090 - val_mean_absolute_error: 0.3560\n",
            "Epoch 17/1000\n",
            "2688/2688 [==============================] - 2s 903us/step - loss: 0.1945 - mean_absolute_error: 0.3436 - val_loss: 0.2326 - val_mean_absolute_error: 0.3748\n",
            "Epoch 18/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.1935 - mean_absolute_error: 0.3427 - val_loss: 0.1994 - val_mean_absolute_error: 0.3474\n",
            "Epoch 19/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.1888 - mean_absolute_error: 0.3387 - val_loss: 0.1961 - val_mean_absolute_error: 0.3443\n",
            "Epoch 20/1000\n",
            "2688/2688 [==============================] - 2s 929us/step - loss: 0.1886 - mean_absolute_error: 0.3386 - val_loss: 0.1969 - val_mean_absolute_error: 0.3450\n",
            "Epoch 21/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.1838 - mean_absolute_error: 0.3343 - val_loss: 0.1924 - val_mean_absolute_error: 0.3411\n",
            "Epoch 22/1000\n",
            "2688/2688 [==============================] - 2s 901us/step - loss: 0.1798 - mean_absolute_error: 0.3307 - val_loss: 0.1899 - val_mean_absolute_error: 0.3390\n",
            "Epoch 23/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.1786 - mean_absolute_error: 0.3297 - val_loss: 0.1899 - val_mean_absolute_error: 0.3386\n",
            "Epoch 24/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.1772 - mean_absolute_error: 0.3283 - val_loss: 0.1962 - val_mean_absolute_error: 0.3434\n",
            "Epoch 25/1000\n",
            "2688/2688 [==============================] - 2s 885us/step - loss: 0.1741 - mean_absolute_error: 0.3256 - val_loss: 0.1911 - val_mean_absolute_error: 0.3395\n",
            "Epoch 26/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.1703 - mean_absolute_error: 0.3220 - val_loss: 0.1844 - val_mean_absolute_error: 0.3341\n",
            "Epoch 27/1000\n",
            "2688/2688 [==============================] - 2s 908us/step - loss: 0.1684 - mean_absolute_error: 0.3203 - val_loss: 0.1805 - val_mean_absolute_error: 0.3303\n",
            "Epoch 28/1000\n",
            "2688/2688 [==============================] - 2s 890us/step - loss: 0.1681 - mean_absolute_error: 0.3199 - val_loss: 0.1808 - val_mean_absolute_error: 0.3306\n",
            "Epoch 29/1000\n",
            "2688/2688 [==============================] - 2s 904us/step - loss: 0.1669 - mean_absolute_error: 0.3190 - val_loss: 0.1806 - val_mean_absolute_error: 0.3301\n",
            "Epoch 30/1000\n",
            "2688/2688 [==============================] - 2s 910us/step - loss: 0.1640 - mean_absolute_error: 0.3160 - val_loss: 0.1852 - val_mean_absolute_error: 0.3345\n",
            "Epoch 31/1000\n",
            "2688/2688 [==============================] - 2s 894us/step - loss: 0.1610 - mean_absolute_error: 0.3132 - val_loss: 0.1787 - val_mean_absolute_error: 0.3282\n",
            "Epoch 32/1000\n",
            "2688/2688 [==============================] - 2s 917us/step - loss: 0.1622 - mean_absolute_error: 0.3145 - val_loss: 0.1758 - val_mean_absolute_error: 0.3261\n",
            "Epoch 33/1000\n",
            "2688/2688 [==============================] - 2s 902us/step - loss: 0.1587 - mean_absolute_error: 0.3112 - val_loss: 0.1773 - val_mean_absolute_error: 0.3273\n",
            "Epoch 34/1000\n",
            "2688/2688 [==============================] - 2s 892us/step - loss: 0.1566 - mean_absolute_error: 0.3092 - val_loss: 0.1752 - val_mean_absolute_error: 0.3255\n",
            "Epoch 35/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.1543 - mean_absolute_error: 0.3069 - val_loss: 0.1797 - val_mean_absolute_error: 0.3289\n",
            "Epoch 36/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.1543 - mean_absolute_error: 0.3071 - val_loss: 0.1750 - val_mean_absolute_error: 0.3248\n",
            "Epoch 37/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.1532 - mean_absolute_error: 0.3060 - val_loss: 0.1743 - val_mean_absolute_error: 0.3241\n",
            "Epoch 38/1000\n",
            "2688/2688 [==============================] - 2s 881us/step - loss: 0.1536 - mean_absolute_error: 0.3064 - val_loss: 0.1707 - val_mean_absolute_error: 0.3211\n",
            "Epoch 39/1000\n",
            "2688/2688 [==============================] - 2s 882us/step - loss: 0.1511 - mean_absolute_error: 0.3040 - val_loss: 0.1724 - val_mean_absolute_error: 0.3228\n",
            "Epoch 40/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.1495 - mean_absolute_error: 0.3024 - val_loss: 0.1713 - val_mean_absolute_error: 0.3212\n",
            "Epoch 41/1000\n",
            "2688/2688 [==============================] - 2s 880us/step - loss: 0.1481 - mean_absolute_error: 0.3012 - val_loss: 0.1710 - val_mean_absolute_error: 0.3216\n",
            "Epoch 42/1000\n",
            "2688/2688 [==============================] - 2s 876us/step - loss: 0.1469 - mean_absolute_error: 0.3000 - val_loss: 0.1673 - val_mean_absolute_error: 0.3178\n",
            "Epoch 43/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.1464 - mean_absolute_error: 0.2994 - val_loss: 0.1734 - val_mean_absolute_error: 0.3234\n",
            "Epoch 44/1000\n",
            "2688/2688 [==============================] - 2s 889us/step - loss: 0.1437 - mean_absolute_error: 0.2967 - val_loss: 0.1679 - val_mean_absolute_error: 0.3189\n",
            "Epoch 45/1000\n",
            "2688/2688 [==============================] - 2s 893us/step - loss: 0.1430 - mean_absolute_error: 0.2961 - val_loss: 0.1697 - val_mean_absolute_error: 0.3203\n",
            "Epoch 46/1000\n",
            "2688/2688 [==============================] - 2s 894us/step - loss: 0.1410 - mean_absolute_error: 0.2939 - val_loss: 0.1673 - val_mean_absolute_error: 0.3175\n",
            "Epoch 47/1000\n",
            "2688/2688 [==============================] - 2s 915us/step - loss: 0.1405 - mean_absolute_error: 0.2936 - val_loss: 0.1663 - val_mean_absolute_error: 0.3165\n",
            "Epoch 48/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.1406 - mean_absolute_error: 0.2937 - val_loss: 0.1702 - val_mean_absolute_error: 0.3210\n",
            "Epoch 49/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.1392 - mean_absolute_error: 0.2922 - val_loss: 0.1705 - val_mean_absolute_error: 0.3219\n",
            "Epoch 50/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.1377 - mean_absolute_error: 0.2907 - val_loss: 0.1686 - val_mean_absolute_error: 0.3190\n",
            "Epoch 51/1000\n",
            "2688/2688 [==============================] - 2s 897us/step - loss: 0.1383 - mean_absolute_error: 0.2914 - val_loss: 0.1671 - val_mean_absolute_error: 0.3182\n",
            "Epoch 52/1000\n",
            "2688/2688 [==============================] - 2s 912us/step - loss: 0.1361 - mean_absolute_error: 0.2890 - val_loss: 0.1639 - val_mean_absolute_error: 0.3143\n",
            "Epoch 53/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.1347 - mean_absolute_error: 0.2875 - val_loss: 0.1650 - val_mean_absolute_error: 0.3156\n",
            "Epoch 54/1000\n",
            "2688/2688 [==============================] - 2s 902us/step - loss: 0.1354 - mean_absolute_error: 0.2883 - val_loss: 0.1634 - val_mean_absolute_error: 0.3142\n",
            "Epoch 55/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.1337 - mean_absolute_error: 0.2866 - val_loss: 0.1644 - val_mean_absolute_error: 0.3151\n",
            "Epoch 56/1000\n",
            "2688/2688 [==============================] - 2s 899us/step - loss: 0.1324 - mean_absolute_error: 0.2852 - val_loss: 0.1674 - val_mean_absolute_error: 0.3176\n",
            "Epoch 57/1000\n",
            "2688/2688 [==============================] - 2s 900us/step - loss: 0.1317 - mean_absolute_error: 0.2845 - val_loss: 0.1646 - val_mean_absolute_error: 0.3148\n",
            "Epoch 58/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.1314 - mean_absolute_error: 0.2841 - val_loss: 0.1626 - val_mean_absolute_error: 0.3139\n",
            "Epoch 59/1000\n",
            "2688/2688 [==============================] - 2s 893us/step - loss: 0.1315 - mean_absolute_error: 0.2842 - val_loss: 0.1672 - val_mean_absolute_error: 0.3173\n",
            "Epoch 60/1000\n",
            "2688/2688 [==============================] - 2s 889us/step - loss: 0.1297 - mean_absolute_error: 0.2823 - val_loss: 0.1656 - val_mean_absolute_error: 0.3162\n",
            "Epoch 61/1000\n",
            "2688/2688 [==============================] - 2s 883us/step - loss: 0.1306 - mean_absolute_error: 0.2834 - val_loss: 0.1616 - val_mean_absolute_error: 0.3123\n",
            "Epoch 62/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.1285 - mean_absolute_error: 0.2811 - val_loss: 0.1646 - val_mean_absolute_error: 0.3148\n",
            "Epoch 63/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.1280 - mean_absolute_error: 0.2804 - val_loss: 0.1633 - val_mean_absolute_error: 0.3138\n",
            "Epoch 64/1000\n",
            "2688/2688 [==============================] - 2s 890us/step - loss: 0.1274 - mean_absolute_error: 0.2799 - val_loss: 0.1642 - val_mean_absolute_error: 0.3148\n",
            "Epoch 65/1000\n",
            "2688/2688 [==============================] - 2s 892us/step - loss: 0.1270 - mean_absolute_error: 0.2795 - val_loss: 0.1605 - val_mean_absolute_error: 0.3111\n",
            "Epoch 66/1000\n",
            "2688/2688 [==============================] - 2s 886us/step - loss: 0.1259 - mean_absolute_error: 0.2782 - val_loss: 0.1631 - val_mean_absolute_error: 0.3135\n",
            "Epoch 67/1000\n",
            "2688/2688 [==============================] - 2s 891us/step - loss: 0.1255 - mean_absolute_error: 0.2779 - val_loss: 0.1623 - val_mean_absolute_error: 0.3125\n",
            "Epoch 68/1000\n",
            "2688/2688 [==============================] - 2s 905us/step - loss: 0.1263 - mean_absolute_error: 0.2787 - val_loss: 0.1703 - val_mean_absolute_error: 0.3223\n",
            "Epoch 69/1000\n",
            "2688/2688 [==============================] - 2s 886us/step - loss: 0.1262 - mean_absolute_error: 0.2787 - val_loss: 0.1621 - val_mean_absolute_error: 0.3130\n",
            "Epoch 70/1000\n",
            "2688/2688 [==============================] - 2s 896us/step - loss: 0.1243 - mean_absolute_error: 0.2766 - val_loss: 0.1640 - val_mean_absolute_error: 0.3148\n",
            "Epoch 71/1000\n",
            "2688/2688 [==============================] - 2s 875us/step - loss: 0.1249 - mean_absolute_error: 0.2772 - val_loss: 0.1606 - val_mean_absolute_error: 0.3111\n",
            "Epoch 72/1000\n",
            "2688/2688 [==============================] - 2s 867us/step - loss: 0.1231 - mean_absolute_error: 0.2752 - val_loss: 0.1609 - val_mean_absolute_error: 0.3115\n",
            "Epoch 73/1000\n",
            "2688/2688 [==============================] - 2s 864us/step - loss: 0.1222 - mean_absolute_error: 0.2743 - val_loss: 0.1629 - val_mean_absolute_error: 0.3140\n",
            "Epoch 74/1000\n",
            "2688/2688 [==============================] - 2s 876us/step - loss: 0.1225 - mean_absolute_error: 0.2746 - val_loss: 0.1606 - val_mean_absolute_error: 0.3112\n",
            "Epoch 75/1000\n",
            "2688/2688 [==============================] - 2s 866us/step - loss: 0.1215 - mean_absolute_error: 0.2734 - val_loss: 0.1604 - val_mean_absolute_error: 0.3109\n",
            "Epoch 76/1000\n",
            "2688/2688 [==============================] - 2s 868us/step - loss: 0.1211 - mean_absolute_error: 0.2730 - val_loss: 0.1599 - val_mean_absolute_error: 0.3104\n",
            "Epoch 77/1000\n",
            "2688/2688 [==============================] - 2s 869us/step - loss: 0.1235 - mean_absolute_error: 0.2758 - val_loss: 0.1615 - val_mean_absolute_error: 0.3121\n",
            "Epoch 78/1000\n",
            "2688/2688 [==============================] - 2s 868us/step - loss: 0.1213 - mean_absolute_error: 0.2732 - val_loss: 0.1610 - val_mean_absolute_error: 0.3113\n",
            "Epoch 79/1000\n",
            "2688/2688 [==============================] - 2s 867us/step - loss: 0.1201 - mean_absolute_error: 0.2718 - val_loss: 0.1619 - val_mean_absolute_error: 0.3125\n",
            "Epoch 80/1000\n",
            "2688/2688 [==============================] - 2s 885us/step - loss: 0.1203 - mean_absolute_error: 0.2721 - val_loss: 0.1617 - val_mean_absolute_error: 0.3124\n",
            "Epoch 81/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.1189 - mean_absolute_error: 0.2704 - val_loss: 0.1613 - val_mean_absolute_error: 0.3117\n",
            "Epoch 82/1000\n",
            "2688/2688 [==============================] - 2s 874us/step - loss: 0.1188 - mean_absolute_error: 0.2703 - val_loss: 0.1616 - val_mean_absolute_error: 0.3123\n",
            "Epoch 83/1000\n",
            "2688/2688 [==============================] - 2s 865us/step - loss: 0.1198 - mean_absolute_error: 0.2715 - val_loss: 0.1618 - val_mean_absolute_error: 0.3123\n",
            "Epoch 84/1000\n",
            "2688/2688 [==============================] - 2s 867us/step - loss: 0.1180 - mean_absolute_error: 0.2696 - val_loss: 0.1602 - val_mean_absolute_error: 0.3107\n",
            "Epoch 85/1000\n",
            "2688/2688 [==============================] - 2s 877us/step - loss: 0.1178 - mean_absolute_error: 0.2692 - val_loss: 0.1608 - val_mean_absolute_error: 0.3114\n",
            "Epoch 86/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.1169 - mean_absolute_error: 0.2683 - val_loss: 0.1631 - val_mean_absolute_error: 0.3131\n",
            "Epoch 87/1000\n",
            "2688/2688 [==============================] - 2s 884us/step - loss: 0.1180 - mean_absolute_error: 0.2696 - val_loss: 0.1631 - val_mean_absolute_error: 0.3135\n",
            "Epoch 88/1000\n",
            "2688/2688 [==============================] - 2s 879us/step - loss: 0.1179 - mean_absolute_error: 0.2695 - val_loss: 0.1602 - val_mean_absolute_error: 0.3107\n",
            "Epoch 89/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.1164 - mean_absolute_error: 0.2676 - val_loss: 0.1605 - val_mean_absolute_error: 0.3108\n",
            "Epoch 90/1000\n",
            "2688/2688 [==============================] - 2s 867us/step - loss: 0.1179 - mean_absolute_error: 0.2695 - val_loss: 0.1679 - val_mean_absolute_error: 0.3183\n",
            "Epoch 91/1000\n",
            "2688/2688 [==============================] - 2s 865us/step - loss: 0.1182 - mean_absolute_error: 0.2698 - val_loss: 0.1624 - val_mean_absolute_error: 0.3130\n",
            "Epoch 92/1000\n",
            "2688/2688 [==============================] - 2s 872us/step - loss: 0.1157 - mean_absolute_error: 0.2669 - val_loss: 0.1619 - val_mean_absolute_error: 0.3120\n",
            "Epoch 93/1000\n",
            "2688/2688 [==============================] - 2s 866us/step - loss: 0.1162 - mean_absolute_error: 0.2675 - val_loss: 0.1625 - val_mean_absolute_error: 0.3127\n",
            "Epoch 94/1000\n",
            "2688/2688 [==============================] - 2s 866us/step - loss: 0.1157 - mean_absolute_error: 0.2669 - val_loss: 0.1626 - val_mean_absolute_error: 0.3128\n",
            "Epoch 95/1000\n",
            "2688/2688 [==============================] - 2s 865us/step - loss: 0.1154 - mean_absolute_error: 0.2666 - val_loss: 0.1614 - val_mean_absolute_error: 0.3123\n",
            "Epoch 96/1000\n",
            "2688/2688 [==============================] - 2s 865us/step - loss: 0.1145 - mean_absolute_error: 0.2656 - val_loss: 0.1614 - val_mean_absolute_error: 0.3121\n",
            "Epoch 97/1000\n",
            "2688/2688 [==============================] - 2s 870us/step - loss: 0.1142 - mean_absolute_error: 0.2652 - val_loss: 0.1591 - val_mean_absolute_error: 0.3096\n",
            "Epoch 98/1000\n",
            "2688/2688 [==============================] - 2s 873us/step - loss: 0.1137 - mean_absolute_error: 0.2647 - val_loss: 0.1601 - val_mean_absolute_error: 0.3109\n",
            "Epoch 99/1000\n",
            "2688/2688 [==============================] - 2s 870us/step - loss: 0.1134 - mean_absolute_error: 0.2644 - val_loss: 0.1591 - val_mean_absolute_error: 0.3097\n",
            "Epoch 100/1000\n",
            "2688/2688 [==============================] - 2s 869us/step - loss: 0.1128 - mean_absolute_error: 0.2636 - val_loss: 0.1627 - val_mean_absolute_error: 0.3142\n",
            "Epoch 101/1000\n",
            "2688/2688 [==============================] - 2s 872us/step - loss: 0.1155 - mean_absolute_error: 0.2668 - val_loss: 0.1626 - val_mean_absolute_error: 0.3128\n",
            "Epoch 102/1000\n",
            "2688/2688 [==============================] - 2s 873us/step - loss: 0.1132 - mean_absolute_error: 0.2641 - val_loss: 0.1606 - val_mean_absolute_error: 0.3112\n",
            "Epoch 103/1000\n",
            "2688/2688 [==============================] - 2s 874us/step - loss: 0.1119 - mean_absolute_error: 0.2626 - val_loss: 0.1614 - val_mean_absolute_error: 0.3120\n",
            "Epoch 104/1000\n",
            "2688/2688 [==============================] - 2s 864us/step - loss: 0.1136 - mean_absolute_error: 0.2648 - val_loss: 0.1618 - val_mean_absolute_error: 0.3122\n",
            "Epoch 105/1000\n",
            "2688/2688 [==============================] - 2s 874us/step - loss: 0.1124 - mean_absolute_error: 0.2632 - val_loss: 0.1618 - val_mean_absolute_error: 0.3123\n",
            "Epoch 106/1000\n",
            "2688/2688 [==============================] - 2s 869us/step - loss: 0.1129 - mean_absolute_error: 0.2638 - val_loss: 0.1613 - val_mean_absolute_error: 0.3118\n",
            "Epoch 107/1000\n",
            "2688/2688 [==============================] - 2s 873us/step - loss: 0.1119 - mean_absolute_error: 0.2627 - val_loss: 0.1609 - val_mean_absolute_error: 0.3115\n",
            "Epoch 108/1000\n",
            "2688/2688 [==============================] - 2s 875us/step - loss: 0.1115 - mean_absolute_error: 0.2621 - val_loss: 0.1592 - val_mean_absolute_error: 0.3097\n",
            "Epoch 109/1000\n",
            "2688/2688 [==============================] - 2s 868us/step - loss: 0.1114 - mean_absolute_error: 0.2620 - val_loss: 0.1599 - val_mean_absolute_error: 0.3108\n",
            "Epoch 110/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.1108 - mean_absolute_error: 0.2614 - val_loss: 0.1636 - val_mean_absolute_error: 0.3139\n",
            "Epoch 111/1000\n",
            "2688/2688 [==============================] - 2s 872us/step - loss: 0.1122 - mean_absolute_error: 0.2631 - val_loss: 0.1603 - val_mean_absolute_error: 0.3114\n",
            "Epoch 112/1000\n",
            "2688/2688 [==============================] - 2s 875us/step - loss: 0.1112 - mean_absolute_error: 0.2618 - val_loss: 0.1602 - val_mean_absolute_error: 0.3107\n",
            "Epoch 113/1000\n",
            "2688/2688 [==============================] - 2s 864us/step - loss: 0.1107 - mean_absolute_error: 0.2613 - val_loss: 0.1607 - val_mean_absolute_error: 0.3115\n",
            "Epoch 114/1000\n",
            "2688/2688 [==============================] - 2s 870us/step - loss: 0.1104 - mean_absolute_error: 0.2610 - val_loss: 0.1623 - val_mean_absolute_error: 0.3127\n",
            "Epoch 115/1000\n",
            "2688/2688 [==============================] - 2s 867us/step - loss: 0.1112 - mean_absolute_error: 0.2618 - val_loss: 0.1611 - val_mean_absolute_error: 0.3117\n",
            "Epoch 116/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.1088 - mean_absolute_error: 0.2589 - val_loss: 0.1606 - val_mean_absolute_error: 0.3110\n",
            "Epoch 117/1000\n",
            "2688/2688 [==============================] - 2s 876us/step - loss: 0.1093 - mean_absolute_error: 0.2596 - val_loss: 0.1615 - val_mean_absolute_error: 0.3116\n",
            "Epoch 118/1000\n",
            "2688/2688 [==============================] - 2s 865us/step - loss: 0.1093 - mean_absolute_error: 0.2595 - val_loss: 0.1610 - val_mean_absolute_error: 0.3117\n",
            "Epoch 119/1000\n",
            "2688/2688 [==============================] - 2s 873us/step - loss: 0.1115 - mean_absolute_error: 0.2622 - val_loss: 0.1619 - val_mean_absolute_error: 0.3124\n",
            "Epoch 120/1000\n",
            "2688/2688 [==============================] - 2s 880us/step - loss: 0.1096 - mean_absolute_error: 0.2600 - val_loss: 0.1600 - val_mean_absolute_error: 0.3105\n",
            "Epoch 121/1000\n",
            "2688/2688 [==============================] - 2s 867us/step - loss: 0.1084 - mean_absolute_error: 0.2586 - val_loss: 0.1601 - val_mean_absolute_error: 0.3106\n",
            "Epoch 122/1000\n",
            "2688/2688 [==============================] - 2s 887us/step - loss: 0.1076 - mean_absolute_error: 0.2576 - val_loss: 0.1607 - val_mean_absolute_error: 0.3111\n",
            "Epoch 123/1000\n",
            "2688/2688 [==============================] - 2s 878us/step - loss: 0.1074 - mean_absolute_error: 0.2573 - val_loss: 0.1617 - val_mean_absolute_error: 0.3125\n",
            "Epoch 124/1000\n",
            "2688/2688 [==============================] - 2s 872us/step - loss: 0.1076 - mean_absolute_error: 0.2576 - val_loss: 0.1622 - val_mean_absolute_error: 0.3127\n",
            "Epoch 125/1000\n",
            "2688/2688 [==============================] - 2s 871us/step - loss: 0.1084 - mean_absolute_error: 0.2586 - val_loss: 0.1623 - val_mean_absolute_error: 0.3124\n",
            "Epoch 126/1000\n",
            "2688/2688 [==============================] - 2s 872us/step - loss: 0.1095 - mean_absolute_error: 0.2599 - val_loss: 0.1673 - val_mean_absolute_error: 0.3185\n",
            "Epoch 127/1000\n",
            "2688/2688 [==============================] - 2s 867us/step - loss: 0.1076 - mean_absolute_error: 0.2577 - val_loss: 0.1619 - val_mean_absolute_error: 0.3124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7eff3176dbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "RfvCtJTd5bsr",
        "colab_type": "code",
        "outputId": "6cd826f6-ec4b-41fc-b91e-a06eb7558ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "error = model.evaluate(val_x,val_y)\n",
        "print (error)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "840/840 [==============================] - 0s 258us/step\n",
            "[0.15931686404205503, 0.30976539055506386]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_mgkvCKEl4n6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-Y3vypHql7d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing Set NRMSE Loss"
      ]
    },
    {
      "metadata": {
        "id": "BoIk0Kr9p686",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "3586b7f0-21f0-48d3-90f6-e94f7ecb1fdc"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def nrmse(y_true,y_pred):\n",
        "\n",
        "    return (np.sqrt(np.mean(np.square(y_true - y_pred))))/np.mean(y_pred)\n",
        "def calculate_nrmse():\n",
        "  y_pred = np.exp(model.predict(val_x))\n",
        "  print (f'shape of y_pred {y_pred.shape}')\n",
        "  y_true = np.exp(val_y)\n",
        "  print (f'shape of y_true {y_true.shape}')\n",
        "\n",
        "  loss =  nrmse(y_true,y_pred)\n",
        "\n",
        "  print (loss)\n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "nrmse_loss = calculate_nrmse()\n",
        "print ('claculate valuatdaion NRMSE loss', nrmse_loss)\n",
        "\n",
        "print ('\\n', 'start preparing test data')\n",
        "#print (os.path.isfile(src+'test2.npy'))\n",
        "test = np.load(src+'test2.npy')\n",
        "test_data = (test - data_mean)/data_std\n",
        "\n",
        "test_data = test_data[:,:,:,select_list_2]\n",
        "\n",
        "test_data = test_data[:,select_list_0,:,:]\n",
        "\n",
        "test_data = test_data[:,:,select_list_0,:]\n",
        "print ('test_data shape', test_data.shape)\n",
        "\n",
        "\n",
        "prediction = model.predict(test_data)\n",
        "prediction = np.exp(prediction)\n",
        "pred = prediction.reshape(1800,-1)\n",
        "#print (pred.shape)\n",
        "\n",
        "\n",
        "\n",
        "src_2 =  'drive/My Drive/geo/test_data/test_dataset/'\n",
        "\n",
        "test_label_df = pd.read_csv(src_2+'solution.csv')\n",
        "\n",
        "\n",
        "\n",
        "test_label_df = test_label_df.drop('ID', axis=1)\n",
        "\n",
        "\n",
        "test_label = test_label_df.values\n",
        "\n",
        "\n",
        "print ('test set NRMSE error', nrmse(test_label,pred))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of y_pred (840, 20, 20, 1)\n",
            "shape of y_true (840, 20, 20, 1)\n",
            "0.638197131631338\n",
            "claculate valuatdaion NRMSE loss 0.638197131631338\n",
            "\n",
            " start preparing test data\n",
            "test_data shape (1800, 11, 11, 10)\n",
            "test set NRMSE error 0.6196663668975134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xSZah7QXp7BU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tjpzup0_p7H3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7mlN_NhSp7Sd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}